{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d9dd9c",
   "metadata": {},
   "source": [
    "# Understanding Mamba: State Space Models from First Principles\n",
    "\n",
    "Transformers revolutionized sequence modeling, but they have a fundamental limitation: **O(nÂ²) attention complexity**. For a 100k token context, that's 10 billion operations per layer. \n",
    "\n",
    "Mamba offers an alternative: **O(n) linear complexity** while maintaining expressiveness. But how does it work? And why did it take until 2023 to figure this out?\n",
    "\n",
    "This notebook builds intuition from the ground upâ€”starting from physics and control theory, through the evolution of state space models, to understanding exactly why Mamba works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat, einsum\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "# Add parent directory to path for our utilities\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from silen_lib.utils import utils\n",
    "utils.set_seed(42)\n",
    "\n",
    "# Check if we have a GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2a3ff",
   "metadata": {},
   "source": [
    "## The Transformer Bottleneck\n",
    "\n",
    "Before we understand Mamba, let's feel the pain it solves.\n",
    "\n",
    "In a Transformer, every token attends to every other token. If we have $n$ tokens, that's $n \\times n$ attention computations per layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With 1,000 tokens\n",
    "n = 1000\n",
    "n * n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29460be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With 100,000 tokens (modern context windows)\n",
    "n = 100_000\n",
    "f\"{n * n:,} operations\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ed448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize how these scale\n",
    "\n",
    "seq_lengths = torch.arange(1, 10001, 100)\n",
    "\n",
    "# Transformer: O(nÂ²) attention\n",
    "transformer_ops = seq_lengths ** 2\n",
    "\n",
    "# Mamba: O(n) linear\n",
    "mamba_ops = seq_lengths * 16  # state dimension typically 16\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(seq_lengths, transformer_ops / 1e6, label='Transformer O(nÂ²)', linewidth=2)\n",
    "ax.plot(seq_lengths, mamba_ops / 1e6, label='Mamba O(n)', linewidth=2)\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Operations (millions)')\n",
    "ax.set_title('The Quadratic Wall')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b0652",
   "metadata": {},
   "source": [
    "## Where Did This Thinking Come From?\n",
    "\n",
    "State Space Models aren't new to deep learningâ€”they come from **control theory** and **signal processing**, developed in the 1960s.\n",
    "\n",
    "**The lineage:**\n",
    "- **1960s**: Rudolf Kalman develops state space representations for control systems (Kalman filters)\n",
    "- **1970s-2000s**: SSMs become fundamental to signal processing, robotics, and engineering\n",
    "- **2020**: HiPPO paper shows SSMs can have long-range memory in deep learning\n",
    "- **2021**: S4 (Structured State Spaces) achieves breakthrough on Long Range Arena\n",
    "- **2023**: Mamba adds selectivity, matching Transformer quality with linear complexity\n",
    "\n",
    "The key insight: **engineers have been solving the \"process a sequence efficiently\" problem for 60 years**. Deep learning just needed to adapt these ideas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b6e83",
   "metadata": {},
   "source": [
    "## What is \"State\"? Building from Physics\n",
    "\n",
    "Before we dive into equations, let's understand what \"state\" means intuitively.\n",
    "\n",
    "**A physics example:** Imagine a ball flying through the air. If I tell you only its position right now, can you predict where it will be in 1 second?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20dd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ball at position x=5 meters\n",
    "position = 5.0\n",
    "position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e6bf2",
   "metadata": {},
   "source": [
    "No! It could be moving left, right, up, down, or sitting still. Position alone doesn't determine the future.\n",
    "\n",
    "But if I tell you **both position and velocity**...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a936e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position AND velocity together = the STATE\n",
    "position = 5.0\n",
    "velocity = 2.0  # moving right at 2 m/s\n",
    "\n",
    "state = torch.tensor([position, velocity])\n",
    "state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I CAN predict the future! After 1 second:\n",
    "dt = 1.0\n",
    "new_position = position + velocity * dt\n",
    "new_position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f62ee",
   "metadata": {},
   "source": [
    "**This is the key insight:** State is the minimal information needed to predict the future, given the dynamics.\n",
    "\n",
    "For sequences (like text), we can think of state as a **compressed summary of everything we've seen so far**. Instead of storing all past tokens (like Transformers do with their KV cache), we maintain a fixed-size state that evolves as we see new tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a3c08",
   "metadata": {},
   "source": [
    "### State as Compressed Memory\n",
    "\n",
    "Let's make this concrete with a simple example. Imagine you're reading a story and trying to predict the next word.\n",
    "\n",
    "**Option 1: Store everything** (Transformer approach)\n",
    "- \"The cat sat on the mat and then the cat jumped onto the...\"\n",
    "- Keep all 12 tokens in memory, compute attention over all of them\n",
    "\n",
    "**Option 2: Compress into state** (SSM approach)  \n",
    "- Maintain a hidden state that captures: \"we're talking about a cat doing actions\"\n",
    "- Update this state with each new token\n",
    "- Fixed memory regardless of sequence length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99731aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple state that \"remembers\" by exponential averaging\n",
    "# Think of it as a leaky bucket of information\n",
    "\n",
    "state = 0.0\n",
    "decay = 0.9  # how much of the old state we keep\n",
    "\n",
    "# Incoming \"tokens\" (just numbers for now)\n",
    "tokens = [1.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0]\n",
    "\n",
    "print(\"Token â†’ New State\")\n",
    "for t in tokens:\n",
    "    state = decay * state + (1 - decay) * t\n",
    "    print(f\"  {t:.1f}  â†’  {state:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6409c",
   "metadata": {},
   "source": [
    "Notice how:\n",
    "- The state gradually forgets token 1 (0.100 â†’ 0.090 â†’ 0.081...)\n",
    "- When token 5 arrives, it gets incorporated\n",
    "- The state is a **weighted average of the history**, with recent tokens weighted more\n",
    "\n",
    "This is the simplest possible \"state space model\". The real ones are more sophisticated, but the intuition is the same: **compress history into a fixed-size representation that evolves over time**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ea9a5",
   "metadata": {},
   "source": [
    "## Continuous-Time State Space Models\n",
    "\n",
    "Now let's formalize this. In control theory, a continuous-time state space model is defined by:\n",
    "\n",
    "$$\\frac{dx}{dt} = Ax + Bu$$\n",
    "$$y = Cx + Du$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the **state** (hidden representation)\n",
    "- $u$ is the **input** (incoming signal)\n",
    "- $y$ is the **output** (what we predict)\n",
    "- $A$ controls how the state evolves on its own\n",
    "- $B$ controls how input affects the state\n",
    "- $C$ controls how state maps to output\n",
    "- $D$ is a skip connection (input directly to output)\n",
    "\n",
    "Let's build intuition by simulating a simple system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461dfa7",
   "metadata": {},
   "source": [
    "### Example: A Damped Oscillator\n",
    "\n",
    "A mass on a spring with friction is a perfect SSM example. The state is [position, velocity], and the physics determines how it evolves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da282be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State: [position, velocity]\n",
    "# dx/dt = Ax means: d[pos, vel]/dt = [[0, 1], [-k, -b]] @ [pos, vel]\n",
    "# This encodes: d(pos)/dt = vel, d(vel)/dt = -k*pos - b*vel (spring + friction)\n",
    "\n",
    "k = 1.0   # spring constant\n",
    "b = 0.3   # damping/friction\n",
    "\n",
    "A = torch.tensor([\n",
    "    [0, 1],      # d(position)/dt = velocity\n",
    "    [-k, -b]     # d(velocity)/dt = -k*position - b*velocity\n",
    "])\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05eb300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate using Euler's method (simple numerical integration)\n",
    "# x(t + dt) â‰ˆ x(t) + dt * dx/dt = x(t) + dt * A @ x(t)\n",
    "\n",
    "dt = 0.05\n",
    "steps = 200\n",
    "x = torch.tensor([1.0, 0.0])  # start at position=1, velocity=0\n",
    "\n",
    "trajectory = [x.clone()]\n",
    "for _ in range(steps):\n",
    "    dx_dt = A @ x\n",
    "    x = x + dt * dx_dt\n",
    "    trajectory.append(x.clone())\n",
    "\n",
    "trajectory = torch.stack(trajectory)\n",
    "trajectory.shape  # (steps+1, 2) - position and velocity at each step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "time = torch.arange(len(trajectory)) * dt\n",
    "\n",
    "# Position over time\n",
    "axes[0].plot(time, trajectory[:, 0])\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Position')\n",
    "axes[0].set_title('Damped Oscillator: Position')\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Phase space (position vs velocity)\n",
    "axes[1].plot(trajectory[:, 0], trajectory[:, 1])\n",
    "axes[1].scatter([trajectory[0, 0]], [trajectory[0, 1]], color='green', s=100, label='Start', zorder=5)\n",
    "axes[1].scatter([trajectory[-1, 0]], [trajectory[-1, 1]], color='red', s=100, label='End', zorder=5)\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Velocity')\n",
    "axes[1].set_title('Phase Space: State Evolution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d46d4",
   "metadata": {},
   "source": [
    "The state spirals inward (energy dissipates due to friction) and settles at equilibrium. \n",
    "\n",
    "**Key insight about A:** The matrix A determines the system's dynamics. Different A matrices create different behaviors:\n",
    "- If eigenvalues have negative real parts â†’ system is stable (decays to equilibrium)\n",
    "- If eigenvalues have positive real parts â†’ system is unstable (explodes)\n",
    "- Complex eigenvalues â†’ oscillation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check eigenvalues of our A matrix\n",
    "eigenvalues = torch.linalg.eigvals(A)\n",
    "eigenvalues  # Complex with negative real parts = stable oscillation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2d2e8",
   "metadata": {},
   "source": [
    "### Adding Inputs: The B Matrix\n",
    "\n",
    "So far our system has no inputâ€”it just evolves on its own. But for language modeling, we need to feed in tokens! That's what B does: it maps input to state changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ee9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B maps input to state changes\n",
    "# If we push the mass, it affects velocity (not position directly)\n",
    "B = torch.tensor([[0.0], [1.0]])  # input affects velocity\n",
    "B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now simulate with periodic \"pushes\" (like tokens coming in)\n",
    "x = torch.tensor([0.0, 0.0])  # start at rest\n",
    "\n",
    "# Input signal: periodic pushes\n",
    "inputs = torch.zeros(steps)\n",
    "inputs[20] = 5.0   # push at step 20\n",
    "inputs[80] = -3.0  # push other direction at step 80\n",
    "inputs[140] = 4.0  # another push\n",
    "\n",
    "trajectory_with_input = [x.clone()]\n",
    "for i in range(steps):\n",
    "    u = inputs[i:i+1]  # current input (shape [1])\n",
    "    dx_dt = A @ x + (B @ u).squeeze()\n",
    "    x = x + dt * dx_dt\n",
    "    trajectory_with_input.append(x.clone())\n",
    "\n",
    "trajectory_with_input = torch.stack(trajectory_with_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063bfe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "ax.plot(time, trajectory_with_input[:, 0], label='Position')\n",
    "\n",
    "# Mark input times\n",
    "for i, inp in enumerate(inputs):\n",
    "    if inp != 0:\n",
    "        ax.axvline(x=i*dt, color='red', alpha=0.5, linestyle='--')\n",
    "        ax.annotate(f'Push: {inp:.0f}', (i*dt, 0.8), fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Position')\n",
    "ax.set_title('SSM with Inputs: Each \"push\" affects future states')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d4cdf",
   "metadata": {},
   "source": [
    "**This is exactly how SSMs process sequences!**\n",
    "- Each token is like a \"push\" (input through B)\n",
    "- The state evolves according to A, remembering past inputs\n",
    "- The effect of each input ripples through time\n",
    "\n",
    "### The C Matrix: Reading the State\n",
    "\n",
    "Finally, C maps state to output. We might only care about certain aspects of the state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2eecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C extracts output from state\n",
    "# Maybe we only care about position, not velocity\n",
    "C = torch.tensor([[1.0, 0.0]])  # output = position only\n",
    "C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output for each timestep\n",
    "outputs = (C @ trajectory_with_input.T).squeeze()\n",
    "outputs.shape  # one output per timestep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5337763",
   "metadata": {},
   "source": [
    "## From Continuous to Discrete: Discretization\n",
    "\n",
    "There's a problem: our equations are continuous (derivatives), but our data is discrete (tokens at specific positions).\n",
    "\n",
    "We need to **discretize** the continuous SSM. The most common method is **Zero-Order Hold (ZOH)**: assume the input is constant between timesteps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36966f",
   "metadata": {},
   "source": [
    "### The Discretization Formulas\n",
    "\n",
    "Given step size $\\Delta$ (not to be confused with \"change\"):\n",
    "\n",
    "$$\\bar{A} = e^{A\\Delta}$$\n",
    "$$\\bar{B} = (A)^{-1}(e^{A\\Delta} - I) B$$\n",
    "\n",
    "Then the discrete recurrence becomes:\n",
    "$$x_k = \\bar{A} x_{k-1} + \\bar{B} u_k$$\n",
    "$$y_k = C x_k$$\n",
    "\n",
    "Let's see what this means intuitively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest case: 1D state, 1D input\n",
    "# dx/dt = a*x + b*u  (scalar version)\n",
    "\n",
    "a = -0.5  # decay rate (negative = stable)\n",
    "b = 1.0   # input sensitivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize with step size delta\n",
    "delta = 0.1\n",
    "\n",
    "# A_bar = exp(a * delta)\n",
    "A_bar = math.exp(a * delta)\n",
    "A_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_bar = (1/a) * (exp(a*delta) - 1) * b\n",
    "B_bar = (1/a) * (A_bar - 1) * b\n",
    "B_bar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b693046",
   "metadata": {},
   "source": [
    "**What does $\\bar{A} = 0.951$ mean?**\n",
    "\n",
    "Each step, the state is multiplied by 0.951. So after one step, we retain 95.1% of the previous state. After 10 steps: $0.951^{10} = 0.606$. After 100 steps: $0.951^{100} = 0.007$. \n",
    "\n",
    "This is **exponential decay**â€”the hallmark of linear systems. The eigenvalues of A control how fast things are forgotten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe28c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete SSM step is now trivial: x_k = A_bar * x_{k-1} + B_bar * u_k\n",
    "x = 0.0\n",
    "for u in [1.0, 0.0, 0.0, 0.0, 0.0]:\n",
    "    x = A_bar * x + B_bar * u\n",
    "    print(f\"input={u:.1f} â†’ state={x:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672b82e",
   "metadata": {},
   "source": [
    "### Visualizing Discretization\n",
    "\n",
    "Let's see how the continuous and discrete systems compare:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704c178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare continuous and discrete at different step sizes\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# \"True\" continuous solution: x(t) = x0 * exp(a*t) for zero input starting from x=1\n",
    "t_continuous = torch.linspace(0, 5, 500)\n",
    "x_continuous = torch.exp(a * t_continuous)\n",
    "ax.plot(t_continuous, x_continuous, 'k-', label='Continuous', linewidth=2)\n",
    "\n",
    "# Discrete with different step sizes\n",
    "for delta in [0.1, 0.5, 1.0]:\n",
    "    A_bar = math.exp(a * delta)\n",
    "    t_discrete = torch.arange(0, 5 + delta, delta)\n",
    "    x_discrete = torch.tensor([A_bar ** k for k in range(len(t_discrete))])\n",
    "    ax.plot(t_discrete, x_discrete, 'o--', label=f'Discrete Î”={delta}', markersize=5)\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('State')\n",
    "ax.set_title('Continuous vs Discrete: Smaller Î” = Better Approximation')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cdf5fa",
   "metadata": {},
   "source": [
    "**Key insight for Mamba:** In Mamba, $\\Delta$ (the discretization step) becomes a **learnable parameter** that can vary per token! \n",
    "\n",
    "- Small $\\Delta$ â†’ the system changes slowly, focusing on the input\n",
    "- Large $\\Delta$ â†’ the system evolves more, mixing with past state\n",
    "\n",
    "This will become crucial later when we discuss **selectivity**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032d0dd",
   "metadata": {},
   "source": [
    "### ðŸ§ª Test Problem: Discretize a 2D SSM\n",
    "\n",
    "Given the continuous A and B matrices from our oscillator, compute the discrete versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test problem: Discretize our 2D oscillator SSM\n",
    "test_A = torch.tensor([[0.0, 1.0], [-1.0, -0.3]], dtype=torch.float32)\n",
    "test_B = torch.tensor([[0.0], [1.0]], dtype=torch.float32)\n",
    "test_delta = 0.1\n",
    "\n",
    "# Fill in the code to compute A_bar and B_bar\n",
    "# Hint: Use torch.linalg.matrix_exp for matrix exponential\n",
    "# Hint: Use torch.linalg.solve or torch.linalg.inv for the inverse\n",
    "\n",
    "# test_A_bar = ???  # fill in code here\n",
    "# test_B_bar = ???  # fill in code here\n",
    "\n",
    "# Uncomment to check your answer:\n",
    "# assert test_A_bar.shape == (2, 2), \"A_bar should be 2x2\"\n",
    "# assert test_B_bar.shape == (2, 1), \"B_bar should be 2x1\"\n",
    "# assert torch.allclose(test_A_bar[0, 0], torch.tensor(0.9851), atol=1e-3), \"Check your A_bar computation\"\n",
    "# print(\"âœ“ Discretization correct!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9439e",
   "metadata": {},
   "source": [
    "## The Two Faces of SSMs: Recurrence vs Convolution\n",
    "\n",
    "Here's a remarkable fact: the same SSM computation can be done two completely different ways:\n",
    "\n",
    "1. **Recurrent mode**: Process step-by-step like an RNN\n",
    "2. **Convolutional mode**: Precompute a kernel and convolve\n",
    "\n",
    "They give **identical results**, but have different computational tradeoffs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43aa374",
   "metadata": {},
   "source": [
    "### Mode 1: Recurrent Computation\n",
    "\n",
    "The obvious wayâ€”process each token sequentially:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssm_recurrent(A_bar, B_bar, C, u_sequence):\n",
    "    \"\"\"\n",
    "    Process sequence recurrently.\n",
    "    \n",
    "    Args:\n",
    "        A_bar: (N, N) discrete state matrix\n",
    "        B_bar: (N, 1) discrete input matrix\n",
    "        C: (1, N) output matrix\n",
    "        u_sequence: (L,) input sequence\n",
    "    \n",
    "    Returns:\n",
    "        (L,) output sequence\n",
    "    \"\"\"\n",
    "    N = A_bar.shape[0]\n",
    "    L = len(u_sequence)\n",
    "    \n",
    "    x = torch.zeros(N)  # initial state\n",
    "    outputs = []\n",
    "    \n",
    "    for t in range(L):\n",
    "        x = A_bar @ x + B_bar.squeeze() * u_sequence[t]\n",
    "        y = C @ x\n",
    "        outputs.append(y.item())\n",
    "    \n",
    "    return torch.tensor(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 1D SSM for demonstration\n",
    "N = 1  # state dimension\n",
    "A_demo = torch.tensor([[0.9]])  # decay factor\n",
    "B_demo = torch.tensor([[0.1]])  # input scaling\n",
    "C_demo = torch.tensor([[1.0]])  # output = state\n",
    "\n",
    "# Fake embeddings for tokens \"The\" \"cat\" \"sat\"\n",
    "u_demo = torch.randn(10)\n",
    "u_demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_recurrent = ssm_recurrent(A_demo, B_demo, C_demo, u_demo)\n",
    "y_recurrent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a5a88",
   "metadata": {},
   "source": [
    "### Mode 2: Convolutional Computation\n",
    "\n",
    "Now the magic: we can **precompute a convolution kernel** that does the same thing!\n",
    "\n",
    "The kernel is: $\\mathcal{K} = (C\\bar{B}, C\\bar{A}\\bar{B}, C\\bar{A}^2\\bar{B}, ..., C\\bar{A}^{L-1}\\bar{B})$\n",
    "\n",
    "This is basically \"how much does input at time 0 affect output at time k?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fc215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ssm_kernel(A_bar, B_bar, C, L):\n",
    "    \"\"\"\n",
    "    Compute the SSM convolution kernel.\n",
    "    \n",
    "    K[k] = C @ A_bar^k @ B_bar\n",
    "    \"\"\"\n",
    "    kernel = []\n",
    "    A_power = torch.eye(A_bar.shape[0])\n",
    "    \n",
    "    for k in range(L):\n",
    "        K_k = C @ A_power @ B_bar\n",
    "        kernel.append(K_k.item())\n",
    "        A_power = A_power @ A_bar\n",
    "    \n",
    "    return torch.tensor(kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = compute_ssm_kernel(A_demo, B_demo, C_demo, len(u_demo))\n",
    "kernel  # Notice: exponentially decaying!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssm_convolutional(kernel, u_sequence):\n",
    "    \"\"\"\n",
    "    Process sequence via convolution.\n",
    "    This is a causal convolution (only past affects present).\n",
    "    \"\"\"\n",
    "    L = len(u_sequence)\n",
    "    outputs = torch.zeros(L)\n",
    "    \n",
    "    for t in range(L):\n",
    "        # Sum over past inputs weighted by kernel\n",
    "        for k in range(t + 1):\n",
    "            outputs[t] += kernel[k] * u_sequence[t - k]\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_conv = ssm_convolutional(kernel, u_demo)\n",
    "y_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37591c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# They should be identical!\n",
    "torch.allclose(y_recurrent, y_conv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fd04d",
   "metadata": {},
   "source": [
    "### Why Two Modes?\n",
    "\n",
    "**Recurrent mode:**\n",
    "- Complexity: O(L) sequential steps\n",
    "- Training: Slow (can't parallelize across time)  \n",
    "- Inference: Fast (just update state with each new token)\n",
    "\n",
    "**Convolutional mode:**\n",
    "- Complexity: O(L log L) using FFT\n",
    "- Training: Fast (fully parallelizable!)\n",
    "- Inference: Slow (need full sequence to convolve)\n",
    "\n",
    "**The clever trick:** During training, use convolutional mode for parallelism. During inference, use recurrent mode for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8fa849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the kernel - it's like a \"memory window\"\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.bar(range(len(kernel)), kernel)\n",
    "ax.set_xlabel('Lag (how many steps ago)')\n",
    "ax.set_ylabel('Weight')\n",
    "ax.set_title('SSM Kernel: How much past inputs affect current output')\n",
    "ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553cb4e2",
   "metadata": {},
   "source": [
    "## Connection to RNNs: Linear Recurrence\n",
    "\n",
    "Look at the recurrent update again:\n",
    "\n",
    "$$x_k = \\bar{A} x_{k-1} + \\bar{B} u_k$$\n",
    "\n",
    "Compare to an RNN:\n",
    "\n",
    "$$h_k = \\tanh(W_h h_{k-1} + W_x x_k)$$\n",
    "\n",
    "The SSM is like a **linear RNN** (no nonlinearity in the recurrence itself). This seems limiting, but it's actually a feature!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b4c0fb",
   "metadata": {},
   "source": [
    "### Why Linearity is Actually Good\n",
    "\n",
    "**Problem with RNNs:** The tanh squishes gradients. After many steps, gradients either vanish (â†’0) or explode (â†’âˆž).\n",
    "\n",
    "**SSM advantage:** Linear recurrence means we can analyze gradient flow exactly using eigenvalues of A. If all eigenvalues have magnitude < 1, gradients are bounded!\n",
    "\n",
    "Let's see this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a79dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For linear recurrence x_k = A @ x_{k-1}, after n steps:\n",
    "# x_n = A^n @ x_0\n",
    "# Gradient of x_n w.r.t. x_0 is just A^n\n",
    "\n",
    "# If eigenvalues have magnitude < 1, A^n â†’ 0 (bounded, no explosion)\n",
    "# If eigenvalues have magnitude > 1, A^n â†’ âˆž (explosion)\n",
    "\n",
    "A_stable = torch.tensor([[0.9]])  # eigenvalue = 0.9 < 1\n",
    "A_unstable = torch.tensor([[1.1]])  # eigenvalue = 1.1 > 1\n",
    "\n",
    "n_steps = 50\n",
    "powers_stable = [torch.linalg.matrix_power(A_stable, n).item() for n in range(n_steps)]\n",
    "powers_unstable = [torch.linalg.matrix_power(A_unstable, n).item() for n in range(n_steps)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(powers_stable, label='Stable (Î»=0.9)', linewidth=2)\n",
    "ax.plot(powers_unstable, label='Unstable (Î»=1.1)', linewidth=2)\n",
    "ax.set_xlabel('Steps')\n",
    "ax.set_ylabel('A^n')\n",
    "ax.set_title('Gradient Scaling: Stable vs Unstable Systems')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f302e53",
   "metadata": {},
   "source": [
    "### But Wait, Isn't Linear Too Simple?\n",
    "\n",
    "Yes, a single linear SSM is limited. The solution:\n",
    "\n",
    "1. Stack multiple SSM layers with **nonlinearities between them**\n",
    "2. The nonlinearity is outside the recurrence (in the MLP/gating), not inside\n",
    "3. The SSM handles long-range dependencies; MLPs handle local nonlinear mixing\n",
    "\n",
    "This gives us stability of linear systems + expressiveness of deep networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3868d",
   "metadata": {},
   "source": [
    "## The HiPPO Matrix: Learning to Remember\n",
    "\n",
    "We've seen that the A matrix controls how the system remembers. But how should we initialize A?\n",
    "\n",
    "Random initialization works poorly. The breakthrough came from the **HiPPO** (High-order Polynomial Projection Operators) framework.\n",
    "\n",
    "**Key insight:** Instead of hoping the network learns good memory, we can mathematically derive an A matrix that **optimally compresses the history** into the state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242882d9",
   "metadata": {},
   "source": [
    "### The Intuition: Polynomial Approximation\n",
    "\n",
    "Imagine you want to remember a signal f(t) using only N numbers (your state). What's the best way?\n",
    "\n",
    "**Answer:** Project f(t) onto an orthogonal basis (like Legendre polynomials). Store the N coefficients.\n",
    "\n",
    "HiPPO derives the A and B matrices that maintain these optimal projections as new inputs arrive!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9425b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hippo_legs(N):\n",
    "    \"\"\"\n",
    "    Create the HiPPO-LegS (Legendre) matrix.\n",
    "    This matrix optimally compresses history into Legendre polynomial coefficients.\n",
    "    \"\"\"\n",
    "    P = torch.zeros(N, N)\n",
    "    for n in range(N):\n",
    "        for k in range(N):\n",
    "            if n > k:\n",
    "                P[n, k] = (2*n + 1) ** 0.5 * (2*k + 1) ** 0.5\n",
    "            elif n == k:\n",
    "                P[n, k] = n + 1\n",
    "    return -P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e766dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an 8-dimensional HiPPO matrix\n",
    "N = 8\n",
    "A_hippo = make_hippo_legs(N)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(A_hippo, cmap='RdBu', vmin=-10, vmax=10)\n",
    "ax.set_title('HiPPO-LegS Matrix (N=8)')\n",
    "ax.set_xlabel('Input dimension')\n",
    "ax.set_ylabel('Output dimension')\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49afeab5",
   "metadata": {},
   "source": [
    "### Memory Comparison: Random vs HiPPO\n",
    "\n",
    "Let's compare how well different A matrices remember a signal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two different SSMs: random vs HiPPO initialization\n",
    "N = 16\n",
    "dt = 0.01\n",
    "\n",
    "# Random A (scaled to be stable)\n",
    "A_random = torch.randn(N, N) * 0.5\n",
    "\n",
    "# HiPPO A\n",
    "A_hippo = make_hippo_legs(N)\n",
    "\n",
    "# Simple B (all ones, scaled)\n",
    "B = torch.ones(N, 1)\n",
    "\n",
    "# Discretize both\n",
    "A_bar_random = torch.linalg.matrix_exp(A_random * dt)\n",
    "A_bar_hippo = torch.linalg.matrix_exp(A_hippo * dt)\n",
    "# Simplified B discretization for demonstration\n",
    "B_bar = B * dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41526e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a \"spike\" at the beginning, then zeros\n",
    "# A good memory should maintain information about this spike\n",
    "L = 500\n",
    "u = torch.zeros(L)\n",
    "u[0] = 1.0  # impulse at t=0\n",
    "\n",
    "# Run both SSMs\n",
    "x_random = torch.zeros(N)\n",
    "x_hippo = torch.zeros(N)\n",
    "states_random = []\n",
    "states_hippo = []\n",
    "\n",
    "for t in range(L):\n",
    "    x_random = A_bar_random @ x_random + B_bar.squeeze() * u[t]\n",
    "    x_hippo = A_bar_hippo @ x_hippo + B_bar.squeeze() * u[t]\n",
    "    states_random.append(x_random.norm().item())\n",
    "    states_hippo.append(x_hippo.norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(states_random, label='Random A', alpha=0.8)\n",
    "ax.plot(states_hippo, label='HiPPO A', alpha=0.8)\n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('State norm (memory of impulse)')\n",
    "ax.set_title('Memory Decay: HiPPO maintains information longer')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f039a",
   "metadata": {},
   "source": [
    "## S4: Structured State Spaces\n",
    "\n",
    "The HiPPO matrix looks great, but there's a problem: computing the convolution kernel naively is O(NÂ²L) which is expensive.\n",
    "\n",
    "**S4 (Structured State Space)** discovered that if we parameterize A in a special way (DPLR: Diagonal Plus Low-Rank), we can compute the kernel efficiently using the **Cauchy kernel** trick.\n",
    "\n",
    "The key insight: work in the **frequency domain** using complex numbers!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea1c94",
   "metadata": {},
   "source": [
    "### The Diagonal Trick\n",
    "\n",
    "When A is diagonal, the SSM becomes much simpler. Each dimension evolves independently:\n",
    "\n",
    "$$x_k^{(i)} = \\lambda_i \\cdot x_{k-1}^{(i)} + b_i \\cdot u_k$$\n",
    "\n",
    "where $\\lambda_i$ is the i-th diagonal element. The convolution kernel is just geometric series!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagonal SSM: each dimension is independent\n",
    "# Lambda values (complex for oscillation + decay)\n",
    "lambdas = torch.tensor([0.9 + 0.1j, 0.95 - 0.05j, 0.8, 0.99])\n",
    "lambdas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For diagonal A, kernel[k] = C * A^k * B = sum_i (c_i * lambda_i^k * b_i)\n",
    "# This is just a weighted sum of exponentials!\n",
    "\n",
    "L = 100\n",
    "c = torch.ones(4, dtype=torch.cfloat)  # output weights\n",
    "b = torch.ones(4, dtype=torch.cfloat)  # input weights\n",
    "\n",
    "# Compute kernel: K[k] = sum_i c_i * lambda_i^k * b_i\n",
    "k_indices = torch.arange(L)\n",
    "kernel_diagonal = torch.zeros(L, dtype=torch.cfloat)\n",
    "for i in range(4):\n",
    "    kernel_diagonal += c[i] * (lambdas[i] ** k_indices) * b[i]\n",
    "\n",
    "# Take real part for visualization\n",
    "kernel_diagonal = kernel_diagonal.real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e72b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(kernel_diagonal)\n",
    "ax.set_xlabel('Lag')\n",
    "ax.set_ylabel('Kernel value')\n",
    "ax.set_title('Diagonal SSM Kernel: Sum of exponentials (with oscillation from complex eigenvalues)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc511c",
   "metadata": {},
   "source": [
    "## The Selectivity Problem: Why S4 Wasn't Enough\n",
    "\n",
    "S4 achieved impressive results on benchmarks like Long Range Arena. But it struggled with tasks that require **content-aware** processing.\n",
    "\n",
    "The core issue: in S4, the matrices A, B, C are **fixed** for all inputs. The same transformation is applied regardless of whether the input is \"the\" or \"revolutionary\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b82feb",
   "metadata": {},
   "source": [
    "### A Task That Requires Selection: Selective Copying\n",
    "\n",
    "Consider this task: given a sequence with \"markers\", copy only the marked elements.\n",
    "\n",
    "```\n",
    "Input:  [a, b, c, *, d, e, *, f, g]\n",
    "        (where * means \"copy previous\")\n",
    "Output: [_, _, _, c, _, _, e, _, _]\n",
    "```\n",
    "\n",
    "A transformer can do this easilyâ€”just attend to the marked positions. But a fixed SSM treats every position the same!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrating the problem: Transformer attention vs fixed SSM\n",
    "\n",
    "# Transformer can attend selectively\n",
    "print(\"Transformer attention (content-dependent):\")\n",
    "print(\"Token 'copy_marker' â†’ high attention to previous tokens\")\n",
    "print(\"Token 'regular_word' â†’ normal attention pattern\")\n",
    "print()\n",
    "print(\"Fixed SSM (same for all):\")\n",
    "print(\"Token 'copy_marker' â†’ same A, B, C applied\")\n",
    "print(\"Token 'regular_word' â†’ same A, B, C applied\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e2e8cb",
   "metadata": {},
   "source": [
    "## Mamba: The Key Insight\n",
    "\n",
    "Mamba's breakthrough is simple but powerful: **make B, C, and Î” depend on the input!**\n",
    "\n",
    "Instead of:\n",
    "- $B$ = fixed matrix\n",
    "- $C$ = fixed matrix  \n",
    "- $\\Delta$ = fixed step size\n",
    "\n",
    "We have:\n",
    "- $B(x)$ = function of current input\n",
    "- $C(x)$ = function of current input\n",
    "- $\\Delta(x)$ = function of current input\n",
    "\n",
    "This makes the SSM **selective**â€”it can choose what to remember based on context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf17dc",
   "metadata": {},
   "source": [
    "### ðŸ“· Insert Image Here\n",
    "**Paste the Mamba architecture diagram from the paper (Figure 3) showing the selective SSM block**\n",
    "\n",
    "The image should show how B, C, Î” are computed from the input x via linear projections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7379ead",
   "metadata": {},
   "source": [
    "### What Does Selectivity Enable?\n",
    "\n",
    "Think about it intuitively:\n",
    "\n",
    "- **Large Î”(x):** \"This token is important, integrate it strongly into state\"\n",
    "- **Small Î”(x):** \"This token is noise, let state decay without much update\"\n",
    "\n",
    "- **B(x) large:** \"Write this token's information into state\"\n",
    "- **B(x) small:** \"Don't store this token\"\n",
    "\n",
    "- **C(x) large:** \"Read from state to produce output here\"\n",
    "- **C(x) small:** \"Don't need state information for this output\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d20c82",
   "metadata": {},
   "source": [
    "### The Trade-off: No More Convolution Mode\n",
    "\n",
    "There's a catch: when B, C, Î” vary per position, we can't precompute a single kernel. The convolutional trick breaks!\n",
    "\n",
    "**The solution: Parallel Scan**\n",
    "\n",
    "The recurrence $x_k = \\bar{A}_k x_{k-1} + \\bar{B}_k u_k$ looks sequential, but it can actually be parallelized using a technique called **parallel prefix scan** (also used in GPUs for cumulative sums).\n",
    "\n",
    "Key insight: the operation is **associative**, so we can compute it in O(log L) parallel steps instead of O(L) sequential steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c069333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: parallel prefix sum\n",
    "# Sequential: [1, 2, 3, 4] â†’ [1, 1+2, 1+2+3, 1+2+3+4] = [1, 3, 6, 10]\n",
    "# This takes O(n) sequential operations\n",
    "\n",
    "# But we can do it in O(log n) parallel steps!\n",
    "# Step 1: [1, 1+2, 3, 3+4] = [1, 3, 3, 7]\n",
    "# Step 2: [1, 3, 1+3, 3+7] = [1, 3, 6, 10]\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "# Sequential\n",
    "cumsum_seq = x.cumsum(0)\n",
    "print(f\"Sequential cumsum: {cumsum_seq}\")\n",
    "\n",
    "# Note: PyTorch's cumsum is already optimized, but the principle applies to SSM recurrence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b684f",
   "metadata": {},
   "source": [
    "## The Mamba Block Architecture\n",
    "\n",
    "Now let's build the complete Mamba block step by step.\n",
    "\n",
    "**Components:**\n",
    "1. **Input projection**: Expand input dimension\n",
    "2. **Conv1D**: Short local convolution for local context\n",
    "3. **Selective SSM**: The core state space model with input-dependent parameters\n",
    "4. **Gating**: SiLU activation + gating for nonlinearity\n",
    "5. **Output projection**: Return to original dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c25218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our dimensions\n",
    "d_model = 64       # input/output dimension\n",
    "d_inner = 128      # expanded inner dimension (2x d_model is typical)\n",
    "d_state = 16       # SSM state dimension (N)\n",
    "d_conv = 4         # local convolution width\n",
    "dt_rank = 8        # rank for Î” projection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd709fe",
   "metadata": {},
   "source": [
    "### Step 1: Input Projection\n",
    "\n",
    "Like in transformers, we first project to a larger dimension:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake input: batch of 2 sequences, length 10, dim 64\n",
    "# Think of this as embeddings for \"The cat sat on the mat...\"\n",
    "x = torch.randn(2, 10, d_model)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to expanded dimension (split for gating later)\n",
    "in_proj = nn.Linear(d_model, d_inner * 2, bias=False)\n",
    "xz = in_proj(x)\n",
    "xz.shape  # (batch, seq, 2 * d_inner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f49f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: one path goes through SSM, other is used for gating\n",
    "x_ssm, z = xz.chunk(2, dim=-1)\n",
    "x_ssm.shape, z.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e091b96",
   "metadata": {},
   "source": [
    "### Step 2: Short Convolution\n",
    "\n",
    "A 1D convolution captures local patterns before the SSM processes global dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b485ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal 1D convolution (padding to maintain causality)\n",
    "conv1d = nn.Conv1d(\n",
    "    in_channels=d_inner,\n",
    "    out_channels=d_inner,\n",
    "    kernel_size=d_conv,\n",
    "    padding=d_conv - 1,  # causal padding\n",
    "    groups=d_inner  # depthwise = each channel separately\n",
    ")\n",
    "\n",
    "# Conv1d expects (batch, channels, length)\n",
    "x_conv = conv1d(x_ssm.transpose(1, 2))[:, :, :x_ssm.shape[1]]  # trim extra padding\n",
    "x_conv = x_conv.transpose(1, 2)  # back to (batch, length, channels)\n",
    "x_conv.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e755532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SiLU (Swish) activation\n",
    "x_conv = F.silu(x_conv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f5ea1",
   "metadata": {},
   "source": [
    "### Step 3: Selective SSM\n",
    "\n",
    "Now the core of Mamba! We compute B, C, Î” as functions of the input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projections to compute B, C, Î” from input\n",
    "x_dbl_proj = nn.Linear(d_inner, dt_rank + d_state * 2, bias=False)\n",
    "dt_proj = nn.Linear(dt_rank, d_inner, bias=True)\n",
    "\n",
    "# Compute selective parameters\n",
    "x_dbl = x_dbl_proj(x_conv)  # (batch, seq, dt_rank + 2*d_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into delta, B, C\n",
    "delta, B, C = x_dbl.split([dt_rank, d_state, d_state], dim=-1)\n",
    "print(f\"delta shape (before expansion): {delta.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"C shape: {C.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project delta to full inner dimension and apply softplus (ensures positive)\n",
    "delta = dt_proj(delta)  # (batch, seq, d_inner)\n",
    "delta = F.softplus(delta)  # positive step sizes\n",
    "print(f\"delta shape (after expansion): {delta.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A is NOT input-dependent - it's a learnable parameter\n",
    "# Parameterized in log-space for numerical stability\n",
    "A_log = nn.Parameter(torch.log(torch.arange(1, d_state + 1, dtype=torch.float32)))\n",
    "A = -torch.exp(A_log)  # negative for stability\n",
    "A.shape  # (d_state,) - diagonal elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d8a90d",
   "metadata": {},
   "source": [
    "### Step 4: Discretization (Per-Position!)\n",
    "\n",
    "Now we discretize A and B using the input-dependent delta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c17bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize: A_bar = exp(delta * A)\n",
    "# For diagonal A, this is element-wise exp\n",
    "# delta: (batch, seq, d_inner)\n",
    "# A: (d_state,)\n",
    "\n",
    "# We need to broadcast properly\n",
    "# delta_A: (batch, seq, d_inner, d_state)\n",
    "delta_A = delta.unsqueeze(-1) * A.unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "A_bar = torch.exp(delta_A)\n",
    "A_bar.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a85a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified discretization for B (using Euler instead of exact for clarity)\n",
    "# B_bar â‰ˆ delta * B\n",
    "# B: (batch, seq, d_state)\n",
    "# delta: (batch, seq, d_inner)\n",
    "\n",
    "# We need B to be (batch, seq, d_inner, d_state) to match shapes\n",
    "B_bar = delta.unsqueeze(-1) * B.unsqueeze(-2)  # (batch, seq, d_inner, d_state)\n",
    "B_bar.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da3a4e",
   "metadata": {},
   "source": [
    "### Step 5: Run the Selective SSM\n",
    "\n",
    "Now we run the recurrence (simplified version - real Mamba uses parallel scan):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_ssm_recurrent(x_conv, A_bar, B_bar, C):\n",
    "    \"\"\"\n",
    "    Selective SSM forward pass (recurrent mode).\n",
    "    \n",
    "    Args:\n",
    "        x_conv: (batch, seq, d_inner) - input after convolution\n",
    "        A_bar: (batch, seq, d_inner, d_state) - discretized A\n",
    "        B_bar: (batch, seq, d_inner, d_state) - discretized B  \n",
    "        C: (batch, seq, d_state) - output projection\n",
    "    \n",
    "    Returns:\n",
    "        (batch, seq, d_inner) - output\n",
    "    \"\"\"\n",
    "    batch, seq_len, d_inner = x_conv.shape\n",
    "    d_state = A_bar.shape[-1]\n",
    "    \n",
    "    # Initialize state\n",
    "    h = torch.zeros(batch, d_inner, d_state, device=x_conv.device)\n",
    "    outputs = []\n",
    "    \n",
    "    for t in range(seq_len):\n",
    "        # h_new = A_bar[t] * h + B_bar[t] * x[t]\n",
    "        # Note: element-wise for diagonal A\n",
    "        h = A_bar[:, t] * h + B_bar[:, t] * x_conv[:, t, :, None]\n",
    "        \n",
    "        # y = C[t] @ h (for each inner dim)\n",
    "        # h: (batch, d_inner, d_state)\n",
    "        # C: (batch, seq, d_state) -> C[t]: (batch, d_state)\n",
    "        y = (h * C[:, t, None, :]).sum(dim=-1)  # (batch, d_inner)\n",
    "        outputs.append(y)\n",
    "    \n",
    "    return torch.stack(outputs, dim=1)  # (batch, seq, d_inner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ssm = selective_ssm_recurrent(x_conv, A_bar, B_bar, C)\n",
    "y_ssm.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3746c6ff",
   "metadata": {},
   "source": [
    "### Step 6: Gating and Output\n",
    "\n",
    "Finally, we apply gating (multiply with the z branch) and project back:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gating: multiply by silu(z)\n",
    "# This is like the gating in GLU (Gated Linear Unit)\n",
    "y_gated = y_ssm * F.silu(z)\n",
    "y_gated.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bde308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project back to original dimension\n",
    "out_proj = nn.Linear(d_inner, d_model, bias=False)\n",
    "output = out_proj(y_gated)\n",
    "output.shape  # Back to (batch, seq, d_model)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd78c0",
   "metadata": {},
   "source": [
    "### Complete Mamba Block\n",
    "\n",
    "Let's wrap everything into a proper module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.d_inner = int(expand * d_model)\n",
    "        self.dt_rank = max(d_model // 16, 1)\n",
    "        \n",
    "        # Input projection\n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n",
    "        \n",
    "        # Convolution\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            self.d_inner, self.d_inner, \n",
    "            kernel_size=d_conv, padding=d_conv - 1,\n",
    "            groups=self.d_inner\n",
    "        )\n",
    "        \n",
    "        # SSM parameters\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + d_state * 2, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "        \n",
    "        # A in log space (not input-dependent)\n",
    "        self.A_log = nn.Parameter(torch.log(torch.arange(1, d_state + 1, dtype=torch.float32)))\n",
    "        \n",
    "        # D is a skip connection parameter\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        # Input projection and split\n",
    "        xz = self.in_proj(x)\n",
    "        x_ssm, z = xz.chunk(2, dim=-1)\n",
    "        \n",
    "        # Conv1d\n",
    "        x_conv = self.conv1d(x_ssm.transpose(1, 2))[:, :, :seq_len].transpose(1, 2)\n",
    "        x_conv = F.silu(x_conv)\n",
    "        \n",
    "        # Compute selective parameters\n",
    "        x_dbl = self.x_proj(x_conv)\n",
    "        delta, B, C = x_dbl.split([self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "        delta = F.softplus(self.dt_proj(delta))\n",
    "        \n",
    "        # Discretize\n",
    "        A = -torch.exp(self.A_log)\n",
    "        A_bar = torch.exp(delta.unsqueeze(-1) * A)\n",
    "        B_bar = delta.unsqueeze(-1) * B.unsqueeze(-2)\n",
    "        \n",
    "        # SSM recurrence\n",
    "        y = self._ssm_recurrent(x_conv, A_bar, B_bar, C)\n",
    "        \n",
    "        # Skip connection\n",
    "        y = y + x_conv * self.D\n",
    "        \n",
    "        # Gating and output\n",
    "        y = y * F.silu(z)\n",
    "        return self.out_proj(y)\n",
    "    \n",
    "    def _ssm_recurrent(self, x, A_bar, B_bar, C):\n",
    "        batch, seq_len, d_inner = x.shape\n",
    "        h = torch.zeros(batch, d_inner, self.d_state, device=x.device)\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            h = A_bar[:, t] * h + B_bar[:, t] * x[:, t, :, None]\n",
    "            y = (h * C[:, t, None, :]).sum(dim=-1)\n",
    "            outputs.append(y)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe545c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our Mamba block\n",
    "mamba_block = MambaBlock(d_model=64)\n",
    "test_input = torch.randn(2, 10, 64)\n",
    "test_output = mamba_block(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in mamba_block.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471dbc96",
   "metadata": {},
   "source": [
    "### ðŸ§ª Test Problem: Modify the Block\n",
    "\n",
    "Add a RMSNorm before the Mamba block (pre-norm style, like modern transformers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0086c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test problem: Create a pre-normed Mamba block\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Fill in code here: compute RMS norm\n",
    "        # Hint: rms = sqrt(mean(x^2))\n",
    "        # return x / rms * self.weight\n",
    "        pass\n",
    "\n",
    "class PreNormMambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16):\n",
    "        super().__init__()\n",
    "        # Fill in code here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Fill in code here: apply norm, then mamba, then residual connection\n",
    "        pass\n",
    "\n",
    "# Uncomment to test:\n",
    "# test_block = PreNormMambaBlock(64)\n",
    "# test_out = test_block(torch.randn(2, 10, 64))\n",
    "# assert test_out.shape == (2, 10, 64), \"Output shape should match input\"\n",
    "# print(\"âœ“ Pre-normed block working!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401736a",
   "metadata": {},
   "source": [
    "## Building a Complete Mamba Model\n",
    "\n",
    "Now let's build a full Mamba model for text classification, then language modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fcad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return (x / rms) * self.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5592cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaLM(nn.Module):\n",
    "    \"\"\"Complete Mamba Language Model\"\"\"\n",
    "    def __init__(self, vocab_size, d_model, n_layers, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'norm': RMSNorm(d_model),\n",
    "                'mamba': MambaBlock(d_model, d_state, d_conv, expand)\n",
    "            })\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm_f = RMSNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = x + layer['mamba'](layer['norm'](x))\n",
    "        \n",
    "        x = self.norm_f(x)\n",
    "        return self.lm_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b71585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model = MambaLM(vocab_size=10000, d_model=128, n_layers=4)\n",
    "test_ids = torch.randint(0, 10000, (2, 32))  # batch of 2, seq len 32\n",
    "logits = model(test_ids)\n",
    "print(f\"Input: {test_ids.shape}\")\n",
    "print(f\"Output logits: {logits.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3aba6",
   "metadata": {},
   "source": [
    "## Training Mamba: Text Classification\n",
    "\n",
    "Before we tackle language modeling, let's train on a simpler task: text classification with AG News.\n",
    "\n",
    "This will help us verify our implementation works and build intuition for training dynamics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1707315",
   "metadata": {},
   "source": [
    "# Load AG News dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use a simple tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load AG News (4 classes: World, Sports, Business, Sci/Tech)\n",
    "dataset = load_dataset('ag_news')\n",
    "print(f\"Train size: {len(dataset['train'])}\")\n",
    "print(f\"Test size: {len(dataset['test'])}\")\n",
    "print(f\"Classes: {dataset['train'].features['label'].names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e230e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at an example\n",
    "example = dataset['train'][0]\n",
    "print(f\"Text: {example['text'][:200]}...\")\n",
    "print(f\"Label: {example['label']} ({dataset['train'].features['label'].names[example['label']]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "max_length = 128\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "tokenized.set_format('torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaClassifier(nn.Module):\n",
    "    \"\"\"Mamba for sequence classification\"\"\"\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_classes, d_state=16):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'norm': RMSNorm(d_model),\n",
    "                'mamba': MambaBlock(d_model, d_state)\n",
    "            })\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm_f = RMSNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = x + layer['mamba'](layer['norm'](x))\n",
    "        \n",
    "        x = self.norm_f(x)\n",
    "        \n",
    "        # Pool: take the last token's representation (or mean)\n",
    "        if attention_mask is not None:\n",
    "            # Mean pooling over non-padded tokens\n",
    "            mask = attention_mask.unsqueeze(-1).float()\n",
    "            x = (x * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        else:\n",
    "            x = x.mean(dim=1)\n",
    "        \n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae89981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(tokenized['train'], batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(tokenized['test'], batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ded01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "classifier = MambaClassifier(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    n_layers=4,\n",
    "    n_classes=4\n",
    ").to(device)\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in classifier.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{correct/total:.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc8a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few epochs\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=1e-4)\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{n_epochs}\")\n",
    "    train_loss, train_acc = train_epoch(classifier, train_loader, optimizer, device)\n",
    "    test_acc = evaluate(classifier, test_loader, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecbe52f",
   "metadata": {},
   "source": [
    "## Language Modeling with TinyStories\n",
    "\n",
    "Now let's train a proper language model on TinyStories - a dataset of simple children's stories that's perfect for testing language models efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TinyStories dataset\n",
    "tinystories = load_dataset('roneneldan/TinyStories', split='train[:100000]')  # subset for speed\n",
    "print(f\"Loaded {len(tinystories)} stories\")\n",
    "print(f\"\\nExample story:\\n{tinystories[0]['text'][:500]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02322b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPT-2 tokenizer for language modeling\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "lm_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "lm_tokenizer.pad_token = lm_tokenizer.eos_token\n",
    "\n",
    "# Tokenize and create chunks\n",
    "context_length = 256\n",
    "\n",
    "def tokenize_lm(examples):\n",
    "    tokens = lm_tokenizer(examples['text'], truncation=True, max_length=context_length, padding='max_length')\n",
    "    tokens['labels'] = tokens['input_ids'].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized_stories = tinystories.map(tokenize_lm, batched=True, remove_columns=['text'])\n",
    "tokenized_stories.set_format('torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller model for training\n",
    "lm_model = MambaLM(\n",
    "    vocab_size=lm_tokenizer.vocab_size,\n",
    "    d_model=256,\n",
    "    n_layers=6,\n",
    "    d_state=16\n",
    ").to(device)\n",
    "\n",
    "print(f\"LM Parameters: {sum(p.numel() for p in lm_model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training LM')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Shift for next-token prediction\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "        \n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "            ignore_index=lm_tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'ppl': f'{math.exp(loss.item()):.2f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e4ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "lm_loader = DataLoader(tokenized_stories, batch_size=16, shuffle=True)\n",
    "lm_optimizer = torch.optim.AdamW(lm_model.parameters(), lr=3e-4)\n",
    "\n",
    "# Train for a few epochs (increase for better results)\n",
    "for epoch in range(2):\n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    loss = train_lm_epoch(lm_model, lm_loader, lm_optimizer, device)\n",
    "    print(f\"Average Loss: {loss:.4f}, Perplexity: {math.exp(loss):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1895b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=100, temperature=0.8):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some stories!\n",
    "prompts = [\n",
    "    \"Once upon a time, there was a\",\n",
    "    \"The little girl walked into the\",\n",
    "    \"Tom and his dog went to\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generate(lm_model, lm_tokenizer, prompt)}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f66f3",
   "metadata": {},
   "source": [
    "## Dissecting Mamba 1.4B\n",
    "\n",
    "Now let's load a pretrained Mamba model and explore its internals to build deeper intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a189e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mamba-ssm if needed (run in terminal: pip install mamba-ssm)\n",
    "try:\n",
    "    from mamba_ssm import MambaLMHeadModel\n",
    "    MAMBA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"mamba-ssm not installed. Run: pip install mamba-ssm causal-conv1d>=1.1.0\")\n",
    "    MAMBA_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAMBA_AVAILABLE:\n",
    "    # Load the 1.4B model\n",
    "    mamba_1b = MambaLMHeadModel.from_pretrained(\"state-spaces/mamba-1.4b\", device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Loaded Mamba 1.4B\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in mamba_1b.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAMBA_AVAILABLE:\n",
    "    # Explore the model structure\n",
    "    print(\"Model architecture:\")\n",
    "    print(mamba_1b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAMBA_AVAILABLE:\n",
    "    # Look at the A matrix in the first layer\n",
    "    first_layer = mamba_1b.backbone.layers[0].mixer\n",
    "    A_log = first_layer.A_log.detach().cpu()\n",
    "    \n",
    "    print(f\"A_log shape: {A_log.shape}\")\n",
    "    print(f\"A (negative eigenvalues): {-torch.exp(A_log[0, :10])}...\")\n",
    "    \n",
    "    # Visualize the learned A values\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    A_vals = -torch.exp(A_log.mean(dim=0))\n",
    "    ax.bar(range(len(A_vals)), A_vals.numpy())\n",
    "    ax.set_xlabel('State dimension')\n",
    "    ax.set_ylabel('A value (decay rate)')\n",
    "    ax.set_title('Learned A matrix values (averaged over inner dims)')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be94e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAMBA_AVAILABLE:\n",
    "    # Generate text with the pretrained model\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    mamba_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "    \n",
    "    prompt = \"The theory of relativity states that\"\n",
    "    input_ids = mamba_tokenizer(prompt, return_tensors='pt').input_ids.to(mamba_1b.device)\n",
    "    \n",
    "    output = mamba_1b.generate(input_ids, max_length=100, temperature=0.7)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {mamba_tokenizer.decode(output[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd259e3",
   "metadata": {},
   "source": [
    "## Mamba vs Transformers: When to Use What\n",
    "\n",
    "Now that we understand both architectures, let's compare them systematically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430bc30",
   "metadata": {},
   "source": [
    "### Complexity Comparison\n",
    "\n",
    "**Memory:**\n",
    "- Transformer: O(LÂ²) for attention (KV cache grows with sequence)\n",
    "- Mamba: O(1) state per layer (fixed size regardless of sequence length!)\n",
    "\n",
    "**Compute:**\n",
    "- Transformer: O(LÂ² Ã— d) for attention\n",
    "- Mamba: O(L Ã— d Ã— N) where N is state dimension (typically 16)\n",
    "\n",
    "**Training:**\n",
    "- Transformer: Fully parallel (all positions computed simultaneously)\n",
    "- Mamba: Parallel via parallel scan (almost as fast)\n",
    "\n",
    "**Inference:**\n",
    "- Transformer: Need to store full KV cache; each new token is O(L)\n",
    "- Mamba: Just update state; each new token is O(1)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d37f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory usage during inference\n",
    "seq_lengths = torch.arange(100, 10001, 100)\n",
    "d_model = 1024\n",
    "n_layers = 24\n",
    "n_heads = 16\n",
    "d_state = 16\n",
    "\n",
    "# Transformer KV cache: 2 * n_layers * L * d_model (K and V for each layer)\n",
    "transformer_memory = 2 * n_layers * seq_lengths * d_model * 4 / 1e9  # GB (float32)\n",
    "\n",
    "# Mamba state: n_layers * d_model * d_state (fixed!)\n",
    "mamba_memory = torch.ones_like(seq_lengths) * n_layers * d_model * d_state * 4 / 1e9  # GB\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(seq_lengths, transformer_memory, label='Transformer KV Cache', linewidth=2)\n",
    "ax.plot(seq_lengths, mamba_memory, label='Mamba State', linewidth=2)\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Memory (GB)')\n",
    "ax.set_title('Inference Memory: Transformer vs Mamba')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8af0cd",
   "metadata": {},
   "source": [
    "### Qualitative Differences\n",
    "\n",
    "**Transformers excel at:**\n",
    "- **Retrieval**: Looking up specific information from context (\"What did John say about X?\")\n",
    "- **In-context learning**: Learning patterns from few examples in the prompt\n",
    "- **Copying**: Reproducing exact sequences from input\n",
    "- **Precise attention**: When you need to focus on specific tokens\n",
    "\n",
    "**Mamba excels at:**\n",
    "- **Compression**: Summarizing long contexts into useful representations\n",
    "- **Long-range dependencies**: When information must flow across very long sequences\n",
    "- **Efficiency**: Especially for long sequences and real-time generation\n",
    "- **Streaming**: Processing continuous input streams\n",
    "\n",
    "**The intuition:** Transformers \"store and retrieve\" while Mamba \"compresses and flows\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f74d2",
   "metadata": {},
   "source": [
    "## Hybrid Architectures: Best of Both Worlds\n",
    "\n",
    "Given the complementary strengths, why not combine them? This is exactly what hybrid architectures like **Jamba** and **Zamba2** do.\n",
    "\n",
    "### Design Patterns\n",
    "\n",
    "**1. Interleaved:** Alternate Mamba and attention layers\n",
    "```\n",
    "Mamba â†’ Mamba â†’ Attention â†’ Mamba â†’ Mamba â†’ Attention â†’ ...\n",
    "```\n",
    "\n",
    "**2. Parallel:** Run both in parallel and combine outputs\n",
    "```\n",
    "x â†’ [Mamba(x) + Attention(x)] â†’ ...\n",
    "```\n",
    "\n",
    "**3. Hierarchical:** Use Mamba for local, attention for global\n",
    "```\n",
    "Mamba(local) â†’ Pool â†’ Attention(global) â†’ Unpool â†’ Mamba(local)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Simplified attention for hybrid model\"\"\"\n",
    "    def __init__(self, d_model, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, L, 3, self.n_heads, self.head_dim)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # (3, B, H, L, D)\n",
    "        \n",
    "        # Causal attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()\n",
    "        attn = attn.masked_fill(mask, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, L, D)\n",
    "        return self.proj(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridBlock(nn.Module):\n",
    "    \"\"\"A block that uses either Mamba or Attention based on layer index\"\"\"\n",
    "    def __init__(self, d_model, use_attention=False, d_state=16, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        if use_attention:\n",
    "            self.mixer = SimpleAttention(d_model, n_heads)\n",
    "        else:\n",
    "            self.mixer = MambaBlock(d_model, d_state)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.mixer(self.norm(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridLM(nn.Module):\n",
    "    \"\"\"Hybrid Mamba + Attention Language Model (like Zamba/Jamba)\"\"\"\n",
    "    def __init__(self, vocab_size, d_model, n_layers, attention_every=4, d_state=16, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Interleave: every `attention_every` layers, use attention\n",
    "        self.layers = nn.ModuleList([\n",
    "            HybridBlock(\n",
    "                d_model, \n",
    "                use_attention=(i % attention_every == attention_every - 1),\n",
    "                d_state=d_state,\n",
    "                n_heads=n_heads\n",
    "            )\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm_f = RMSNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight  # tie weights\n",
    "        \n",
    "        # Count layers\n",
    "        n_mamba = sum(1 for l in self.layers if isinstance(l.mixer, MambaBlock))\n",
    "        n_attn = sum(1 for l in self.layers if isinstance(l.mixer, SimpleAttention))\n",
    "        print(f\"Hybrid model: {n_mamba} Mamba layers, {n_attn} Attention layers\")\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm_f(x)\n",
    "        return self.lm_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the hybrid model\n",
    "hybrid = HybridLM(vocab_size=50257, d_model=256, n_layers=12, attention_every=4)\n",
    "print(f\"Parameters: {sum(p.numel() for p in hybrid.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e050da6",
   "metadata": {},
   "source": [
    "## Training a Hybrid Model with W&B Logging\n",
    "\n",
    "Now let's train a larger hybrid model with proper monitoring using Weights & Biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Configuration for training\n",
    "config = {\n",
    "    'vocab_size': 50257,\n",
    "    'd_model': 512,\n",
    "    'n_layers': 16,\n",
    "    'attention_every': 4,  # 1 attention layer every 4 layers\n",
    "    'd_state': 16,\n",
    "    'n_heads': 8,\n",
    "    'batch_size': 8,\n",
    "    'gradient_accumulation_steps': 4,  # effective batch = 32\n",
    "    'learning_rate': 3e-4,\n",
    "    'warmup_steps': 500,\n",
    "    'max_steps': 5000,\n",
    "    'context_length': 512,\n",
    "    'mixed_precision': True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79094745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B\n",
    "wandb.init(project='mamba-hybrid', config=config)\n",
    "\n",
    "# Create model\n",
    "hybrid_model = HybridLM(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    d_model=config['d_model'],\n",
    "    n_layers=config['n_layers'],\n",
    "    attention_every=config['attention_every'],\n",
    "    d_state=config['d_state'],\n",
    "    n_heads=config['n_heads']\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in hybrid_model.parameters()):,}\")\n",
    "wandb.watch(hybrid_model, log='all', log_freq=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154881fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step, warmup_steps, max_lr, max_steps):\n",
    "    \"\"\"Cosine learning rate schedule with warmup\"\"\"\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * step / warmup_steps\n",
    "    progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    return max_lr * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "def log_mamba_specific_metrics(model, step):\n",
    "    \"\"\"Log metrics specific to Mamba layers\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if isinstance(layer.mixer, MambaBlock):\n",
    "            # Log A matrix statistics (decay rates)\n",
    "            A = -torch.exp(layer.mixer.A_log.detach())\n",
    "            metrics[f'mamba_layer_{i}/A_mean'] = A.mean().item()\n",
    "            metrics[f'mamba_layer_{i}/A_min'] = A.min().item()\n",
    "            metrics[f'mamba_layer_{i}/A_max'] = A.max().item()\n",
    "            \n",
    "            # Log D (skip connection) statistics\n",
    "            D = layer.mixer.D.detach()\n",
    "            metrics[f'mamba_layer_{i}/D_mean'] = D.mean().item()\n",
    "    \n",
    "    wandb.log(metrics, step=step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91796e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "train_loader_hybrid = DataLoader(tokenized_stories, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(hybrid_model.parameters(), lr=config['learning_rate'], weight_decay=0.1)\n",
    "\n",
    "# Mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler() if config['mixed_precision'] and device.type == 'cuda' else None\n",
    "\n",
    "# Training loop\n",
    "global_step = 0\n",
    "accumulation_steps = config['gradient_accumulation_steps']\n",
    "hybrid_model.train()\n",
    "\n",
    "pbar = tqdm(total=config['max_steps'], desc='Training Hybrid')\n",
    "data_iter = iter(train_loader_hybrid)\n",
    "\n",
    "while global_step < config['max_steps']:\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for micro_step in range(accumulation_steps):\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(train_loader_hybrid)\n",
    "            batch = next(data_iter)\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = hybrid_model(input_ids)\n",
    "                shift_logits = logits[:, :-1, :].contiguous()\n",
    "                shift_labels = labels[:, 1:].contiguous()\n",
    "                loss = F.cross_entropy(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1),\n",
    "                    ignore_index=lm_tokenizer.pad_token_id\n",
    "                ) / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            logits = hybrid_model(input_ids)\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=lm_tokenizer.pad_token_id\n",
    "            ) / accumulation_steps\n",
    "            loss.backward()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr = get_lr(global_step, config['warmup_steps'], config['learning_rate'], config['max_steps'])\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # Gradient clipping and step\n",
    "    if scaler:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(hybrid_model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        torch.nn.utils.clip_grad_norm_(hybrid_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Logging\n",
    "    if global_step % 10 == 0:\n",
    "        wandb.log({\n",
    "            'train/loss': total_loss,\n",
    "            'train/perplexity': math.exp(total_loss),\n",
    "            'train/lr': lr,\n",
    "        }, step=global_step)\n",
    "    \n",
    "    if global_step % 100 == 0:\n",
    "        log_mamba_specific_metrics(hybrid_model, global_step)\n",
    "    \n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix({'loss': f'{total_loss:.4f}', 'ppl': f'{math.exp(total_loss):.2f}'})\n",
    "    global_step += 1\n",
    "\n",
    "pbar.close()\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae714aa3",
   "metadata": {},
   "source": [
    "## Productionizing Mamba: Monitoring & Best Practices\n",
    "\n",
    "Training Mamba models has some unique considerations compared to Transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97fe0d",
   "metadata": {},
   "source": [
    "### Mamba-Specific Monitoring\n",
    "\n",
    "**What to watch in Mamba that differs from Transformers:**\n",
    "\n",
    "1. **A matrix eigenvalues**: The decay rates control memory. If they drift too close to 0, the model \"forgets everything instantly\". If too close to 1, state might explode.\n",
    "\n",
    "2. **Delta (Î”) distribution**: The step sizes should have healthy variance. If all Î” collapse to the same value, selectivity is lost.\n",
    "\n",
    "3. **State norm**: Unlike Transformer activations, Mamba state accumulates over time. Monitor for explosion or collapse.\n",
    "\n",
    "4. **D (skip connection)**: If D dominates, the SSM isn't doing much work.\n",
    "\n",
    "**What's the same:**\n",
    "- Loss curves, gradient norms, learning rate schedules\n",
    "- Weight distributions and activation statistics\n",
    "- Dead neurons in MLPs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021524ff",
   "metadata": {},
   "source": [
    "### Training Stability Tricks\n",
    "\n",
    "**Mamba-specific:**\n",
    "- **Initialize A in log-space**: Prevents negative eigenvalues (instability)\n",
    "- **Softplus for Î”**: Ensures positive step sizes\n",
    "- **Bounded A initialization**: Start with reasonable decay rates (e.g., 1 to N)\n",
    "\n",
    "**General (apply to both):**\n",
    "- **Pre-norm**: Apply normalization before each block (more stable than post-norm)\n",
    "- **Gradient clipping**: Clip to 1.0 to prevent explosion\n",
    "- **Weight decay**: 0.1 is typical for language models\n",
    "- **Learning rate warmup**: Crucial for stability at start\n",
    "\n",
    "**Compute efficiency:**\n",
    "- **Mixed precision (FP16/BF16)**: 2x memory savings, faster compute\n",
    "- **Gradient accumulation**: Simulate larger batches on limited memory\n",
    "- **Gradient checkpointing**: Trade compute for memory on very long sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with our trained hybrid model\n",
    "hybrid_model.eval()\n",
    "print(\"Generating with hybrid model:\\n\")\n",
    "\n",
    "for prompt in [\"Once upon a time\", \"The scientist discovered\"]:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    output = generate(hybrid_model, lm_tokenizer, prompt, max_new_tokens=50)\n",
    "    print(f\"Output: {output}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8f008",
   "metadata": {},
   "source": [
    "## Cool Intuitions & Cross-Domain Connections\n",
    "\n",
    "### SSMs as Learnable Filters\n",
    "From signal processing: SSMs are essentially learnable infinite impulse response (IIR) filters. The kernel we computed earlier is the impulse response. Different A matrices create different filter characteristics (low-pass, band-pass, etc.).\n",
    "\n",
    "### Connection to Differential Equations\n",
    "Mamba's continuous SSM is literally a neural ODE! The discretization step connects to numerical methods like Euler and Runge-Kutta.\n",
    "\n",
    "### Biological Plausibility  \n",
    "Neurons in the brain can be modeled as dynamical systems with state. The selectivity mechanism mirrors how biological neurons gate information based on context.\n",
    "\n",
    "### The Compression vs Retrieval Tradeoff\n",
    "- **Transformers**: Store everything (like a database), retrieve via attention\n",
    "- **Mamba**: Compress continuously (like a summary), no retrieval needed\n",
    "\n",
    "This explains why Transformers dominate tasks requiring precise recall (\"What was the 5th word?\") while Mamba excels at tasks requiring integration (\"What's the overall sentiment?\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de47076",
   "metadata": {},
   "source": [
    "## Summary: What We Learned\n",
    "\n",
    "1. **State Space Models** come from control theoryâ€”they compress sequence history into a fixed-size state\n",
    "\n",
    "2. **The key equations**: $x_k = \\bar{A}x_{k-1} + \\bar{B}u_k$ and $y_k = Cx_k$\n",
    "\n",
    "3. **Two computation modes**: Recurrent (O(n) sequential) and Convolutional (O(n log n) parallel)\n",
    "\n",
    "4. **HiPPO** provides optimal memory initialization for the A matrix\n",
    "\n",
    "5. **The selectivity problem**: Fixed SSM parameters can't do content-aware processing\n",
    "\n",
    "6. **Mamba's insight**: Make B, C, Î” input-dependent! Use parallel scan to stay efficient\n",
    "\n",
    "7. **Hybrid models** combine Mamba's efficiency with attention's retrieval capability\n",
    "\n",
    "8. **Production considerations**: Monitor A eigenvalues, Î” distribution, and state norms\n",
    "\n",
    "The field is rapidly evolvingâ€”Mamba2, Griffin, and new architectures continue to push the boundaries of efficient sequence modeling!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4e987",
   "metadata": {},
   "source": [
    "### ðŸ“· Insert Image Here\n",
    "**Suggested: Mamba paper Figure 1 showing the architecture comparison between Transformer and Mamba**\n",
    "\n",
    "URL: https://arxiv.org/abs/2312.00752 (Figure 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536cde7f",
   "metadata": {},
   "source": [
    "## References & Further Reading\n",
    "\n",
    "**Papers:**\n",
    "- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) - The original Mamba paper\n",
    "- [Efficiently Modeling Long Sequences with Structured State Spaces (S4)](https://arxiv.org/abs/2111.00396) - The S4 breakthrough\n",
    "- [HiPPO: Recurrent Memory with Optimal Polynomial Projections](https://arxiv.org/abs/2008.07669) - The memory theory\n",
    "- [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887) - AI21's hybrid approach\n",
    "- [Zamba: A Compact 7B SSM Hybrid Model](https://arxiv.org/abs/2405.16712) - Efficient hybrid design\n",
    "\n",
    "**Code:**\n",
    "- [state-spaces/mamba](https://github.com/state-spaces/mamba) - Official Mamba implementation\n",
    "- [HazyResearch/safari](https://github.com/HazyResearch/safari) - S4 and SSM research code\n",
    "\n",
    "**Tutorials:**\n",
    "- [The Annotated S4](https://srush.github.io/annotated-s4/) - Detailed walkthrough of S4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd65938",
   "metadata": {},
   "source": [
    "https://developer.nvidia.com/blog/inside-nvidia-nemotron-3-techniques-tools-and-data-that-make-it-efficient-and-accurate/?utm_source=tldrai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8fb6f6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
