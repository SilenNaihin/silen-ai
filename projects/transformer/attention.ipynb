{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "273a6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,torch\n",
    "from torch import nn\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add workspace root to Python path so we can import silen_lib\n",
    "workspace_root = Path.cwd().parent.parent\n",
    "if str(workspace_root) not in sys.path:\n",
    "    sys.path.insert(0, str(workspace_root))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from diffusers.models.attention_processor import Attention\n",
    "\n",
    "import silen_lib.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bd330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_seed(42)\n",
    "# Here, we simulate the output from a convolutional block (not a raw image), \n",
    "# which is why num_channels (C) is 32 instead of 1 or 3 as with input images.\n",
    "x = torch.randn(64,32,16,16) # N (batch size), C (channels from conv feature map), H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8174db82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x shape: torch.Size([64, 32, 16, 16])\n",
      "After view and transpose, t shape: torch.Size([64, 256, 32])\n"
     ]
    }
   ],
   "source": [
    "# The code below reshapes the 4D tensor of shape (batch_size, channels, height, width)\n",
    "# into a 3D tensor suitable for attention modules.\n",
    "# - x.shape[:2] unpacks (N, C)\n",
    "# - .view(*x.shape[:2], -1) collapses H*W into the last dimension: (N, C, H*W)\n",
    "# - .transpose(1, 2) swaps the channels and \"sequence\" axes to: (N, H*W, C)\n",
    "#\n",
    "# This is done to treat each spatial location as an element in a sequence,\n",
    "# with the \"embedding\" size being the channels.\n",
    "\n",
    "# Example:\n",
    "# Suppose x.shape == (64, 32, 16, 16)\n",
    "# After `.view(*x.shape[:2], -1)`, shape is (64, 32, 256)\n",
    "# After `.transpose(1, 2)`, shape is (64, 256, 32)\n",
    "\n",
    "t = x.view(*x.shape[:2], -1).transpose(1, 2)\n",
    "print(\"Original x shape:\", x.shape)\n",
    "print(\"After view and transpose, t shape:\", t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51245dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f10cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = nn.Linear(ni, ni)\n",
    "sq = nn.Linear(ni, ni)\n",
    "sv = nn.Linear(ni, ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15237a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = sk(t)\n",
    "q = sq(t)\n",
    "v = sv(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d34cd0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q@k.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15062786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.GroupNorm(1, ni)\n",
    "        self.q = nn.Linear(ni, ni)\n",
    "        self.k = nn.Linear(ni, ni)\n",
    "        self.v = nn.Linear(ni, ni)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        n,c,h,w = x.shape\n",
    "        x = self.norm(x)\n",
    "        x = x.view(n, c, -1).transpose(1, 2)\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb48706",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SelfAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fb4ae6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra = sa(x)\n",
    "ra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce69830a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9104,  1.4186,  0.8385, -2.1584,  0.6318, -1.2443, -0.0789, -1.6844,\n",
       "        -0.7939,  1.6117, -0.3852, -1.4307, -0.7494, -0.6010, -0.8335,  0.7477],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1a380fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_parms(a,b):\n",
    "    b.weight = a.weight\n",
    "    b.bias = a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc6e3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Attention class API:\n",
    "# - Uses query_dim instead of just a single channels arg\n",
    "# - heads=1, dim_head=32 for single-head attention matching our implementation\n",
    "# - norm_num_groups=1 for group normalization\n",
    "# - residual_connection=True to add input to output\n",
    "at = Attention(\n",
    "    query_dim=32,\n",
    "    heads=1, \n",
    "    dim_head=32,\n",
    "    norm_num_groups=1,\n",
    "    residual_connection=True,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "# New attribute names: to_q, to_k, to_v (instead of query, key, value)\n",
    "# and to_out[0] for the projection (it's a ModuleList)\n",
    "src = sa.q, sa.k, sa.v, sa.proj, sa.norm\n",
    "dst = at.to_q, at.to_k, at.to_v, at.to_out[0], at.group_norm\n",
    "for s,d in zip(src,dst): cp_parms(s,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bfc0087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9104,  1.4186,  0.8385, -2.1584,  0.6318, -1.2443, -0.0789, -1.6844,\n",
       "        -0.7939,  1.6117, -0.3852, -1.4307, -0.7494, -0.6010, -0.8335,  0.7477],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The new Attention class expects hidden_states (same as x)\n",
    "rb = at(x)\n",
    "rb[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a4f25e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 96])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqkv = nn.Linear(ni, ni*3)\n",
    "st = sqkv(t)\n",
    "st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64df0786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,k,v = torch.chunk(st, 3, dim=-1)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afdd291d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(k@q.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cde31928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        n,c,h,w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        q,k,v = torch.chunk(self.qkv(x), 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1caa223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x).transpose(1, 2)\n",
    "        q,k,v = torch.chunk(self.qkv(x), 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        return self.proj(x).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67202ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32768x16 and 32x96)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sa \u001b[38;5;241m=\u001b[39m SelfAttention(\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m sa(x)\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     q,k,v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x), \u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     s \u001b[38;5;241m=\u001b[39m (q\u001b[38;5;129m@k\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;129m@v\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32768x16 and 32x96)"
     ]
    }
   ],
   "source": [
    "sa = SelfAttention(32)\n",
    "sa(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf0c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa(x).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a167b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heads_to_batch(x, heads):\n",
    "    n,sl,d = x.shape\n",
    "    x = x.reshape(n, sl, heads, -1)\n",
    "    return x.transpose(2, 1).reshape(n*heads,sl,-1)\n",
    "\n",
    "def batch_to_heads(x, heads):\n",
    "    n,sl,d = x.shape\n",
    "    x = x.reshape(-1, heads, sl, d)\n",
    "    return x.transpose(2, 1).reshape(-1,sl,d*heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11734bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c3466d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 32]), torch.Size([512, 256, 4]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = rearrange(t , 'n s (h d) -> (n h) s d', h=8)\n",
    "t.shape, t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64105f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = rearrange(t2, '(n h) s d -> n s (h d)', h=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f34fabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 256, 4]), torch.Size([64, 256, 32]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape,t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4739d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t==t3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ced6c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, ni, nheads):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        # Scale factor for attention scores to help with stable gradients\n",
    "        # (division by sqrt of dimension per head)\n",
    "        self.scale = math.sqrt(ni/nheads)\n",
    "        # Normalize input on channel dimension for more stable and performant training\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        # Linear to compute all queries, keys, values at once\n",
    "        # Output has 3x input feature dimension: for q, k, v concatenated\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        # Linear projection for output after attention mechanism\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        n, c, h, w = inp.shape  # n=batch, c=channels, h & w = spatial dims\n",
    "        # Apply normalization, then flatten spatial dims for attention\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)  # shape: (n, hw, c)\n",
    "        # Compute queries, keys, values (all at once)\n",
    "        x = self.qkv(x)  # shape: (n, sequence_len, c*3)\n",
    "        # Rearrange so that 'n' and 'nheads' are combined into one dimension, \n",
    "        # splitting the (c*3) inner dimension into (number of heads x features per head).\n",
    "        # einops notation: \n",
    "        #    - n: batch size\n",
    "        #    - s: sequence length (here hw)\n",
    "        #    - h: number of heads\n",
    "        #    - d: features per head (should be c // nheads)\n",
    "        # Reshape for multi-head: (n, s, h*d) --> (n*h, s, d)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        # Split heads into Q, K, V on last dimension (each of shape (n*h, s, d))\n",
    "        q, k, v = torch.chunk(x, 3, dim=-1)\n",
    "        # Compute scaled dot-product attention scores\n",
    "        # k.transpose(1,2) changes (n*h, s, d) -> (n*h, d, s)\n",
    "        # Resulting s: (n*h, s, s), attention map for each head & batch element\n",
    "        s = (q @ k.transpose(1, 2)) / self.scale\n",
    "        # Apply softmax so each row of attention adds up to 1 (probability distribution)\n",
    "        # then multiply by v to get weighted representations\n",
    "        x = s.softmax(dim=-1) @ v  # (n*h, s, d)\n",
    "        # Rearrange back to (batch, sequence, features): (n*h, s, d) -> (n, s, h*d)\n",
    "        # This undoes the earlier reshape for multi-head\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        # Final linear projection, then reshape back to original (n, c, h, w)\n",
    "        # .transpose(1,2): (n, s, c) -> (n, c, s). .reshape(n, c, h, w) maps back to 4D\n",
    "        x = self.proj(x).transpose(1, 2).reshape(n, c, h, w)\n",
    "        # Add residual connection to preserve input information\n",
    "        return x + inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SelfAttentionMultiHead(32, 4)\n",
    "sx = sa(x)\n",
    "sx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c46b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx.mean(),sx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b3f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)\n",
    "nmx,nmw = nm(t,t,t)\n",
    "nmx = nmx+t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "451f2d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0015, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0034, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmx.mean(),nmx.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b6f93",
   "metadata": {},
   "source": [
    "Thoughts\n",
    "- Has someone played around with the ratio of MLP params to attention params?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82998fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
