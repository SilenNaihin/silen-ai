{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e49174d",
   "metadata": {},
   "source": [
    "# From Hopfield Networks to Boltzmann Machines\n",
    "\n",
    "**The Big Question**: What if neural networks could \"remember\" patterns and complete them from partial information?\n",
    "\n",
    "Standard neural networks learn to map inputs → outputs. But what if we want something different: given a *corrupted* or *partial* pattern, recover the original? This is what your brain does constantly—you see half a face and instantly recall the whole person.\n",
    "\n",
    "Hopfield networks and Boltzmann machines approach this through a beautiful idea borrowed from physics: **energy minimization**. Just as a ball rolls downhill to find the lowest point, these networks evolve their state to minimize an \"energy\" function—and the stored memories sit at the valleys.\n",
    "\n",
    "This notebook builds intuition from scratch, culminating in training a production-scale Boltzmann machine and connecting to modern transformer attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Add parent directory to path for our utils\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from silen_lib.utils import utils\n",
    "\n",
    "utils.set_seed(42)\n",
    "\n",
    "# Use MPS if available (Apple Silicon), otherwise CUDA, otherwise CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2d480",
   "metadata": {},
   "source": [
    "## Why Energy? The Physics Intuition\n",
    "\n",
    "Before we touch any neural network code, let's understand **why** physicists discovered that thinking about \"energy\" is powerful.\n",
    "\n",
    "Consider a ball on a curved surface. You don't need to compute forces, accelerations, or differential equations to predict where it will end up. You just know: **it will roll to the lowest point**.\n",
    "\n",
    "This is profound. The universe seems to \"want\" to minimize energy. Water flows downhill. Hot things cool down. Stretched springs contract. Nature finds the path of least resistance.\n",
    "\n",
    "The question that leads to Hopfield networks: **Can we design a system where \"good\" states (stored memories) have low energy, so the system naturally evolves toward them?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see energy minimization in action\n",
    "# A simple energy landscape: E(x) = x^4 - 2x^2 (has two valleys)\n",
    "\n",
    "x = torch.linspace(-2, 2, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c905f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(x):\n",
    "    return x**4 - 2*x**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184fd4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient tells us which direction is \"downhill\"\n",
    "def gradient(x):\n",
    "    return 4*x**3 - 4*x  # derivative of x^4 - 2x^2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch a \"ball\" roll downhill from a random starting point\n",
    "# This is gradient descent: move opposite to gradient\n",
    "\n",
    "def simulate_ball(start_pos, lr=0.1, steps=30):\n",
    "    \"\"\"Simulate a ball rolling down the energy landscape.\"\"\"\n",
    "    positions = [start_pos]\n",
    "    pos = start_pos\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        grad = gradient(pos)\n",
    "        pos = pos - lr * grad  # move opposite to gradient\n",
    "        positions.append(pos)\n",
    "    \n",
    "    return torch.tensor(positions)\n",
    "\n",
    "# Start from x = 0.3 (slightly right of the peak)\n",
    "trajectory = simulate_ball(start_pos=0.3)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(x.numpy(), energy(x).numpy(), 'b-', linewidth=2, label='Energy landscape')\n",
    "ax.scatter(trajectory.numpy(), energy(trajectory).numpy(), c=range(len(trajectory)), \n",
    "           cmap='Reds', s=100, zorder=5)\n",
    "ax.scatter([trajectory[0]], [energy(trajectory[0])], c='green', s=200, marker='o', label='Start', zorder=6)\n",
    "ax.scatter([trajectory[-1]], [energy(trajectory[-1])], c='red', s=200, marker='*', label='End', zorder=6)\n",
    "ax.axhline(y=-1, color='gray', linestyle='--', alpha=0.5, label='Minimum energy = -1')\n",
    "ax.set_xlabel('State x')\n",
    "ax.set_ylabel('Energy E(x)')\n",
    "ax.set_title('A ball rolls to the nearest valley (local minimum)')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Started at x = {trajectory[0]:.3f}, ended at x = {trajectory[-1]:.3f}\")\n",
    "print(f\"Energy went from {energy(trajectory[0]):.3f} to {energy(trajectory[-1]):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37538373",
   "metadata": {},
   "source": [
    "**Key insight**: The ball doesn't \"know\" calculus. It just follows the local slope. Yet it finds a minimum.\n",
    "\n",
    "Notice it went to x ≈ 1 (the right valley), not x ≈ -1 (the left valley). Starting at x = 0.3, it rolled to the *nearest* valley. This is both a feature and a limitation we'll revisit.\n",
    "\n",
    "**The deep why**: Energy minimization works because:\n",
    "1. It's a **scalar function** — one number summarizes \"how good\" a state is\n",
    "2. **Gradients point toward improvement** — we always know which direction is better\n",
    "3. **Fixed points are stable** — once at a minimum, small perturbations return you there\n",
    "\n",
    "This is exactly what we want for memory: stored patterns should be stable attractors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if we start exactly at the peak (x=0)?\n",
    "trajectory_peak = simulate_ball(start_pos=0.0)\n",
    "trajectory_peak[-1]  # Stays at 0! Unstable equilibrium.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d838959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But add a tiny perturbation...\n",
    "trajectory_perturbed = simulate_ball(start_pos=0.001)\n",
    "trajectory_perturbed[-1]  # Falls into the right valley!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08c236",
   "metadata": {},
   "source": [
    "The peak at x=0 has zero gradient, but it's **unstable** — the slightest push sends the ball rolling. Valleys are **stable** — push a little, it returns. This distinction between stable and unstable fixed points is crucial for memory.\n",
    "\n",
    "---\n",
    "\n",
    "## The Memory Problem\n",
    "\n",
    "Let's make this concrete. Imagine you want to store some patterns (like images of digits) in a network, and later **recover** them from noisy or partial versions.\n",
    "\n",
    "First, let's create some simple 5×5 binary patterns representing digits:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary patterns: +1 (white) and -1 (black)\n",
    "# These are simplified 5x5 \"images\" of digits\n",
    "\n",
    "# Pattern for \"0\" - a ring shape\n",
    "pattern_0 = torch.tensor([\n",
    "    [-1, +1, +1, +1, -1],\n",
    "    [+1, -1, -1, -1, +1],\n",
    "    [+1, -1, -1, -1, +1],\n",
    "    [+1, -1, -1, -1, +1],\n",
    "    [-1, +1, +1, +1, -1]\n",
    "], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern for \"1\" - a vertical line\n",
    "pattern_1 = torch.tensor([\n",
    "    [-1, -1, +1, -1, -1],\n",
    "    [-1, -1, +1, -1, -1],\n",
    "    [-1, -1, +1, -1, -1],\n",
    "    [-1, -1, +1, -1, -1],\n",
    "    [-1, -1, +1, -1, -1]\n",
    "], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern for \"X\" - diagonal cross\n",
    "pattern_x = torch.tensor([\n",
    "    [+1, -1, -1, -1, +1],\n",
    "    [-1, +1, -1, +1, -1],\n",
    "    [-1, -1, +1, -1, -1],\n",
    "    [-1, +1, -1, +1, -1],\n",
    "    [+1, -1, -1, -1, +1]\n",
    "], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06499f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def show_patterns(patterns, titles=None):\n",
    "    \"\"\"Display binary patterns as images.\"\"\"\n",
    "    n = len(patterns)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(3*n, 3))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (ax, pattern) in enumerate(zip(axes, patterns)):\n",
    "        ax.imshow(pattern.numpy(), cmap='gray', vmin=-1, vmax=1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if titles:\n",
    "            ax.set_title(titles[i], fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "patterns = [pattern_0, pattern_1, pattern_x]\n",
    "show_patterns(patterns, titles=['Pattern \"0\"', 'Pattern \"1\"', 'Pattern \"X\"'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba8fe07",
   "metadata": {},
   "source": [
    "Now here's the problem we want to solve: what if we receive a **corrupted** version of one of these patterns?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd943655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def corrupt_pattern(pattern, noise_fraction=0.2):\n",
    "    \"\"\"Flip a fraction of bits randomly to simulate noise/corruption.\"\"\"\n",
    "    flat = pattern.flatten().clone()\n",
    "    n_flip = int(len(flat) * noise_fraction)\n",
    "    flip_idx = torch.randperm(len(flat))[:n_flip]\n",
    "    flat[flip_idx] *= -1  # flip the sign\n",
    "    return flat.view(pattern.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b91ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrupt the \"0\" pattern with 20% noise\n",
    "corrupted_0 = corrupt_pattern(pattern_0, noise_fraction=0.2)\n",
    "\n",
    "show_patterns([pattern_0, corrupted_0], titles=['Original \"0\"', 'Corrupted (20% noise)'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649de2a6",
   "metadata": {},
   "source": [
    "Can you still tell it's a \"0\"? Probably. Your brain does pattern completion automatically.\n",
    "\n",
    "**The Hopfield idea**: Create an energy landscape where:\n",
    "- Each stored pattern sits at a **valley** (local minimum)\n",
    "- Corrupted versions have **higher energy**\n",
    "- Running dynamics will \"roll\" the corrupted pattern into the nearest valley\n",
    "\n",
    "The corrupted \"0\" should have higher energy than the clean \"0\", but lower than clean \"1\" or \"X\" (since it's closer to \"0\").\n",
    "\n",
    "---\n",
    "\n",
    "## Hebbian Learning: Building the Weight Matrix\n",
    "\n",
    "How do we construct an energy function where our patterns are minima?\n",
    "\n",
    "Before diving into the math, let's think about what we need. Imagine each neuron as a person at a party. We want to encode \"rules\" about who should agree:\n",
    "\n",
    "- **Rule 1**: If neurons i and j are both ON (+1) in our memory, they're \"friends\" — they should tend to be ON together\n",
    "- **Rule 2**: If neuron i is ON but j is OFF (-1), they're \"enemies\" — they should tend to disagree\n",
    "- **Rule 3**: The rules should stack — if multiple memories agree that i and j should be friends, the friendship is stronger\n",
    "\n",
    "The key insight comes from neuroscience: **\"Neurons that fire together, wire together.\"** (Hebb's rule, 1949)\n",
    "\n",
    "**The elegant solution**: Make the weight between neurons i and j equal to the **product** of their values:\n",
    "- Both +1? Product = +1 (positive weight = friends)\n",
    "- Both -1? Product = +1 (negative × negative = positive = friends)\n",
    "- One +1, one -1? Product = -1 (negative weight = enemies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny example: 3 neurons with pattern [+1, -1, +1]\n",
    "tiny_pattern = torch.tensor([1., -1., 1.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outer product captures \"who should agree with whom\"\n",
    "torch.outer(tiny_pattern, tiny_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba85e0",
   "metadata": {},
   "source": [
    "**Reading this 3×3 matrix — why the outer product is genius:**\n",
    "\n",
    "The outer product $\\xi \\xi^T$ gives us $W_{ij} = \\xi_i \\cdot \\xi_j$. Let's trace through:\n",
    "\n",
    "- W[0,2] = (+1)(+1) = +1: neurons 0 and 2 are both +1, they should agree ✓\n",
    "- W[0,1] = (+1)(-1) = -1: neuron 0 is +1 but neuron 1 is -1, they disagree ✓\n",
    "- W[1,2] = (-1)(+1) = -1: neuron 1 is -1 but neuron 2 is +1, they disagree ✓\n",
    "\n",
    "**The math captures exactly what we want**: The sign of the product tells us whether neurons should agree (+) or disagree (-).\n",
    "\n",
    "The diagonal W[i,i] = ξ_i² = 1 always (since ±1 squared = 1). This represents \"should neuron i agree with itself?\" which is meaningless, so we zero it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_tiny = torch.outer(tiny_pattern, tiny_pattern)\n",
    "W_tiny.fill_diagonal_(0)\n",
    "W_tiny\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c124a445",
   "metadata": {},
   "source": [
    "For **multiple patterns**, we just add up the outer products. Each pattern contributes its preferences, and they accumulate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten our patterns to 1D vectors (25 neurons for 5x5)\n",
    "p0 = pattern_0.flatten()\n",
    "p1 = pattern_1.flatten()  \n",
    "px = pattern_x.flatten()\n",
    "\n",
    "print(f\"Each pattern is a vector of {len(p0)} neurons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def build_hopfield_weights(patterns):\n",
    "    \"\"\"\n",
    "    Build weight matrix from a list of patterns using Hebbian learning.\n",
    "    W = (1/N) * sum of outer products, with zero diagonal.\n",
    "    \"\"\"\n",
    "    n_neurons = patterns[0].numel()\n",
    "    W = torch.zeros(n_neurons, n_neurons)\n",
    "    \n",
    "    for p in patterns:\n",
    "        flat = p.flatten()\n",
    "        W += torch.outer(flat, flat)\n",
    "    \n",
    "    W /= n_neurons  # normalize by number of neurons\n",
    "    W.fill_diagonal_(0)  # no self-connections\n",
    "    \n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec30379",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = build_hopfield_weights([pattern_0, pattern_1, pattern_x])\n",
    "W.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fffa588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weight matrix - it encodes all our patterns!\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(W.numpy(), cmap='RdBu', vmin=-W.abs().max(), vmax=W.abs().max())\n",
    "plt.colorbar(label='Weight')\n",
    "plt.title('Hopfield Weight Matrix (25×25)\\nRed = neurons should agree, Blue = should disagree')\n",
    "plt.xlabel('Neuron j')\n",
    "plt.ylabel('Neuron i')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1657df04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Hopfield Energy Function\n",
    "\n",
    "Now comes the crucial step: defining an energy that's **low when neurons agree with their weights**.\n",
    "\n",
    "### Deriving the Energy from First Principles\n",
    "\n",
    "We want an energy function where:\n",
    "1. Following the rules (friends agree, enemies disagree) = **low energy** = stable\n",
    "2. Breaking the rules = **high energy** = unstable\n",
    "\n",
    "Let's build it step by step.\n",
    "\n",
    "**Step 1: Single pair contribution**\n",
    "\n",
    "For neurons i and j, we want to reward \"correct\" configurations:\n",
    "- If W_ij > 0 (they should agree) and they DO agree (same sign) → reward this\n",
    "- If W_ij < 0 (they should disagree) and they DO disagree (opposite signs) → reward this\n",
    "\n",
    "The product $W_{ij} \\cdot x_i \\cdot x_j$ does exactly this:\n",
    "- If W_ij = +1 (agree) and x_i = x_j = +1: contribution = +1 (good!)\n",
    "- If W_ij = +1 (agree) and x_i = +1, x_j = -1: contribution = -1 (bad!)\n",
    "- If W_ij = -1 (disagree) and x_i = +1, x_j = -1: contribution = (-1)(+1)(-1) = +1 (good!)\n",
    "\n",
    "**Step 2: Sum over all pairs**\n",
    "\n",
    "Total \"happiness\" = $\\sum_{i,j} W_{ij} x_i x_j = \\mathbf{x}^T W \\mathbf{x}$\n",
    "\n",
    "**Step 3: Flip sign (energy = negative happiness)**\n",
    "\n",
    "By convention, we want LOW energy to be GOOD, so:\n",
    "\n",
    "$$E(\\mathbf{x}) = -\\frac{1}{2} \\mathbf{x}^T W \\mathbf{x}$$\n",
    "\n",
    "The 1/2 is because each pair (i,j) appears twice in the sum (once as i,j and once as j,i since W is symmetric).\n",
    "\n",
    "### Why this works visually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2444cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize WHY stored patterns have low energy\n",
    "# Each cell W_ij * x_i * x_j contributes to the energy sum\n",
    "\n",
    "def visualize_energy_contributions(pattern, W, title=\"\"):\n",
    "    \"\"\"Show which neuron pairs contribute positively/negatively to energy.\"\"\"\n",
    "    flat = pattern.flatten()\n",
    "    n = len(flat)\n",
    "    \n",
    "    # Compute contribution matrix: -W_ij * x_i * x_j (note the negative for energy)\n",
    "    contributions = torch.zeros(n, n)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            contributions[i, j] = -W[i, j] * flat[i] * flat[j]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Pattern\n",
    "    axes[0].imshow(pattern.view(5, 5).numpy(), cmap='gray', vmin=-1, vmax=1)\n",
    "    axes[0].set_title(f'Pattern\\n{title}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Contribution matrix (blue = lowers energy, red = raises energy)\n",
    "    im = axes[1].imshow(contributions.numpy(), cmap='RdBu', \n",
    "                        vmin=-contributions.abs().max(), vmax=contributions.abs().max())\n",
    "    axes[1].set_title('Energy contributions\\nBlue = lowers E (good), Red = raises E (bad)')\n",
    "    axes[1].set_xlabel('Neuron j')\n",
    "    axes[1].set_ylabel('Neuron i')\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    \n",
    "    # Histogram\n",
    "    axes[2].hist(contributions.flatten().numpy(), bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[2].axvline(x=0, color='r', linestyle='--')\n",
    "    axes[2].set_xlabel('Energy contribution')\n",
    "    axes[2].set_ylabel('Count')\n",
    "    total_E = contributions.sum().item() / 2  # divide by 2 for double counting\n",
    "    axes[2].set_title(f'Total Energy = {total_E:.2f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare stored pattern vs corrupted\n",
    "visualize_energy_contributions(pattern_0, W, \"Stored '0' (should be LOW energy)\")\n",
    "visualize_energy_contributions(corrupted_0, W, \"Corrupted '0' (should be HIGHER energy)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eae2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def hopfield_energy(state, W):\n",
    "    \"\"\"Compute energy E(x) = -0.5 * x^T W x\"\"\"\n",
    "    flat = state.flatten()\n",
    "    return -0.5 * flat @ W @ flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy of stored patterns (should be low)\n",
    "hopfield_energy(pattern_0, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786828d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy of corrupted pattern (should be higher)\n",
    "hopfield_energy(corrupted_0, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare energies\n",
    "print(\"Stored patterns (should be low):\")\n",
    "print(f\"  Pattern 0: {hopfield_energy(pattern_0, W):.4f}\")\n",
    "print(f\"  Pattern 1: {hopfield_energy(pattern_1, W):.4f}\")\n",
    "print(f\"  Pattern X: {hopfield_energy(pattern_x, W):.4f}\")\n",
    "print(f\"\\nCorrupted pattern 0 (20% noise): {hopfield_energy(corrupted_0, W):.4f}\")\n",
    "print(f\"\\nRandom noise: {hopfield_energy(torch.sign(torch.randn(5, 5)), W):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c5d1ff",
   "metadata": {},
   "source": [
    "Stored patterns have lower energy than corrupted or random states! The energy landscape has valleys at our memories.\n",
    "\n",
    "---\n",
    "\n",
    "## Dynamics: Making the Network \"Think\"\n",
    "\n",
    "Now we need dynamics that **always decrease energy** (or keep it same). This is like letting the ball roll downhill.\n",
    "\n",
    "### The Local Field: Democratic Voting Among Neurons\n",
    "\n",
    "Imagine you're neuron $i$, trying to decide: should I be +1 or -1?\n",
    "\n",
    "You poll each neighbor $j$ who casts a **weighted vote**:\n",
    "\n",
    "$$h_i = \\sum_j W_{ij} x_j$$\n",
    "\n",
    "**Breaking this down:**\n",
    "\n",
    "- Neighbor $j$ is currently in state $x_j$ (either +1 or -1)\n",
    "- Your connection weight $W_{ij}$ says how much you should agree (+) or disagree (-) with $j$\n",
    "- The vote from $j$ is: $W_{ij} \\cdot x_j$\n",
    "\n",
    "**Example votes:**\n",
    "- $W_{ij} = +1$ (should agree), $x_j = +1$ → vote = +1 (\"be +1 like me!\")\n",
    "- $W_{ij} = +1$ (should agree), $x_j = -1$ → vote = -1 (\"be -1 like me!\")  \n",
    "- $W_{ij} = -1$ (should disagree), $x_j = +1$ → vote = -1 (\"be opposite to me!\")\n",
    "- $W_{ij} = -1$ (should disagree), $x_j = -1$ → vote = +1 (\"be opposite to me!\")\n",
    "\n",
    "**The update rule**: Follow the consensus! Set $x_i = \\text{sign}(h_i)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def local_field(state, W):\n",
    "    \"\"\"\n",
    "    Compute the local field h = W @ x for each neuron.\n",
    "    \n",
    "    h_i > 0 means neighbors vote for neuron i to be +1\n",
    "    h_i < 0 means neighbors vote for neuron i to be -1\n",
    "    \"\"\"\n",
    "    return W @ state.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the network \"want\" the corrupted pattern to become?\n",
    "h = local_field(corrupted_0, W)\n",
    "h.view(5, 5)  # reshape to see it as an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbafb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the local field as \"what neighbors want\"\n",
    "def visualize_local_field(state, W, pattern_name=\"\"):\n",
    "    \"\"\"Show the local field h and how it guides updates.\"\"\"\n",
    "    h = local_field(state, W)\n",
    "    recommended = torch.sign(h)  # what the network \"wants\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(14, 3.5))\n",
    "    \n",
    "    # Current state\n",
    "    axes[0].imshow(state.view(5, 5).numpy(), cmap='gray', vmin=-1, vmax=1)\n",
    "    axes[0].set_title('Current State')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Local field (what neighbors want)\n",
    "    im = axes[1].imshow(h.view(5, 5).numpy(), cmap='RdBu', \n",
    "                        vmin=-h.abs().max(), vmax=h.abs().max())\n",
    "    axes[1].set_title('Local Field h\\\\n(Red=vote -1, Blue=vote +1)')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1], shrink=0.8)\n",
    "    \n",
    "    # Recommended next state\n",
    "    axes[2].imshow(recommended.view(5, 5).numpy(), cmap='gray', vmin=-1, vmax=1)\n",
    "    axes[2].set_title('sign(h)\\\\n(What network recommends)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Difference (what would flip)\n",
    "    diff = (state.flatten() != recommended).float().view(5, 5)\n",
    "    axes[3].imshow(diff.numpy(), cmap='Reds', vmin=0, vmax=1)\n",
    "    axes[3].set_title('Would flip\\\\n(Red = yes)')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Local Field Voting - {pattern_name}', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_local_field(corrupted_0, W, \"Corrupted '0'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d203919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The update rule: align with the sign of the local field\n",
    "torch.sign(h).view(5, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2532ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def hopfield_update(state, W, async_update=True, max_steps=100):\n",
    "    \"\"\"\n",
    "    Run Hopfield dynamics until convergence.\n",
    "    \n",
    "    async_update=True: Update one random neuron at a time (guaranteed to converge)\n",
    "    async_update=False: Update all neurons simultaneously (faster but may oscillate)\n",
    "    \"\"\"\n",
    "    state = state.flatten().clone()\n",
    "    n = len(state)\n",
    "    energies = [hopfield_energy(state, W).item()]\n",
    "    states = [state.clone()]\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if async_update:\n",
    "            # Pick a random neuron and update it\n",
    "            i = torch.randint(0, n, (1,)).item()\n",
    "            h_i = W[i] @ state\n",
    "            new_val = 1.0 if h_i >= 0 else -1.0\n",
    "            if state[i] == new_val:\n",
    "                continue  # no change\n",
    "            state[i] = new_val\n",
    "        else:\n",
    "            # Update all neurons at once\n",
    "            h = W @ state\n",
    "            new_state = torch.sign(h)\n",
    "            new_state[new_state == 0] = 1  # tie-break\n",
    "            if torch.all(state == new_state):\n",
    "                break\n",
    "            state = new_state\n",
    "        \n",
    "        energies.append(hopfield_energy(state, W).item())\n",
    "        states.append(state.clone())\n",
    "    \n",
    "    return state, energies, states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24beee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the corrupted \"0\" pattern!\n",
    "recovered, energies, states = hopfield_update(corrupted_0, W, async_update=False)\n",
    "\n",
    "show_patterns([corrupted_0, recovered.view(5, 5), pattern_0], \n",
    "              titles=['Corrupted input', 'Recovered', 'Original target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae919fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy always decreases (or stays same) - this is the Lyapunov function!\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(energies, 'b-o', markersize=8)\n",
    "plt.xlabel('Update step')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('Energy Decreases Until Pattern is Recovered')\n",
    "plt.axhline(y=hopfield_energy(pattern_0, W).item(), color='g', linestyle='--', label='Target pattern energy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2318f2b",
   "metadata": {},
   "source": [
    "### Why Does Energy Always Decrease? (The Lyapunov Proof)\n",
    "\n",
    "This is beautiful and worth understanding deeply. Let's derive it step by step.\n",
    "\n",
    "**Setup**: We're updating neuron $i$. Before update, its value is $x_i$. After update, it becomes $x_i^{\\text{new}} = \\text{sign}(h_i)$.\n",
    "\n",
    "**Step 1: Energy only depends on neuron i through terms involving i**\n",
    "\n",
    "$$E = -\\frac{1}{2} \\sum_{j,k} W_{jk} x_j x_k$$\n",
    "\n",
    "The terms involving neuron $i$ are: $-\\frac{1}{2} \\sum_{j \\neq i} W_{ij} x_i x_j - \\frac{1}{2} \\sum_{j \\neq i} W_{ji} x_j x_i = -\\sum_{j \\neq i} W_{ij} x_i x_j = -x_i h_i$\n",
    "\n",
    "(The factor of 2 comes from W being symmetric: W_ij = W_ji)\n",
    "\n",
    "**Step 2: Energy change when we flip neuron i**\n",
    "\n",
    "$$\\Delta E = E^{\\text{new}} - E^{\\text{old}} = -x_i^{\\text{new}} h_i - (-x_i^{\\text{old}} h_i) = (x_i^{\\text{old}} - x_i^{\\text{new}}) h_i$$\n",
    "\n",
    "**Step 3: Our update rule guarantees non-increase**\n",
    "\n",
    "Our rule sets $x_i^{\\text{new}} = \\text{sign}(h_i)$. This means:\n",
    "- If $h_i > 0$: we set $x_i^{\\text{new}} = +1$, so $x_i^{\\text{new}} h_i > 0$\n",
    "- If $h_i < 0$: we set $x_i^{\\text{new}} = -1$, so $x_i^{\\text{new}} h_i > 0$ (negative × negative)\n",
    "- If $h_i = 0$: energy doesn't change\n",
    "\n",
    "In all cases, $x_i^{\\text{new}} h_i \\geq x_i^{\\text{old}} h_i$, so $\\Delta E \\leq 0$. Energy never increases!\n",
    "\n",
    "**Why this matters**: This is a **Lyapunov function** — a quantity that monotonically decreases under dynamics. It guarantees:\n",
    "1. The system will eventually stop changing (convergence)\n",
    "2. It will stop at a local minimum of energy\n",
    "3. That minimum is a stored pattern (or spurious state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7956cc",
   "metadata": {},
   "source": [
    "### Test Problem: Implement Asynchronous Update\n",
    "\n",
    "The synchronous update (all neurons at once) can oscillate in certain cases. Asynchronous update (one neuron at a time) is guaranteed to converge. Complete the function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f01f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_async_update_step(test_state, test_W, test_neuron_idx):\n",
    "    \"\"\"\n",
    "    Perform ONE asynchronous update step on a single neuron.\n",
    "    \n",
    "    Args:\n",
    "        test_state: Current state vector (1D tensor)\n",
    "        test_W: Weight matrix\n",
    "        test_neuron_idx: Which neuron to update\n",
    "    \n",
    "    Returns:\n",
    "        Updated state (modified in place is fine)\n",
    "    \"\"\"\n",
    "    # FILL IN CODE HERE\n",
    "    # 1. Compute the local field h_i = sum_j W[i,j] * x[j]\n",
    "    # 2. Set state[i] = +1 if h_i >= 0, else -1\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test: after updating, energy should not increase\n",
    "test_state = corrupted_0.flatten().clone()\n",
    "test_orig_energy = hopfield_energy(test_state, W).item()\n",
    "\n",
    "test_async_update_step(test_state, W, test_neuron_idx=0)\n",
    "\n",
    "test_new_energy = hopfield_energy(test_state, W).item()\n",
    "# Uncomment to check: assert test_new_energy <= test_orig_energy + 1e-6, \"Energy should not increase!\"\n",
    "# print(\"✓ Passed: Energy did not increase\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71554f87",
   "metadata": {},
   "source": [
    "### Animated Recovery\n",
    "\n",
    "Let's watch the network \"think\" step by step as it recovers a heavily corrupted pattern:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heavily corrupt pattern X (40% noise)\n",
    "corrupted_x = corrupt_pattern(pattern_x, noise_fraction=0.4)\n",
    "\n",
    "# Recover using asynchronous updates (many steps since we update one neuron at a time)\n",
    "recovered_x, energies_x, states_x = hopfield_update(corrupted_x, W, async_update=True, max_steps=500)\n",
    "\n",
    "# Show key frames of the recovery\n",
    "n_frames = min(8, len(states_x))\n",
    "frame_indices = [int(i * (len(states_x) - 1) / (n_frames - 1)) for i in range(n_frames)]\n",
    "\n",
    "fig, axes = plt.subplots(1, n_frames, figsize=(2.5 * n_frames, 3))\n",
    "for ax, idx in zip(axes, frame_indices):\n",
    "    ax.imshow(states_x[idx].view(5, 5).numpy(), cmap='gray', vmin=-1, vmax=1)\n",
    "    ax.set_title(f'Step {idx}\\nE={energies_x[idx]:.2f}', fontsize=10)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Pattern Recovery Animation (40% noise → Original X)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed441e47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Capacity and Spurious States\n",
    "\n",
    "How many patterns can a Hopfield network store? This is crucial for practical use.\n",
    "\n",
    "**Theoretical result (Amit, Gutfreund, Sompolinsky, 1985)**: For N neurons, you can reliably store at most ~**0.14N** random patterns.\n",
    "\n",
    "Beyond this, the network develops **spurious states** — false memories that weren't stored but appear as local minima. These are like hallucinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39246afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our 25-neuron network, theoretical capacity is ~0.14 * 25 ≈ 3.5 patterns\n",
    "# We stored 3, which is within capacity. Let's try overloading it.\n",
    "\n",
    "n_neurons = 64  # 8x8 patterns\n",
    "capacity_limit = int(0.14 * n_neurons)\n",
    "print(f\"For {n_neurons} neurons, theoretical capacity: ~{capacity_limit} patterns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c579ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_capacity(n_neurons, n_patterns, noise_fraction=0.2, n_trials=20):\n",
    "    \"\"\"Test how well a Hopfield network recovers patterns.\"\"\"\n",
    "    # Generate random binary patterns\n",
    "    patterns = [torch.sign(torch.randn(n_neurons)) for _ in range(n_patterns)]\n",
    "    \n",
    "    # Build weight matrix\n",
    "    W = torch.zeros(n_neurons, n_neurons)\n",
    "    for p in patterns:\n",
    "        W += torch.outer(p, p)\n",
    "    W /= n_neurons\n",
    "    W.fill_diagonal_(0)\n",
    "    \n",
    "    # Test recovery\n",
    "    successes = 0\n",
    "    for _ in range(n_trials):\n",
    "        # Pick a random pattern and corrupt it\n",
    "        idx = torch.randint(0, n_patterns, (1,)).item()\n",
    "        original = patterns[idx]\n",
    "        corrupted = original.clone()\n",
    "        flip_idx = torch.randperm(n_neurons)[:int(n_neurons * noise_fraction)]\n",
    "        corrupted[flip_idx] *= -1\n",
    "        \n",
    "        # Recover\n",
    "        recovered, _, _ = hopfield_update(corrupted, W, async_update=False, max_steps=50)\n",
    "        \n",
    "        # Check if recovered matches original\n",
    "        if torch.all(recovered == original):\n",
    "            successes += 1\n",
    "    \n",
    "    return successes / n_trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of patterns\n",
    "n_neurons = 100\n",
    "pattern_counts = [1, 5, 10, 14, 20, 30, 50]  # 0.14 * 100 = 14\n",
    "\n",
    "accuracies = []\n",
    "for n_patterns in pattern_counts:\n",
    "    acc = test_capacity(n_neurons, n_patterns, noise_fraction=0.1)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"{n_patterns:3d} patterns: {acc*100:.0f}% recovery\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the capacity cliff\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(pattern_counts, accuracies, 'bo-', markersize=10, linewidth=2)\n",
    "plt.axvline(x=0.14*n_neurons, color='r', linestyle='--', label=f'Theoretical limit (0.14N = {0.14*n_neurons:.0f})')\n",
    "plt.xlabel('Number of Stored Patterns')\n",
    "plt.ylabel('Recovery Accuracy')\n",
    "plt.title(f'Hopfield Network Capacity (N={n_neurons} neurons)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932fd9c1",
   "metadata": {},
   "source": [
    "Notice the sharp drop around the theoretical limit! Beyond capacity, the network hallucinates spurious patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison to Neural Networks\n",
    "\n",
    "How does the Hopfield network differ from a standard feedforward neural network?\n",
    "\n",
    "**Feedforward NN**: Layers, forward pass, one-shot computation x → f(x), trained with gradient descent.\n",
    "\n",
    "**Hopfield Network**: Single recurrent layer, iterate until equilibrium, Hebbian learning (one-shot storage).\n",
    "\n",
    "The key conceptual difference: feedforward NNs **compute** answers, while Hopfield networks **settle** into them. It's the difference between calculating 2+2=4 versus recognizing that \"4\" feels right.\n",
    "\n",
    "**Autoregressive models** (like GPT) are somewhere in between: they generate one token at a time through forward passes, but the sequential nature creates implicit dynamics. Interestingly, modern Hopfield networks reveal that attention mechanisms can be viewed as a single energy minimization step — we'll see this later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909acf61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Problem: Getting Stuck in Local Minima\n",
    "\n",
    "Hopfield networks have a critical flaw: they always go \"downhill\" in energy. This greedy behavior means they can get stuck in spurious local minima (false memories) instead of finding the true stored pattern.\n",
    "\n",
    "Think about it: if you're in a small dip, you can't see there's a deeper valley nearby because every direction looks \"uphill.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd197a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An energy landscape with a local minimum trap\n",
    "x = torch.linspace(-3, 3, 300)\n",
    "energy_with_trap = 0.5 * x**4 - x**3 - 2*x**2 + 3*x\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x.numpy(), energy_with_trap.numpy(), 'b-', linewidth=2)\n",
    "plt.scatter([0.5], [0.5*0.5**4 - 0.5**3 - 2*0.5**2 + 3*0.5], c='red', s=200, zorder=5, label='Local minimum (trap)')\n",
    "plt.scatter([-1.8], [0.5*(-1.8)**4 - (-1.8)**3 - 2*(-1.8)**2 + 3*(-1.8)], c='green', s=200, zorder=5, label='Global minimum (true memory)')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('Greedy descent gets trapped in local minima')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30d110",
   "metadata": {},
   "source": [
    "**The solution from physics**: Add temperature!\n",
    "\n",
    "At temperature T > 0, particles don't just sit at minimum energy. They jiggle around randomly, and occasionally \"jump\" uphill. Higher temperature = more randomness = can escape traps.\n",
    "\n",
    "This is the **Boltzmann distribution** from statistical mechanics:\n",
    "\n",
    "$$P(\\text{state}) \\propto e^{-E(\\text{state}) / T}$$\n",
    "\n",
    "- At T → 0: Only the lowest energy state has non-zero probability (deterministic)\n",
    "- At T → ∞: All states equally likely (pure randomness)\n",
    "- At intermediate T: Lower energy states more likely, but can still sample higher energy states\n",
    "\n",
    "Let's visualize this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74479967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple energy landscape: E = x^2 (single minimum at 0)\n",
    "x = torch.linspace(-3, 3, 200)\n",
    "E = x**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83accda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boltzmann distribution at different temperatures\n",
    "def boltzmann_prob(E, T):\n",
    "    \"\"\"P(x) ∝ exp(-E/T), normalized.\"\"\"\n",
    "    unnorm = torch.exp(-E / T)\n",
    "    return unnorm / unnorm.sum()\n",
    "\n",
    "temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Energy landscape\n",
    "axes[0].plot(x.numpy(), E.numpy(), 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('State x')\n",
    "axes[0].set_ylabel('Energy')\n",
    "axes[0].set_title('Energy Landscape E(x) = x²')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Probability distributions at different T\n",
    "colors = plt.cm.coolwarm(np.linspace(0, 1, len(temperatures)))\n",
    "for T, color in zip(temperatures, colors):\n",
    "    prob = boltzmann_prob(E, T)\n",
    "    axes[1].plot(x.numpy(), prob.numpy(), linewidth=2, color=color, label=f'T={T}')\n",
    "\n",
    "axes[1].set_xlabel('State x')\n",
    "axes[1].set_ylabel('Probability')\n",
    "axes[1].set_title('Boltzmann Distribution: P(x) ∝ exp(-E/T)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff985b",
   "metadata": {},
   "source": [
    "At low temperature (T=0.1, blue), probability is sharply concentrated at the minimum. At high temperature (T=5, red), it's spread out — the system explores more states.\n",
    "\n",
    "**Simulated annealing**: Start hot (explore widely), gradually cool down (settle into minimum). This is how Boltzmann machines escape local minima.\n",
    "\n",
    "---\n",
    "\n",
    "## Boltzmann Machines: Stochastic Hopfield Networks\n",
    "\n",
    "### Where Does the Sigmoid Come From?\n",
    "\n",
    "This derivation is beautiful and connects physics to neural networks. Let's build it from scratch.\n",
    "\n",
    "**The Physical Setup**: We want to sample from the Boltzmann distribution:\n",
    "\n",
    "$$P(\\text{state } \\mathbf{x}) \\propto e^{-E(\\mathbf{x})/T}$$\n",
    "\n",
    "**The Question**: Given that we're updating neuron $i$, what's the probability it should be +1 vs -1?\n",
    "\n",
    "**Step 1: Conditional probability**\n",
    "\n",
    "We want $P(x_i = +1 | \\text{all other neurons})$. By Bayes' rule, this is proportional to $e^{-E(\\mathbf{x}^{+})/T}$ where $\\mathbf{x}^{+}$ is the state with $x_i = +1$.\n",
    "\n",
    "**Step 2: Energy difference**\n",
    "\n",
    "From our earlier derivation, the terms involving $x_i$ contribute $-x_i h_i$ to the energy.\n",
    "\n",
    "- If $x_i = +1$: contribution = $-h_i$\n",
    "- If $x_i = -1$: contribution = $+h_i$\n",
    "\n",
    "**Step 3: The ratio**\n",
    "\n",
    "$$\\frac{P(x_i = +1)}{P(x_i = -1)} = \\frac{e^{-(-h_i)/T}}{e^{-(+h_i)/T}} = \\frac{e^{h_i/T}}{e^{-h_i/T}} = e^{2h_i/T}$$\n",
    "\n",
    "**Step 4: Normalize to get probability**\n",
    "\n",
    "Since $P(+1) + P(-1) = 1$:\n",
    "\n",
    "$$P(x_i = +1) = \\frac{e^{h_i/T}}{e^{h_i/T} + e^{-h_i/T}} = \\frac{1}{1 + e^{-2h_i/T}}$$\n",
    "\n",
    "With a rescaling of T, this is exactly the **sigmoid function**: $\\sigma(h_i/T) = \\frac{1}{1 + e^{-h_i/T}}$\n",
    "\n",
    "**The stunning insight**: The sigmoid function isn't arbitrary — it's the ONLY function that samples correctly from the Boltzmann distribution!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid at different temperatures\n",
    "h = torch.linspace(-5, 5, 200)  # local field values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for T, color in zip([0.2, 0.5, 1.0, 2.0, 5.0], plt.cm.coolwarm(np.linspace(0, 1, 5))):\n",
    "    prob = torch.sigmoid(h / T)\n",
    "    ax.plot(h.numpy(), prob.numpy(), linewidth=2, color=color, label=f'T={T}')\n",
    "\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Local field $h_i$')\n",
    "ax.set_ylabel('P($x_i$ = +1)')\n",
    "ax.set_title('Stochastic neuron firing probability at different temperatures')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0dcb67",
   "metadata": {},
   "source": [
    "### Understanding Temperature: A Unifying Concept\n",
    "\n",
    "Temperature T appears everywhere in ML, often hidden under different names:\n",
    "\n",
    "| Concept | Where It Appears | Effect |\n",
    "|---------|-----------------|--------|\n",
    "| **Hopfield T** | Boltzmann machine | Exploration vs exploitation |\n",
    "| **Softmax temperature** | Transformer attention | Sharpness of attention |\n",
    "| **Sampling temperature** | LLM generation | Creativity vs coherence |\n",
    "| **Annealing schedule** | Optimization | Escaping local minima |\n",
    "\n",
    "**The Deep Why**: All of these are the SAME concept from statistical mechanics:\n",
    "\n",
    "$$P(\\text{state}) \\propto e^{-E(\\text{state})/T}$$\n",
    "\n",
    "- **T → 0**: Only the minimum energy state has probability (deterministic/greedy)\n",
    "- **T → ∞**: All states equally likely (random)\n",
    "- **Finite T**: Explore states proportional to their \"quality\"\n",
    "\n",
    "**This is exploration vs exploitation!** High T explores (might find better solutions), low T exploits (commits to current best). The optimal strategy is often to start hot and cool down (annealing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ad9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of temperature on sampling\n",
    "# Low T → greedy/deterministic, High T → random exploration\n",
    "\n",
    "def sample_many_states(start_state, W, T, n_samples=20, steps_per_sample=100):\n",
    "    \"\"\"Sample multiple final states at a given temperature.\"\"\"\n",
    "    samples = []\n",
    "    for _ in range(n_samples):\n",
    "        final, _ = boltzmann_update(start_state.clone(), W, T=T, n_steps=steps_per_sample)\n",
    "        samples.append(final)\n",
    "    return torch.stack(samples)\n",
    "\n",
    "# Start from random state\n",
    "random_start = torch.sign(torch.randn(25))\n",
    "\n",
    "# Sample at different temperatures\n",
    "temps = [0.1, 0.5, 1.0, 3.0]\n",
    "n_show = 8\n",
    "\n",
    "fig, axes = plt.subplots(len(temps), n_show + 1, figsize=(3*(n_show+1), 3*len(temps)))\n",
    "\n",
    "for row, T in enumerate(temps):\n",
    "    samples = sample_many_states(random_start, W, T, n_samples=n_show, steps_per_sample=200)\n",
    "    \n",
    "    # Show temperature label\n",
    "    axes[row, 0].text(0.5, 0.5, f'T={T}', fontsize=16, ha='center', va='center', fontweight='bold')\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # Show samples\n",
    "    for col in range(n_show):\n",
    "        axes[row, col+1].imshow(samples[col].view(5, 5).numpy(), cmap='gray', vmin=-1, vmax=1)\n",
    "        \n",
    "        # Check if it matches any stored pattern\n",
    "        matches = []\n",
    "        for name, pattern in [(\"0\", pattern_0), (\"1\", pattern_1), (\"X\", pattern_x)]:\n",
    "            if torch.all(samples[col].view(5, 5) == pattern):\n",
    "                matches.append(name)\n",
    "        \n",
    "        title = f\"→{matches[0]}\" if matches else \"spurious\"\n",
    "        axes[row, col+1].set_title(title, fontsize=10)\n",
    "        axes[row, col+1].axis('off')\n",
    "\n",
    "plt.suptitle('Boltzmann Sampling at Different Temperatures\\\\n'\n",
    "             'Low T: Always finds stored patterns (but can get stuck)\\\\n'\n",
    "             'High T: Explores more (but may not settle)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e720262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def boltzmann_update(state, W, T=1.0, n_steps=100):\n",
    "    \"\"\"\n",
    "    Stochastic Boltzmann machine update.\n",
    "    Neurons flip probabilistically based on local field and temperature.\n",
    "    \n",
    "    The key insight: instead of x_i = sign(h_i), we use\n",
    "    P(x_i = +1) = sigmoid(h_i / T)\n",
    "    \n",
    "    This samples from the Boltzmann distribution P(state) ∝ exp(-E/T)\n",
    "    \"\"\"\n",
    "    state = state.flatten().clone()\n",
    "    n = len(state)\n",
    "    energies = [hopfield_energy(state, W).item()]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Pick a random neuron\n",
    "        i = torch.randint(0, n, (1,)).item()\n",
    "        \n",
    "        # Compute local field: what do my neighbors want?\n",
    "        h_i = W[i] @ state\n",
    "        \n",
    "        # Probability of being +1 (derived from Boltzmann distribution!)\n",
    "        prob_plus = torch.sigmoid(torch.tensor(h_i / T))\n",
    "        \n",
    "        # Stochastic update: flip a biased coin\n",
    "        if torch.rand(1) < prob_plus:\n",
    "            state[i] = 1.0\n",
    "        else:\n",
    "            state[i] = -1.0\n",
    "        \n",
    "        energies.append(hopfield_energy(state, W).item())\n",
    "    \n",
    "    return state, energies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare energy trajectories at different temperatures\n",
    "corrupted = corrupt_pattern(pattern_0, noise_fraction=0.3).flatten()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, T in zip(axes, [0.1, 1.0, 5.0]):\n",
    "    _, energies = boltzmann_update(corrupted.clone(), W, T=T, n_steps=300)\n",
    "    ax.plot(energies, alpha=0.7)\n",
    "    ax.axhline(y=hopfield_energy(pattern_0, W).item(), color='g', linestyle='--', label='Target energy')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Energy')\n",
    "    ax.set_title(f'T = {T}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Boltzmann Machine Energy at Different Temperatures', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4555d92",
   "metadata": {},
   "source": [
    "See the tradeoff:\n",
    "- **Low T (0.1)**: Energy decreases quickly but might get stuck\n",
    "- **High T (5.0)**: Explores widely but never settles\n",
    "- **Medium T (1.0)**: A balance, but not optimal\n",
    "\n",
    "**Simulated annealing** solves this: start hot, gradually cool:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e53dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def simulated_annealing(state, W, T_start=5.0, T_end=0.1, n_steps=500):\n",
    "    \"\"\"\n",
    "    Run Boltzmann dynamics with decreasing temperature.\n",
    "    \n",
    "    The key insight: high T → explore (escape local minima)\n",
    "                     low T → exploit (settle into minimum)\n",
    "    Gradually cooling combines the best of both!\n",
    "    \"\"\"\n",
    "    state = state.flatten().clone()\n",
    "    n = len(state)\n",
    "    energies = [hopfield_energy(state, W).item()]\n",
    "    temperatures = []\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # Linear cooling schedule (exponential schedule also popular)\n",
    "        T = T_start - (T_start - T_end) * step / n_steps\n",
    "        temperatures.append(T)\n",
    "        \n",
    "        # Pick a random neuron and do stochastic update\n",
    "        i = torch.randint(0, n, (1,)).item()\n",
    "        h_i = W[i] @ state\n",
    "        prob_plus = torch.sigmoid(torch.tensor(h_i / T))\n",
    "        state[i] = 1.0 if torch.rand(1) < prob_plus else -1.0\n",
    "        \n",
    "        energies.append(hopfield_energy(state, W).item())\n",
    "    \n",
    "    return state, energies, temperatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulated annealing on corrupted pattern\n",
    "state_annealed, energies_annealed, temps = simulated_annealing(corrupted, W, n_steps=500)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Energy\n",
    "ax1.plot(energies_annealed, 'b-', alpha=0.7, label='Energy')\n",
    "ax1.axhline(y=hopfield_energy(pattern_0, W).item(), color='g', linestyle='--', label='Target energy')\n",
    "ax1.set_ylabel('Energy')\n",
    "ax1.legend()\n",
    "ax1.set_title('Simulated Annealing: Start Hot, Cool Down')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Temperature\n",
    "ax2.plot(temps, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Temperature')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final energy: {energies_annealed[-1]:.4f}\")\n",
    "print(f\"Target energy: {hopfield_energy(pattern_0, W).item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a2b06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Restricted Boltzmann Machines (RBMs)\n",
    "\n",
    "The full Boltzmann machine is powerful but has a practical problem: **training is intractable**. Computing gradients requires sampling from the model distribution, which takes exponentially long.\n",
    "\n",
    "**Restricted Boltzmann Machines (RBMs)** solve this by introducing a clever restriction:\n",
    "- Divide neurons into **visible** units (data we observe) and **hidden** units (latent features)\n",
    "- **No connections within a layer** — only visible ↔ hidden connections\n",
    "- This bipartite structure makes sampling tractable!\n",
    "\n",
    "Why does removing intra-layer connections help? Given the visible units, hidden units become **conditionally independent** (no connections between them), so we can sample them all in parallel. Same for visible given hidden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0faf4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RBM architecture\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Visible layer (bottom)\n",
    "n_visible, n_hidden = 6, 4\n",
    "for i in range(n_visible):\n",
    "    circle = plt.Circle((i * 1.5 + 1, 1), 0.3, color='steelblue', ec='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(i * 1.5 + 1, 1, f'v{i}', ha='center', va='center', fontsize=10, color='white')\n",
    "\n",
    "# Hidden layer (top)\n",
    "for j in range(n_hidden):\n",
    "    circle = plt.Circle((j * 2 + 1.75, 4), 0.3, color='coral', ec='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(j * 2 + 1.75, 4, f'h{j}', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Connections (visible ↔ hidden only)\n",
    "for i in range(n_visible):\n",
    "    for j in range(n_hidden):\n",
    "        ax.plot([i * 1.5 + 1, j * 2 + 1.75], [1.3, 3.7], 'gray', alpha=0.3, linewidth=1)\n",
    "\n",
    "ax.text(4.5, 5, 'Hidden Layer (latent features)', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.text(4.5, 0, 'Visible Layer (data)', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-1, 10)\n",
    "ax.set_ylim(-1, 6)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Restricted Boltzmann Machine\\n(No intra-layer connections)', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f68df",
   "metadata": {},
   "source": [
    "The RBM energy function:\n",
    "\n",
    "$$E(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{a}^T\\mathbf{v} - \\mathbf{b}^T\\mathbf{h} - \\mathbf{v}^T W \\mathbf{h}$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{v}$ = visible units, $\\mathbf{h}$ = hidden units\n",
    "- $\\mathbf{a}$ = visible biases, $\\mathbf{b}$ = hidden biases\n",
    "- $W$ = weights connecting visible and hidden layers\n",
    "\n",
    "The conditional distributions are simple sigmoids:\n",
    "- $P(h_j = 1 | \\mathbf{v}) = \\sigma(b_j + \\sum_i v_i W_{ij})$\n",
    "- $P(v_i = 1 | \\mathbf{h}) = \\sigma(a_i + \\sum_j W_{ij} h_j)$\n",
    "\n",
    "This is called **block Gibbs sampling** — sample all hidden given visible, then all visible given hidden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class RBM(nn.Module):\n",
    "    \"\"\"\n",
    "    Restricted Boltzmann Machine with binary visible and hidden units.\n",
    "    \n",
    "    The \"restriction\": No connections within layers (only visible ↔ hidden).\n",
    "    This makes sampling tractable: given visible, all hidden are independent!\n",
    "    \"\"\"\n",
    "    def __init__(self, n_visible, n_hidden):\n",
    "        super().__init__()\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        # Small random weights, zero biases\n",
    "        self.W = nn.Parameter(torch.randn(n_visible, n_hidden) * 0.01)\n",
    "        self.a = nn.Parameter(torch.zeros(n_visible))  # visible bias\n",
    "        self.b = nn.Parameter(torch.zeros(n_hidden))   # hidden bias\n",
    "    \n",
    "    def sample_hidden(self, v):\n",
    "        \"\"\"Sample hidden units given visible units.\"\"\"\n",
    "        # P(h_j = 1 | v) = sigmoid(b_j + sum_i v_i W_ij)\n",
    "        # This is the same sigmoid derivation as before!\n",
    "        prob_h = torch.sigmoid(F.linear(v, self.W.t(), self.b))\n",
    "        return torch.bernoulli(prob_h), prob_h\n",
    "    \n",
    "    def sample_visible(self, h):\n",
    "        \"\"\"Sample visible units given hidden units.\"\"\"\n",
    "        # P(v_i = 1 | h) = sigmoid(a_i + sum_j W_ij h_j)\n",
    "        prob_v = torch.sigmoid(F.linear(h, self.W, self.a))\n",
    "        return torch.bernoulli(prob_v), prob_v\n",
    "    \n",
    "    def energy(self, v, h):\n",
    "        \"\"\"Compute energy E(v, h).\"\"\"\n",
    "        return -v @ self.a - h @ self.b - (v @ self.W @ h)\n",
    "    \n",
    "    def free_energy(self, v):\n",
    "        \"\"\"\n",
    "        Free energy F(v) = -log sum_h exp(-E(v,h))\n",
    "        For binary hidden units: F(v) = -a^T v - sum_j log(1 + exp(b_j + v^T W_j))\n",
    "        \n",
    "        This is analytically tractable because hidden units are independent given v!\n",
    "        \"\"\"\n",
    "        vbias_term = v @ self.a\n",
    "        wx_b = F.linear(v, self.W.t(), self.b)\n",
    "        hidden_term = torch.sum(F.softplus(wx_b), dim=-1)\n",
    "        return -vbias_term - hidden_term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01775770",
   "metadata": {},
   "source": [
    "### Contrastive Divergence: The Training Algorithm\n",
    "\n",
    "This is where the magic of learning happens. Let's derive it from scratch.\n",
    "\n",
    "**Goal**: Adjust weights so that training data has LOW energy (high probability).\n",
    "\n",
    "**The likelihood gradient**: We want to maximize $\\log P(\\mathbf{v}_{\\text{data}})$. Taking the derivative:\n",
    "\n",
    "$$\\frac{\\partial \\log P(\\mathbf{v})}{\\partial W_{ij}} = \\langle v_i h_j \\rangle_{\\text{data}} - \\langle v_i h_j \\rangle_{\\text{model}}$$\n",
    "\n",
    "**Intuition for this gradient:**\n",
    "\n",
    "- **Positive term** $\\langle v_i h_j \\rangle_{\\text{data}}$: \"When I see training data, how often do $v_i$ and $h_j$ fire together?\"\n",
    "- **Negative term** $\\langle v_i h_j \\rangle_{\\text{model}}$: \"When the model is left to run freely, how often do they fire together?\"\n",
    "\n",
    "**The learning rule**: \n",
    "- If $v_i$ and $h_j$ fire together MORE in data than in the model → increase $W_{ij}$\n",
    "- If they fire together LESS in data than in the model → decrease $W_{ij}$\n",
    "\n",
    "This is like saying: \"Make the model's 'dreams' match reality!\"\n",
    "\n",
    "**The problem**: Computing $\\langle v_i h_j \\rangle_{\\text{model}}$ requires running the model to equilibrium — which takes forever.\n",
    "\n",
    "**Contrastive Divergence (Hinton, 2002)**: Just run k steps of Gibbs sampling starting from the data! The intuition:\n",
    "- Starting from data, after 1 step, you get something \"close\" to the data\n",
    "- The difference between data and this reconstruction tells you which direction to push\n",
    "\n",
    "CD-1 (k=1) is: data → sample hidden → sample visible → done. Surprisingly effective!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab69a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Contrastive Divergence step by step\n",
    "def visualize_cd_steps(rbm, v_data, k=3):\n",
    "    \"\"\"Show what happens during CD-k: data → hidden → visible → hidden → ...\"\"\"\n",
    "    fig, axes = plt.subplots(2, k+1, figsize=(3*(k+1), 6))\n",
    "    \n",
    "    v = v_data.clone().unsqueeze(0) if v_data.dim() == 1 else v_data[:1]\n",
    "    \n",
    "    # Show original data\n",
    "    axes[0, 0].imshow(v.view(8, 8).detach().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, 0].set_title('v₀ (data)')\n",
    "    axes[0, 0].axis('off')\n",
    "    axes[1, 0].axis('off')\n",
    "    axes[1, 0].text(0.5, 0.5, 'Start', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    for step in range(k):\n",
    "        # Sample hidden\n",
    "        h, prob_h = rbm.sample_hidden(v)\n",
    "        \n",
    "        # Sample visible\n",
    "        v, prob_v = rbm.sample_visible(h)\n",
    "        \n",
    "        # Show hidden probabilities\n",
    "        axes[0, step+1].bar(range(rbm.n_hidden), prob_h.squeeze().detach().numpy(), alpha=0.7)\n",
    "        axes[0, step+1].set_ylim(0, 1)\n",
    "        axes[0, step+1].set_title(f'h_{step} probs')\n",
    "        \n",
    "        # Show reconstructed visible\n",
    "        axes[1, step+1].imshow(prob_v.view(8, 8).detach().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "        axes[1, step+1].set_title(f'v_{step+1} (recon)')\n",
    "        axes[1, step+1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Contrastive Divergence: Data → Hidden → Visible → ...\\\\n'\n",
    "                 'We compare v₀ (data) with v_k (reconstruction)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Use one of our comparison patterns\n",
    "visualize_cd_steps(rbm, train_data[0], k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d76eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def contrastive_divergence(rbm, v_data, k=1, lr=0.1):\n",
    "    \"\"\"\n",
    "    One step of Contrastive Divergence training.\n",
    "    \n",
    "    The algorithm:\n",
    "    1. Positive phase: Clamp visible to data, sample hidden\n",
    "    2. Negative phase: Run k steps of Gibbs sampling  \n",
    "    3. Update: ΔW ∝ ⟨v·h⟩_data - ⟨v·h⟩_reconstruction\n",
    "    \n",
    "    Args:\n",
    "        rbm: RBM model\n",
    "        v_data: Batch of visible data (batch_size x n_visible)\n",
    "        k: Number of Gibbs sampling steps\n",
    "        lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "        Reconstruction error (for monitoring)\n",
    "    \"\"\"\n",
    "    batch_size = v_data.shape[0]\n",
    "    \n",
    "    # Positive phase: sample hidden from data (what the model \"sees\")\n",
    "    h_data, prob_h_data = rbm.sample_hidden(v_data)\n",
    "    \n",
    "    # Negative phase: k steps of Gibbs sampling\n",
    "    v_neg = v_data.clone()\n",
    "    for _ in range(k):\n",
    "        h_neg, _ = rbm.sample_hidden(v_neg)\n",
    "        v_neg, prob_v_neg = rbm.sample_visible(h_neg)\n",
    "    \n",
    "    # Final hidden probabilities for negative phase\n",
    "    _, prob_h_neg = rbm.sample_hidden(v_neg)\n",
    "    \n",
    "    # Compute gradients\n",
    "    # dW = <v_data h_data> - <v_neg h_neg>\n",
    "    positive_grad = v_data.t() @ prob_h_data / batch_size\n",
    "    negative_grad = v_neg.t() @ prob_h_neg / batch_size\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        rbm.W += lr * (positive_grad - negative_grad)\n",
    "        rbm.a += lr * (v_data.mean(0) - v_neg.mean(0))\n",
    "        rbm.b += lr * (prob_h_data.mean(0) - prob_h_neg.mean(0))\n",
    "    \n",
    "    # Reconstruction error\n",
    "    recon_error = F.mse_loss(v_neg, v_data)\n",
    "    return recon_error.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef63e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparison: Hopfield Network vs RBM\n",
    "\n",
    "Let's compare these two models on the same task: learning and reconstructing patterns from a small dataset. We'll use 8×8 binary images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of simple binary patterns\n",
    "# These represent simple shapes: vertical line, horizontal line, diagonal, cross, etc.\n",
    "\n",
    "def create_pattern_dataset(n_patterns=10, size=8):\n",
    "    \"\"\"Create diverse binary patterns for testing.\"\"\"\n",
    "    patterns = []\n",
    "    \n",
    "    # Vertical lines at different positions\n",
    "    for i in [2, 5]:\n",
    "        p = torch.zeros(size, size)\n",
    "        p[:, i] = 1\n",
    "        patterns.append(p)\n",
    "    \n",
    "    # Horizontal lines\n",
    "    for i in [2, 5]:\n",
    "        p = torch.zeros(size, size)\n",
    "        p[i, :] = 1\n",
    "        patterns.append(p)\n",
    "    \n",
    "    # Diagonals\n",
    "    p = torch.zeros(size, size)\n",
    "    for i in range(size):\n",
    "        p[i, i] = 1\n",
    "    patterns.append(p)\n",
    "    \n",
    "    p = torch.zeros(size, size)\n",
    "    for i in range(size):\n",
    "        p[i, size-1-i] = 1\n",
    "    patterns.append(p)\n",
    "    \n",
    "    # Border\n",
    "    p = torch.zeros(size, size)\n",
    "    p[0, :] = 1\n",
    "    p[-1, :] = 1\n",
    "    p[:, 0] = 1\n",
    "    p[:, -1] = 1\n",
    "    patterns.append(p)\n",
    "    \n",
    "    # Center cross\n",
    "    p = torch.zeros(size, size)\n",
    "    p[size//2, :] = 1\n",
    "    p[:, size//2] = 1\n",
    "    patterns.append(p)\n",
    "    \n",
    "    # Checkerboard\n",
    "    p = torch.zeros(size, size)\n",
    "    p[::2, ::2] = 1\n",
    "    p[1::2, 1::2] = 1\n",
    "    patterns.append(p)\n",
    "    \n",
    "    # X shape\n",
    "    p = torch.zeros(size, size)\n",
    "    for i in range(size):\n",
    "        p[i, i] = 1\n",
    "        p[i, size-1-i] = 1\n",
    "    patterns.append(p)\n",
    "    \n",
    "    return patterns[:n_patterns]\n",
    "\n",
    "comparison_patterns = create_pattern_dataset(n_patterns=8, size=8)\n",
    "print(f\"Created {len(comparison_patterns)} patterns of size 8x8 = 64 neurons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the patterns\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, (ax, p) in enumerate(zip(axes.flat, comparison_patterns)):\n",
    "    ax.imshow(p.numpy(), cmap='gray_r', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Pattern {i}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Comparison Dataset: 8 Binary Patterns', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ±1 format for Hopfield (0/1 → -1/+1)\n",
    "hopfield_patterns = [2*p - 1 for p in comparison_patterns]\n",
    "\n",
    "# Build Hopfield weight matrix\n",
    "W_comparison = build_hopfield_weights(hopfield_patterns)\n",
    "print(f\"Hopfield weights: {W_comparison.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a697cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RBM on the patterns\n",
    "n_visible = 64  # 8x8\n",
    "n_hidden = 32   # Half the visible units\n",
    "\n",
    "rbm = RBM(n_visible, n_hidden)\n",
    "\n",
    "# Stack patterns for batch training\n",
    "train_data = torch.stack([p.flatten() for p in comparison_patterns])\n",
    "\n",
    "# Train for several epochs\n",
    "n_epochs = 500\n",
    "recon_errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle data\n",
    "    perm = torch.randperm(len(train_data))\n",
    "    shuffled = train_data[perm]\n",
    "    \n",
    "    error = contrastive_divergence(rbm, shuffled, k=1, lr=0.1)\n",
    "    recon_errors.append(error)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Reconstruction error = {error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBM training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(recon_errors)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Reconstruction Error (MSE)')\n",
    "plt.title('RBM Training: Reconstruction Error Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pattern_recovery(pattern_idx, noise_fraction=0.2):\n",
    "    \"\"\"Compare Hopfield and RBM pattern recovery.\"\"\"\n",
    "    \n",
    "    # Get pattern\n",
    "    original = comparison_patterns[pattern_idx]\n",
    "    original_hopfield = hopfield_patterns[pattern_idx]\n",
    "    \n",
    "    # Corrupt pattern\n",
    "    corrupted_0_1 = original.clone().flatten()\n",
    "    n_flip = int(len(corrupted_0_1) * noise_fraction)\n",
    "    flip_idx = torch.randperm(len(corrupted_0_1))[:n_flip]\n",
    "    corrupted_0_1[flip_idx] = 1 - corrupted_0_1[flip_idx]  # flip 0↔1\n",
    "    \n",
    "    corrupted_pm1 = 2 * corrupted_0_1 - 1  # convert to ±1\n",
    "    \n",
    "    # Hopfield recovery\n",
    "    hopfield_recovered, _, _ = hopfield_update(corrupted_pm1, W_comparison, async_update=False)\n",
    "    hopfield_recovered = ((hopfield_recovered + 1) / 2).view(8, 8)  # back to 0/1\n",
    "    \n",
    "    # RBM recovery: run Gibbs sampling\n",
    "    v = corrupted_0_1.unsqueeze(0)\n",
    "    for _ in range(10):  # 10 Gibbs steps\n",
    "        h, _ = rbm.sample_hidden(v)\n",
    "        v, _ = rbm.sample_visible(h)\n",
    "    rbm_recovered = v.squeeze().view(8, 8)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    \n",
    "    axes[0].imshow(original.numpy(), cmap='gray_r', vmin=0, vmax=1)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(corrupted_0_1.view(8, 8).numpy(), cmap='gray_r', vmin=0, vmax=1)\n",
    "    axes[1].set_title(f'Corrupted ({noise_fraction*100:.0f}% noise)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(hopfield_recovered.numpy(), cmap='gray_r', vmin=0, vmax=1)\n",
    "    hopfield_acc = (hopfield_recovered == original).float().mean()\n",
    "    axes[2].set_title(f'Hopfield\\n({hopfield_acc*100:.0f}% match)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    axes[3].imshow(rbm_recovered.detach().numpy(), cmap='gray_r', vmin=0, vmax=1)\n",
    "    rbm_acc = (rbm_recovered.round() == original).float().mean()\n",
    "    axes[3].set_title(f'RBM\\n({rbm_acc*100:.0f}% match)')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return hopfield_acc.item(), rbm_acc.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8580308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on several patterns\n",
    "for i in [0, 4, 6]:  # vertical line, diagonal, checkerboard\n",
    "    print(f\"\\n--- Pattern {i} ---\")\n",
    "    test_pattern_recovery(i, noise_fraction=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5efc3",
   "metadata": {},
   "source": [
    "**Key differences observed:**\n",
    "\n",
    "| Aspect | Hopfield | RBM |\n",
    "|--------|----------|-----|\n",
    "| **Training** | One-shot (Hebbian) | Iterative (CD) |\n",
    "| **Storage** | Patterns in weights directly | Learned latent representation |\n",
    "| **Recovery** | Deterministic (may get stuck) | Stochastic (explores) |\n",
    "| **Capacity** | ~0.14N patterns | Can model distributions, not just patterns |\n",
    "\n",
    "The Hopfield network directly memorizes patterns; the RBM learns a generative model of the data distribution. RBMs can generate new samples that look like training data, while Hopfield only retrieves stored patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Training a Production-Scale Boltzmann Machine\n",
    "\n",
    "Now let's build and train a larger RBM on real data (MNIST digits), with proper production monitoring using Weights & Biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929315a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production RBM with proper monitoring\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ProductionRBM(nn.Module):\n",
    "    \"\"\"\n",
    "    Production-ready RBM with:\n",
    "    - Proper initialization (Xavier-like for stable training)\n",
    "    - Momentum-based training (faster convergence)\n",
    "    - Weight regularization (prevent explosion)\n",
    "    - Monitoring utilities (dead units, weight stats)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_visible, n_hidden, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.device = device\n",
    "        \n",
    "        # Xavier-like initialization scaled for RBMs\n",
    "        # Why 4*sqrt(6/(n_in + n_out))? Ensures initial activations aren't saturated\n",
    "        std = 4 * np.sqrt(6. / (n_visible + n_hidden))\n",
    "        self.W = nn.Parameter(torch.randn(n_visible, n_hidden, device=device) * std)\n",
    "        self.a = nn.Parameter(torch.zeros(n_visible, device=device))\n",
    "        self.b = nn.Parameter(torch.zeros(n_hidden, device=device))\n",
    "        \n",
    "        # Momentum terms for faster convergence\n",
    "        self.W_momentum = torch.zeros_like(self.W)\n",
    "        self.a_momentum = torch.zeros_like(self.a)\n",
    "        self.b_momentum = torch.zeros_like(self.b)\n",
    "        \n",
    "    def sample_hidden(self, v):\n",
    "        prob_h = torch.sigmoid(F.linear(v, self.W.t(), self.b))\n",
    "        return torch.bernoulli(prob_h), prob_h\n",
    "    \n",
    "    def sample_visible(self, h):\n",
    "        prob_v = torch.sigmoid(F.linear(h, self.W, self.a))\n",
    "        return torch.bernoulli(prob_v), prob_v\n",
    "    \n",
    "    def free_energy(self, v):\n",
    "        \"\"\"Free energy for monitoring.\"\"\"\n",
    "        vbias_term = torch.sum(v * self.a, dim=-1)\n",
    "        wx_b = F.linear(v, self.W.t(), self.b)\n",
    "        hidden_term = torch.sum(F.softplus(wx_b), dim=-1)\n",
    "        return -vbias_term - hidden_term\n",
    "    \n",
    "    def get_weight_stats(self):\n",
    "        \"\"\"Get weight statistics for monitoring training health.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return {\n",
    "                'weight_mean': self.W.mean().item(),\n",
    "                'weight_std': self.W.std().item(),\n",
    "                'weight_max': self.W.abs().max().item(),\n",
    "                'weight_norm': self.W.norm().item(),\n",
    "                'hidden_bias_mean': self.b.mean().item(),\n",
    "                'visible_bias_mean': self.a.mean().item(),\n",
    "            }\n",
    "    \n",
    "    def get_hidden_activation_stats(self, v_batch):\n",
    "        \"\"\"Analyze hidden unit activation patterns.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            _, prob_h = self.sample_hidden(v_batch)\n",
    "            active = (prob_h > 0.5).float()\n",
    "            \n",
    "            return {\n",
    "                'hidden_prob_mean': prob_h.mean().item(),\n",
    "                'hidden_prob_std': prob_h.std().item(),\n",
    "                'fraction_active': active.mean().item(),\n",
    "                'dead_units': (prob_h.mean(0) < 0.01).sum().item(),  # units rarely active\n",
    "                'saturated_units': (prob_h.mean(0) > 0.99).sum().item(),  # units always active\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def train_rbm_production(\n",
    "    rbm, \n",
    "    train_loader, \n",
    "    n_epochs=10,\n",
    "    lr=0.01,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0001,\n",
    "    k=1,  # CD-k\n",
    "    use_wandb=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train RBM with production best practices.\n",
    "    \n",
    "    Uses momentum SGD with weight decay, and optionally logs to wandb.\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'recon_error': [],\n",
    "        'free_energy': [],\n",
    "    }\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_recon = []\n",
    "        epoch_fe = []\n",
    "        \n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            # Flatten and binarize (for MNIST)\n",
    "            v_data = (data.view(-1, rbm.n_visible) > 0.5).float().to(rbm.device)\n",
    "            batch_size = v_data.shape[0]\n",
    "            \n",
    "            # Positive phase\n",
    "            h_data, prob_h_data = rbm.sample_hidden(v_data)\n",
    "            \n",
    "            # Negative phase (CD-k)\n",
    "            v_neg = v_data.clone()\n",
    "            for _ in range(k):\n",
    "                h_neg, _ = rbm.sample_hidden(v_neg)\n",
    "                v_neg, prob_v_neg = rbm.sample_visible(h_neg)\n",
    "            _, prob_h_neg = rbm.sample_hidden(v_neg)\n",
    "            \n",
    "            # Gradients\n",
    "            pos_grad_W = v_data.t() @ prob_h_data / batch_size\n",
    "            neg_grad_W = v_neg.t() @ prob_h_neg / batch_size\n",
    "            \n",
    "            pos_grad_a = v_data.mean(0)\n",
    "            neg_grad_a = v_neg.mean(0)\n",
    "            \n",
    "            pos_grad_b = prob_h_data.mean(0)\n",
    "            neg_grad_b = prob_h_neg.mean(0)\n",
    "            \n",
    "            # Update with momentum and weight decay\n",
    "            with torch.no_grad():\n",
    "                # Momentum update\n",
    "                rbm.W_momentum = momentum * rbm.W_momentum + lr * (pos_grad_W - neg_grad_W - weight_decay * rbm.W)\n",
    "                rbm.a_momentum = momentum * rbm.a_momentum + lr * (pos_grad_a - neg_grad_a)\n",
    "                rbm.b_momentum = momentum * rbm.b_momentum + lr * (pos_grad_b - neg_grad_b)\n",
    "                \n",
    "                rbm.W += rbm.W_momentum\n",
    "                rbm.a += rbm.a_momentum\n",
    "                rbm.b += rbm.b_momentum\n",
    "            \n",
    "            # Metrics\n",
    "            recon_error = F.mse_loss(prob_v_neg, v_data).item()\n",
    "            free_energy = rbm.free_energy(v_data).mean().item()\n",
    "            \n",
    "            epoch_recon.append(recon_error)\n",
    "            epoch_fe.append(free_energy)\n",
    "        \n",
    "        # Epoch metrics\n",
    "        avg_recon = np.mean(epoch_recon)\n",
    "        avg_fe = np.mean(epoch_fe)\n",
    "        \n",
    "        history['recon_error'].append(avg_recon)\n",
    "        history['free_energy'].append(avg_fe)\n",
    "        \n",
    "        # Get detailed stats\n",
    "        weight_stats = rbm.get_weight_stats()\n",
    "        activation_stats = rbm.get_hidden_activation_stats(v_data)\n",
    "        \n",
    "        # Log to wandb\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'recon_error': avg_recon,\n",
    "                'free_energy': avg_fe,\n",
    "                **weight_stats,\n",
    "                **activation_stats\n",
    "            })\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}: Recon={avg_recon:.4f}, FE={avg_fe:.1f}, \"\n",
    "                  f\"Dead units={activation_stats['dead_units']}, \"\n",
    "                  f\"W_norm={weight_stats['weight_norm']:.2f}\")\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Smaller batch for M3 Max - adjust based on your system\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Image size: 28x28 = 784 visible units\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d5291",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "For a production run on 2x Tesla T4 (56GB VRAM total), we'd use:\n",
    "- **n_hidden = 2048** (large hidden layer for rich representations)\n",
    "- **batch_size = 512** (utilize GPU parallelism)\n",
    "- **n_epochs = 100+** (several hours of training)\n",
    "\n",
    "For local M3 Max testing, we'll use smaller settings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea54af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for local training (M3 Max)\n",
    "config = {\n",
    "    'n_visible': 784,\n",
    "    'n_hidden': 500,  # Moderate size for local\n",
    "    'batch_size': 128,\n",
    "    'n_epochs': 30,\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 0.0001,\n",
    "    'k': 1,  # CD-1\n",
    "}\n",
    "\n",
    "# For production on Tesla T4s, uncomment:\n",
    "# config = {\n",
    "#     'n_visible': 784,\n",
    "#     'n_hidden': 2048,\n",
    "#     'batch_size': 512,\n",
    "#     'n_epochs': 100,\n",
    "#     'lr': 0.005,\n",
    "#     'momentum': 0.9,\n",
    "#     'weight_decay': 0.0001,\n",
    "#     'k': 5,  # CD-5 for better gradient estimates\n",
    "# }\n",
    "\n",
    "print(f\"Config: {config}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb (set use_wandb=True to log, False for local testing)\n",
    "use_wandb = False  # Set to True to log to wandb\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"boltzmann-machines\",\n",
    "        config=config,\n",
    "        name=f\"rbm-h{config['n_hidden']}-cd{config['k']}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "rbm_prod = ProductionRBM(\n",
    "    n_visible=config['n_visible'],\n",
    "    n_hidden=config['n_hidden'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in rbm_prod.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7def93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "history = train_rbm_production(\n",
    "    rbm_prod,\n",
    "    train_loader,\n",
    "    n_epochs=config['n_epochs'],\n",
    "    lr=config['lr'],\n",
    "    momentum=config['momentum'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    k=config['k'],\n",
    "    use_wandb=use_wandb\n",
    ")\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c518725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history['recon_error'])\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Reconstruction Error')\n",
    "ax1.set_title('Training: Reconstruction Error')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history['free_energy'])\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Free Energy')\n",
    "ax2.set_title('Training: Free Energy')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned features (hidden unit weight vectors = learned \"features\")\n",
    "def visualize_rbm_features(rbm, n_features=100):\n",
    "    \"\"\"Show what features the hidden units have learned.\"\"\"\n",
    "    W = rbm.W.detach().cpu()  # (n_visible, n_hidden)\n",
    "    \n",
    "    n_show = min(n_features, W.shape[1])\n",
    "    n_cols = 10\n",
    "    n_rows = n_show // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 1.5 * n_rows))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_show:\n",
    "            feature = W[:, i].view(28, 28)\n",
    "            ax.imshow(feature.numpy(), cmap='RdBu', vmin=-feature.abs().max(), vmax=feature.abs().max())\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Learned RBM Features (Hidden Unit Weights)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_rbm_features(rbm_prod, n_features=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8482a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new samples from the RBM\n",
    "def generate_samples(rbm, n_samples=10, n_gibbs=1000):\n",
    "    \"\"\"Generate samples by running Gibbs sampling from random initialization.\"\"\"\n",
    "    # Start from random visible state\n",
    "    v = torch.bernoulli(torch.ones(n_samples, rbm.n_visible) * 0.5).to(rbm.device)\n",
    "    \n",
    "    # Run Gibbs sampling\n",
    "    for _ in range(n_gibbs):\n",
    "        h, _ = rbm.sample_hidden(v)\n",
    "        v, _ = rbm.sample_visible(h)\n",
    "    \n",
    "    return v\n",
    "\n",
    "samples = generate_samples(rbm_prod, n_samples=20, n_gibbs=1000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(samples[i].detach().cpu().view(28, 28).numpy(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Samples Generated by RBM (Gibbs Sampling)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de979a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "### How to Know if Training is Going Well\n",
    "\n",
    "Unlike discriminative models where we track accuracy/loss directly, RBMs require more nuanced monitoring:\n",
    "\n",
    "**Key metrics to watch:**\n",
    "\n",
    "1. **Reconstruction Error**: Should decrease steadily. If it plateaus, try increasing k in CD-k.\n",
    "\n",
    "2. **Free Energy Gap**: Compare average free energy of training data vs random samples. Training data should have lower free energy.\n",
    "\n",
    "3. **Hidden Unit Statistics**:\n",
    "   - **Dead units**: Units that never activate (<1% of the time). Indicates wasted capacity.\n",
    "   - **Saturated units**: Units always active (>99%). Also wasted.\n",
    "   - Ideal: diverse activation patterns with mean activation around 0.1-0.5.\n",
    "\n",
    "4. **Weight Statistics**:\n",
    "   - Weight norm shouldn't explode (use weight decay)\n",
    "   - Mean should stay near 0\n",
    "   - If weights become very large, gradients may become unstable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hidden unit activation distribution\n",
    "sample_batch = next(iter(train_loader))[0][:256]\n",
    "v_sample = (sample_batch.view(-1, 784) > 0.5).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, prob_h = rbm_prod.sample_hidden(v_sample)\n",
    "    \n",
    "# Histogram of mean activations per hidden unit\n",
    "mean_activations = prob_h.mean(0).cpu().numpy()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.hist(mean_activations, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=0.01, color='r', linestyle='--', label='Dead threshold (1%)')\n",
    "ax1.axvline(x=0.99, color='orange', linestyle='--', label='Saturated threshold (99%)')\n",
    "ax1.set_xlabel('Mean Activation per Hidden Unit')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Hidden Unit Activation Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Weight distribution\n",
    "weights = rbm_prod.W.detach().cpu().flatten().numpy()\n",
    "ax2.hist(weights, bins=100, edgecolor='black', alpha=0.7)\n",
    "ax2.set_xlabel('Weight Value')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title(f'Weight Distribution (mean={weights.mean():.3f}, std={weights.std():.3f})')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dead units: {(mean_activations < 0.01).sum()}\")\n",
    "print(f\"Saturated units: {(mean_activations > 0.99).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8b75a",
   "metadata": {},
   "source": [
    "### Time Complexity\n",
    "\n",
    "**Full Boltzmann Machine**: \n",
    "- Inference requires sampling from the full joint distribution\n",
    "- This is NP-hard in general: $O(2^N)$ states to consider\n",
    "- Why? All units are connected, so we can't factorize the distribution\n",
    "\n",
    "**Restricted Boltzmann Machine**:\n",
    "- Inference is $O(N_{visible} \\times N_{hidden})$ per Gibbs step (matrix multiply)\n",
    "- The bipartite structure means we can sample all hidden (or visible) units in parallel\n",
    "- Training with CD-k: $O(k \\times N_v \\times N_h \\times \\text{batch\\_size})$ per iteration\n",
    "\n",
    "### Training Stability Tricks\n",
    "\n",
    "**From transformer land (adapted for RBMs):**\n",
    "\n",
    "1. **Weight initialization**: Xavier-like scaling prevents vanishing/exploding activations\n",
    "2. **Learning rate warmup**: Start with small LR, increase gradually\n",
    "3. **Gradient clipping**: Clip large gradients to prevent instability\n",
    "4. **Weight decay**: L2 regularization prevents weight explosion\n",
    "5. **Momentum**: SGD with momentum smooths gradient updates\n",
    "\n",
    "**RBM-specific:**\n",
    "\n",
    "1. **CD-k**: Higher k gives better gradient estimates (but slower). Start with CD-1, increase if needed.\n",
    "2. **Persistent Contrastive Divergence (PCD)**: Maintain Gibbs chains across batches for better mixing\n",
    "3. **Sparsity penalty**: Encourage sparse hidden activations for better features\n",
    "4. **Adaptive learning rates**: Different LR for weights vs biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd478f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modern Hopfield Networks and Transformer Attention\n",
    "\n",
    "The classic Hopfield network stores ~0.14N patterns. But in 2020, Ramsauer et al. showed that **continuous Hopfield networks with exponential energy** can store **exponentially many patterns**—and their update rule is exactly **transformer attention**!\n",
    "\n",
    "### Why Change the Energy Function?\n",
    "\n",
    "The classic Hopfield energy $E = -\\frac{1}{2} \\mathbf{x}^T W \\mathbf{x}$ is **quadratic** in x. This limits capacity.\n",
    "\n",
    "**Key insight**: Make the energy **exponential** in the similarity between query and patterns. Why exponential?\n",
    "\n",
    "1. **Sharper separation**: Exponentials amplify differences. If pattern A has similarity 2 and B has similarity 1, quadratic gives ratio 4:1, but exponential gives ratio $e^2:e^1 \\approx 7.4:1$\n",
    "2. **Softer than argmax**: Unlike hard maximum, exponential allows \"soft\" retrieval of multiple similar patterns\n",
    "3. **Natural normalization**: logsumexp is a \"soft maximum\" — it approximates max while being differentiable\n",
    "\n",
    "### Deriving the Modern Hopfield Energy\n",
    "\n",
    "**Goal**: Energy should be LOW when query x is similar to a stored pattern.\n",
    "\n",
    "**Step 1: Measure similarity**\n",
    "\n",
    "For stored patterns $\\Xi = [\\xi_1, ..., \\xi_N]$, similarity scores are $\\Xi^T \\mathbf{x}$ (dot products).\n",
    "\n",
    "**Step 2: We want energy low when ANY pattern is similar**\n",
    "\n",
    "The \"soft minimum over all patterns\" is the negative of \"soft maximum of similarities\":\n",
    "\n",
    "$$E = -\\text{logsumexp}(\\beta \\cdot \\Xi^T \\mathbf{x})$$\n",
    "\n",
    "where $\\beta$ controls sharpness (like inverse temperature).\n",
    "\n",
    "**Step 3: Add regularization**\n",
    "\n",
    "To prevent x from blowing up (just making itself larger increases all similarities), add $\\frac{1}{2}\\|\\mathbf{x}\\|^2$:\n",
    "\n",
    "$$E(\\mathbf{x}) = -\\text{logsumexp}(\\beta \\cdot \\boldsymbol{\\Xi}^T \\mathbf{x}) + \\frac{1}{2}\\|\\mathbf{x}\\|^2$$\n",
    "\n",
    "### The Update Rule = Attention!\n",
    "\n",
    "To find the minimum energy state, we take the gradient and set to zero:\n",
    "\n",
    "$$\\nabla_{\\mathbf{x}} E = -\\boldsymbol{\\Xi} \\cdot \\text{softmax}(\\beta \\cdot \\boldsymbol{\\Xi}^T \\mathbf{x}) + \\mathbf{x} = 0$$\n",
    "\n",
    "Solving:\n",
    "\n",
    "$$\\mathbf{x}^{\\text{new}} = \\boldsymbol{\\Xi} \\cdot \\text{softmax}(\\beta \\cdot \\boldsymbol{\\Xi}^T \\mathbf{x})$$\n",
    "\n",
    "**This IS attention!** Compare to transformer attention: $\\text{Attention}(Q, K, V) = \\text{softmax}(QK^T / \\sqrt{d}) \\cdot V$\n",
    "\n",
    "The correspondence:\n",
    "- Query x ↔ Q\n",
    "- Patterns Ξ ↔ Keys K (and also Values V when K=V)\n",
    "- $\\beta$ ↔ $1/\\sqrt{d}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Classic vs Modern Hopfield Energy Landscapes (1D projection)\n",
    "# This shows WHY modern Hopfield can store exponentially more patterns\n",
    "\n",
    "# Two stored patterns in 2D for visualization\n",
    "patterns_2d = torch.tensor([[1.0, 0.0], [0.0, 1.0]])  # Two orthogonal patterns\n",
    "\n",
    "# Create grid of possible states\n",
    "x_range = torch.linspace(-1.5, 1.5, 100)\n",
    "y_range = torch.linspace(-1.5, 1.5, 100)\n",
    "X, Y = torch.meshgrid(x_range, y_range, indexing='ij')\n",
    "states = torch.stack([X.flatten(), Y.flatten()], dim=1)  # (10000, 2)\n",
    "\n",
    "# Classic Hopfield energy: E = -0.5 * x^T W x where W = sum of outer products\n",
    "W_classic = patterns_2d.t() @ patterns_2d  # (2, 2)\n",
    "W_classic.fill_diagonal_(0)\n",
    "E_classic = -0.5 * (states @ W_classic * states).sum(dim=1)  # (10000,)\n",
    "\n",
    "# Modern Hopfield energy: E = -logsumexp(Xi^T x) + 0.5 ||x||^2\n",
    "scores = states @ patterns_2d.t()  # (10000, 2)\n",
    "E_modern = -torch.logsumexp(scores * 2.0, dim=1) + 0.5 * (states ** 2).sum(dim=1)\n",
    "\n",
    "# Reshape for plotting\n",
    "E_classic_2d = E_classic.view(100, 100)\n",
    "E_modern_2d = E_modern.view(100, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Classic Hopfield\n",
    "im1 = axes[0].contourf(X.numpy(), Y.numpy(), E_classic_2d.numpy(), levels=30, cmap='viridis')\n",
    "axes[0].scatter(patterns_2d[:, 0].numpy(), patterns_2d[:, 1].numpy(), \n",
    "                c='red', s=200, marker='*', label='Stored patterns', zorder=5)\n",
    "axes[0].set_xlabel('State dimension 1')\n",
    "axes[0].set_ylabel('State dimension 2')\n",
    "axes[0].set_title('Classic Hopfield Energy\\\\n(Quadratic: shallow, broad valleys)')\n",
    "axes[0].legend()\n",
    "axes[0].set_aspect('equal')\n",
    "plt.colorbar(im1, ax=axes[0], label='Energy')\n",
    "\n",
    "# Modern Hopfield  \n",
    "im2 = axes[1].contourf(X.numpy(), Y.numpy(), E_modern_2d.numpy(), levels=30, cmap='viridis')\n",
    "axes[1].scatter(patterns_2d[:, 0].numpy(), patterns_2d[:, 1].numpy(), \n",
    "                c='red', s=200, marker='*', label='Stored patterns', zorder=5)\n",
    "axes[1].set_xlabel('State dimension 1')\n",
    "axes[1].set_ylabel('State dimension 2')\n",
    "axes[1].set_title('Modern Hopfield Energy\\\\n(Exponential: deep, sharp valleys)')\n",
    "axes[1].legend()\n",
    "axes[1].set_aspect('equal')\n",
    "plt.colorbar(im2, ax=axes[1], label='Energy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Modern Hopfield has SHARPER minima around stored patterns!\")\n",
    "print(\"This is why it can store exponentially more patterns without interference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9dd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: logsumexp as a \"soft maximum\"\n",
    "# This is the key insight connecting Hopfield energy to attention\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "# Consider logsumexp of [x, 0] - how does it compare to max(x, 0)?\n",
    "lse = torch.logsumexp(torch.stack([x, torch.zeros_like(x)], dim=1), dim=1)\n",
    "hard_max = torch.maximum(x, torch.zeros_like(x))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Compare logsumexp to max\n",
    "ax1.plot(x.numpy(), lse.numpy(), 'b-', linewidth=2, label='logsumexp(x, 0)')\n",
    "ax1.plot(x.numpy(), hard_max.numpy(), 'r--', linewidth=2, label='max(x, 0)')\n",
    "ax1.axhline(y=0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax1.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('Output')\n",
    "ax1.set_title('logsumexp is a \"Soft Maximum\"\\\\n(Smooth, differentiable everywhere)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Effect of beta (temperature) on softmax\n",
    "betas = [0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "scores = torch.tensor([1.0, 0.5, 0.3, 0.1, 0.05])  # Different similarities\n",
    "\n",
    "for beta in betas:\n",
    "    probs = F.softmax(beta * scores, dim=0).numpy()\n",
    "    ax2.plot(range(5), probs, 'o-', linewidth=2, markersize=8, label=f'β={beta}')\n",
    "\n",
    "ax2.set_xlabel('Pattern index')\n",
    "ax2.set_ylabel('Attention weight')\n",
    "ax2.set_title('Higher β → Sharper Attention\\\\n(More like argmax)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: β controls the exploration/exploitation tradeoff\")\n",
    "print(\"  - Low β: Soft attention, consider all patterns (exploration)\")\n",
    "print(\"  - High β: Hard attention, pick the best match (exploitation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def modern_hopfield_energy(x, patterns, beta=1.0):\n",
    "    \"\"\"\n",
    "    Modern Hopfield energy with exponential interaction.\n",
    "    \n",
    "    E(x) = -logsumexp(beta * Xi^T x) + 0.5 * ||x||^2\n",
    "    \n",
    "    This energy has EXPONENTIALLY many local minima (one per pattern),\n",
    "    compared to O(N) for classic Hopfield!\n",
    "    \n",
    "    Args:\n",
    "        x: Query state (d,) or (batch, d)\n",
    "        patterns: Stored patterns (n_patterns, d)\n",
    "        beta: Inverse temperature (higher = sharper attention)\n",
    "    \"\"\"\n",
    "    # Xi^T x: similarity scores (n_patterns,) or (batch, n_patterns)\n",
    "    scores = x @ patterns.t() * beta\n",
    "    return -torch.logsumexp(scores, dim=-1) + 0.5 * (x ** 2).sum(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def modern_hopfield_update(x, patterns, beta=1.0):\n",
    "    \"\"\"\n",
    "    One step of Modern Hopfield dynamics.\n",
    "    \n",
    "    x_new = Xi * softmax(beta * Xi^T x)\n",
    "    \n",
    "    This IS attention: Query x attends to keys/values Xi.\n",
    "    The output is a weighted sum of patterns, with weights determined\n",
    "    by similarity to the query.\n",
    "    \"\"\"\n",
    "    # Attention scores: how similar is x to each pattern?\n",
    "    scores = x @ patterns.t() * beta  # (batch, n_patterns)\n",
    "    attention_weights = F.softmax(scores, dim=-1)  # (batch, n_patterns)\n",
    "    \n",
    "    # Weighted combination of patterns\n",
    "    x_new = attention_weights @ patterns  # (batch, d)\n",
    "    \n",
    "    return x_new, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Modern Hopfield pattern retrieval\n",
    "\n",
    "# Create some random \"memory\" patterns (like tokens in a sequence)\n",
    "d = 64  # dimension\n",
    "n_patterns = 10\n",
    "\n",
    "# Stored patterns (normalized for stability)\n",
    "stored = torch.randn(n_patterns, d)\n",
    "stored = stored / stored.norm(dim=1, keepdim=True)  # normalize\n",
    "\n",
    "print(f\"Stored {n_patterns} patterns of dimension {d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3014c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a noisy query (corrupted pattern 3)\n",
    "query = stored[3] + 0.5 * torch.randn(d)\n",
    "query = query / query.norm()\n",
    "query = query.unsqueeze(0)  # add batch dim\n",
    "\n",
    "print(f\"Query similarity to stored patterns:\")\n",
    "print((query @ stored.t()).squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run modern Hopfield update (= one attention step)\n",
    "retrieved, attn_weights = modern_hopfield_update(query, stored, beta=8.0)\n",
    "\n",
    "print(f\"\\nAttention weights (should peak at pattern 3):\")\n",
    "print(attn_weights.squeeze())\n",
    "print(f\"\\nMax attention on pattern: {attn_weights.argmax().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93635ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: recovered pattern vs original\n",
    "print(f\"Cosine similarity to pattern 3: {F.cosine_similarity(retrieved, stored[3].unsqueeze(0)).item():.4f}\")\n",
    "print(f\"Original query similarity to pattern 3: {F.cosine_similarity(query, stored[3].unsqueeze(0)).item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how beta (temperature) affects attention sharpness\n",
    "betas = [0.5, 1.0, 2.0, 4.0, 8.0, 16.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for ax, beta in zip(axes.flat, betas):\n",
    "    _, attn = modern_hopfield_update(query, stored, beta=beta)\n",
    "    attn_np = attn.squeeze().numpy()\n",
    "    \n",
    "    ax.bar(range(len(attn_np)), attn_np, color='steelblue')\n",
    "    ax.axvline(x=3, color='red', linestyle='--', alpha=0.7, label='True pattern (3)')\n",
    "    ax.set_xlabel('Pattern Index')\n",
    "    ax.set_ylabel('Attention Weight')\n",
    "    ax.set_title(f'β = {beta} (higher = sharper)')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Attention Sharpness vs Temperature (β)\\nHigher β → more like argmax → retrieves single pattern', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f812f",
   "metadata": {},
   "source": [
    "### The Transformer Connection\n",
    "\n",
    "In a transformer attention operates as:\n",
    "\n",
    "**Keys K** = stored patterns (what to match against)\n",
    "\n",
    "**Values V** = what to retrieve (often same as K in self-attention)\n",
    "\n",
    "**Queries Q** = what we're looking for\n",
    "\n",
    "Standard attention: Attention(Q, K, V) = softmax(QK^T / sqrt(d)) V\n",
    "\n",
    "The sqrt(d) scaling is like beta = 1/sqrt(d) in Hopfield terms.\n",
    "\n",
    "**What this means:**\n",
    "- Each attention head is doing **content-based memory retrieval**\n",
    "- The query asks \"what stored patterns am I similar to?\"\n",
    "- The output is a weighted average of the matching values\n",
    "- This is exactly what Hopfield networks do, but continuously and differentiably\n",
    "\n",
    "**The deep insight:** Transformers are doing **energy minimization on an exponential energy landscape**, where stored patterns (context tokens) are attractors. Attention is one step of this minimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f020caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that attention IS the Hopfield update\n",
    "def standard_attention(Q, K, V):\n",
    "    \"\"\"Standard scaled dot-product attention.\"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.t() / np.sqrt(d_k)\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    return attn_weights @ V, attn_weights\n",
    "\n",
    "# Use same patterns as keys and values\n",
    "K = stored  # (n_patterns, d)\n",
    "V = stored  # (n_patterns, d)\n",
    "Q = query   # (1, d)\n",
    "\n",
    "# Standard attention\n",
    "attn_out, attn_weights_std = standard_attention(Q, K, V)\n",
    "\n",
    "# Modern Hopfield (with matching beta)\n",
    "beta_equivalent = 1.0 / np.sqrt(d)\n",
    "hopfield_out, hopfield_weights = modern_hopfield_update(Q, stored, beta=beta_equivalent)\n",
    "\n",
    "print(\"Attention weights:\")\n",
    "print(attn_weights_std.squeeze()[:5].detach().numpy(), \"...\")\n",
    "print(\"\\nHopfield weights:\")\n",
    "print(hopfield_weights.squeeze()[:5].detach().numpy(), \"...\")\n",
    "\n",
    "print(f\"\\nAre they identical? {torch.allclose(attn_weights_std, hopfield_weights)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbfe5b",
   "metadata": {},
   "source": [
    "### Exponential Storage Capacity\n",
    "\n",
    "Classic Hopfield: O(N) patterns for N neurons.\n",
    "\n",
    "Modern Hopfield with exponential energy: O(2^(d/2)) patterns for dimension d!\n",
    "\n",
    "This means a 512-dimensional attention head can theoretically store around 2^256 patterns. In practice, transformers work with context lengths of thousands of tokens, which is far below this theoretical limit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6fc9b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Other Cool Energy-Based Insights\n",
    "\n",
    "### Energy-Based Models (EBMs) as a Unifying Framework\n",
    "\n",
    "Many ML models can be viewed through the lens of energy functions:\n",
    "\n",
    "1. **Hopfield/Boltzmann**: E(x) = -x^T W x (quadratic)\n",
    "2. **Modern Hopfield/Attention**: E(x) = -logsumexp(Xi^T x) (exponential)\n",
    "3. **Variational Autoencoders**: ELBO is an energy bound\n",
    "4. **Diffusion Models**: Learn to denoise by estimating score = gradient of log probability = -gradient of energy\n",
    "5. **Contrastive Learning**: Pushes positive pairs to low energy, negatives to high energy\n",
    "\n",
    "### Connection to Diffusion Models\n",
    "\n",
    "Diffusion models learn the **score function**: the gradient of log probability.\n",
    "\n",
    "Since P(x) ∝ exp(-E(x)), we have:\n",
    "- log P(x) = -E(x) + const\n",
    "- Score = ∇ log P(x) = -∇E(x)\n",
    "\n",
    "Denoising is literally **gradient descent on energy**! The model learns to push noisy samples toward low-energy (high probability) regions.\n",
    "\n",
    "### Hopfield Networks in Biological Memory\n",
    "\n",
    "The energy landscape metaphor connects to neuroscience:\n",
    "- **Attractor dynamics**: Brain states settle into stable \"attractors\"\n",
    "- **Memory consolidation**: During sleep, the brain may be doing something like simulated annealing\n",
    "- **Pattern completion**: Hippocampus does content-addressable memory retrieval\n",
    "\n",
    "This isn't just a metaphor—there's real evidence that neural populations exhibit attractor dynamics during memory retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5862e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "We've built intuition from first principles:\n",
    "\n",
    "1. **Energy minimization** is nature's universal optimization algorithm\n",
    "2. **Hopfield networks** encode memories as valleys in an energy landscape\n",
    "3. **Hebbian learning** creates these valleys through outer products\n",
    "4. **Dynamics** evolve states toward nearest memory (attractor)\n",
    "5. **Capacity limits** exist (~0.14N patterns for classic Hopfield)\n",
    "6. **Boltzmann machines** add temperature for exploration\n",
    "7. **RBMs** make this tractable with bipartite structure\n",
    "8. **Modern Hopfield networks** use exponential energy for massive capacity\n",
    "9. **Transformer attention IS Hopfield retrieval** — one step of energy minimization\n",
    "\n",
    "The energy perspective unifies many seemingly disparate ideas in machine learning, from memory networks to diffusion models to attention mechanisms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f403d4",
   "metadata": {},
   "source": [
    "### Final Challenge: Build a Multi-Head Hopfield Attention Layer\n",
    "\n",
    "Implement a multi-head version of modern Hopfield attention that matches PyTorch's MultiheadAttention interface:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestHopfieldMultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention implemented as multi-head Hopfield retrieval.\n",
    "    \n",
    "    TODO: Complete the forward method.\n",
    "    \"\"\"\n",
    "    def __init__(self, test_embed_dim, test_num_heads):\n",
    "        super().__init__()\n",
    "        self.test_embed_dim = test_embed_dim\n",
    "        self.test_num_heads = test_num_heads\n",
    "        self.test_head_dim = test_embed_dim // test_num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.test_W_q = nn.Linear(test_embed_dim, test_embed_dim)\n",
    "        self.test_W_k = nn.Linear(test_embed_dim, test_embed_dim)\n",
    "        self.test_W_v = nn.Linear(test_embed_dim, test_embed_dim)\n",
    "        self.test_W_out = nn.Linear(test_embed_dim, test_embed_dim)\n",
    "    \n",
    "    def forward(self, test_query, test_key, test_value):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            test_query: (batch, seq_q, embed_dim)\n",
    "            test_key: (batch, seq_k, embed_dim)\n",
    "            test_value: (batch, seq_v, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_q, embed_dim)\n",
    "        \"\"\"\n",
    "        # FILL IN CODE HERE\n",
    "        # 1. Project Q, K, V\n",
    "        # 2. Reshape for multi-head: (batch, seq, n_heads, head_dim) -> (batch, n_heads, seq, head_dim)\n",
    "        # 3. For each head, apply Hopfield update: softmax(Q @ K^T / sqrt(d)) @ V\n",
    "        # 4. Concatenate heads and apply output projection\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Test: should work like regular attention\n",
    "test_batch, test_seq = 2, 5\n",
    "test_embed = 64\n",
    "test_heads = 4\n",
    "\n",
    "test_model = TestHopfieldMultiheadAttention(test_embed, test_heads)\n",
    "test_x = torch.randn(test_batch, test_seq, test_embed)\n",
    "\n",
    "# Uncomment after implementing:\n",
    "# test_out = test_model(test_x, test_x, test_x)  # self-attention\n",
    "# assert test_out.shape == (test_batch, test_seq, test_embed), f\"Expected shape {(test_batch, test_seq, test_embed)}, got {test_out.shape}\"\n",
    "# print(\"✓ Passed: Output shape is correct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20f8ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Shortcuts and Libraries\n",
    "\n",
    "Now that you understand the fundamentals, here are production shortcuts:\n",
    "\n",
    "**For RBMs:**\n",
    "- `sklearn.neural_network.BernoulliRBM`: Simple RBM implementation\n",
    "- `pytorch-rbm`: GPU-accelerated RBM with CD-k\n",
    "\n",
    "**For Attention as Hopfield:**\n",
    "- `hopfield-layers` package: Implements modern Hopfield networks from the 2020 paper\n",
    "- PyTorch's `nn.MultiheadAttention`: Standard attention (which IS Hopfield)\n",
    "\n",
    "**For Energy-Based Models:**\n",
    "- `ebm-torch`: General EBM training\n",
    "- Diffusion libraries (e.g., `diffusers`): EBMs trained with score matching\n",
    "\n",
    "The Hopfield perspective helps debug attention: if attention weights are too uniform (high temperature), patterns aren't being retrieved sharply. If weights collapse to single tokens, you're over-indexing on specific memories.liked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9ac83",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "1. **Original Hopfield Paper**: Hopfield, J.J. (1982). \"Neural networks and physical systems with emergent collective computational abilities\"\n",
    "\n",
    "2. **Boltzmann Machines**: Hinton & Sejnowski (1986). \"Learning and relearning in Boltzmann machines\"\n",
    "\n",
    "3. **Contrastive Divergence**: Hinton (2002). \"Training products of experts by minimizing contrastive divergence\"\n",
    "\n",
    "4. **Modern Hopfield Networks**: Ramsauer et al. (2020). \"Hopfield Networks is All You Need\" — The key paper connecting Hopfield to attention\n",
    "\n",
    "5. **Energy-Based Models**: LeCun et al. (2006). \"A Tutorial on Energy-Based Learning\"\n",
    "\n",
    "6. **Score Matching & Diffusion**: Song & Ermon (2019). \"Generative Modeling by Estimating Gradients of the Data Distribution\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4417337",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02c9cf31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbba0351",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
