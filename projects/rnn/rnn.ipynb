{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "title",
   "source": "# RNNs and LSTMs from Scratch\n\n**The question we're answering:** How do neural networks \"remember\" across time?\n\nFeedforward networks process each input independently\u2014they have no memory. But language, music, and time-series data are *sequential*: the meaning of \"bank\" depends on whether we just saw \"river\" or \"money.\"\n\nThis notebook builds intuition for how RNNs and LSTMs solve this problem. We'll discover:\n1. Why sharing weights across time lets us process sequences\n2. Why training RNNs is surprisingly hard (the vanishing gradient problem)\n3. How LSTMs solve this with a clever \"cell state\" that acts like a conveyor belt\n\nBy the end, the LSTM architecture\u2014which looks intimidating at first\u2014will feel like the *obvious* solution."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "imports",
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\n\n# Add workspace root to Python path so we can import silen_lib\nworkspace_root = Path.cwd().parent.parent\nif str(workspace_root) not in sys.path:\n    sys.path.insert(0, str(workspace_root))\n\nimport silen_lib.utils as utils\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nutils.set_seed(42)\nplt.rcParams['figure.figsize'] = (12, 4)\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "rnn-intro",
   "source": "## The Core RNN Idea\n\nThe simplest way to add memory: **use the previous output as additional input**.\n\nAt each timestep $t$, the RNN combines:\n- The current input $x_t$ \n- The previous hidden state $h_{t-1}$ (this is the \"memory\")\n\n$$h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b)$$\n\n**Key insight:** The same weights $W_{xh}$ and $W_{hh}$ are used at every timestep. This is what makes it \"recurrent\"\u2014we're applying the same function over and over, just with different inputs.\n\nLet's build this from scratch. We'll process the sentence \"The cat sat\" character by character."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "simple-rnn-cell",
   "outputs": [],
   "source": "# | simple-rnn-cell inline expanded code-aside\nclass SimpleRNNCell:\n    \"\"\"A single RNN cell implemented from scratch.\"\"\"\n    \n    def __init__(self, input_size, hidden_size):\n        self.hidden_size = hidden_size\n        \n        # Initialize weights (using Xavier/Glorot initialization)\n        scale = np.sqrt(2.0 / (input_size + hidden_size))\n        self.W_xh = np.random.randn(input_size, hidden_size) * scale\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale\n        self.b = np.zeros(hidden_size)\n        \n    def forward(self, x, h_prev):\n        \"\"\"Single forward step: h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b)\"\"\"\n        h_new = np.tanh(x @ self.W_xh + h_prev @ self.W_hh + self.b)\n        return h_new\n    \n    def init_hidden(self, batch_size):\n        \"\"\"Initialize hidden state to zeros.\"\"\"\n        return np.zeros((batch_size, self.hidden_size))\n\n# Let's process \"The cat sat\" one character at a time\n# First, create simple one-hot encodings for our characters\ntext = \"The cat sat\"\nchars = sorted(set(text))\nchar_to_idx = {c: i for i, c in enumerate(chars)}\n\nprint(f\"Our vocabulary: {chars}\")\nprint(f\"Vocabulary size: {len(chars)}\")\n\n# One-hot encode each character\ndef one_hot(char, vocab_size):\n    vec = np.zeros((1, vocab_size))\n    vec[0, char_to_idx[char]] = 1.0\n    return vec\n\n# Create RNN: input_size = vocab_size, hidden_size = 8 (small for visualization)\nrnn_cell = SimpleRNNCell(input_size=len(chars), hidden_size=8)\nh = rnn_cell.init_hidden(1)\n\nprint(f\"\\nProcessing '{text}' character by character:\")\nprint(\"-\" * 50)\n\nfor i, char in enumerate(text):\n    x = one_hot(char, len(chars))\n    h = rnn_cell.forward(x, h)\n    # Show how hidden state evolves\n    print(f\"'{char}' -> h mean: {h.mean():.3f}, h std: {h.std():.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "sequence-processing",
   "source": "Notice how each character changes the hidden state. After seeing \"The c\", the hidden state contains a *compressed summary* of everything seen so far.\n\n## Processing Full Sequences\n\nIn practice, we wrap our RNN cell to process entire sequences at once, storing all hidden states for later use."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "process-sequence",
   "outputs": [],
   "source": "class SimpleRNN:\n    \"\"\"Full RNN that processes sequences and stores all hidden states.\"\"\"\n    \n    def __init__(self, input_size, hidden_size):\n        self.cell = SimpleRNNCell(input_size, hidden_size)\n        self.hidden_size = hidden_size\n        \n    def forward(self, sequence):\n        \"\"\"Process an entire sequence.\n        \n        Args:\n            sequence: (seq_len, batch_size, input_size)\n        \n        Returns:\n            outputs: All hidden states (seq_len, batch_size, hidden_size)\n            h_final: Final hidden state (batch_size, hidden_size)\n        \"\"\"\n        seq_len, batch_size, _ = sequence.shape\n        h = self.cell.init_hidden(batch_size)\n        \n        outputs = []\n        for t in range(seq_len):\n            h = self.cell.forward(sequence[t], h)\n            outputs.append(h)\n            \n        return np.stack(outputs), h\n\n# Prepare our text as a sequence of one-hot vectors\nsequence = np.stack([one_hot(c, len(chars))[0] for c in text])  # (seq_len, vocab_size)\nsequence = sequence[:, np.newaxis, :]  # Add batch dim: (seq_len, 1, vocab_size)\n\nprint(f\"Sequence shape: {sequence.shape} = (seq_len={len(text)}, batch=1, vocab={len(chars)})\")\n\n# Process with our RNN\nrnn = SimpleRNN(input_size=len(chars), hidden_size=8)\nall_hidden, final_hidden = rnn.forward(sequence)\n\nprint(f\"All hidden states shape: {all_hidden.shape}\")\nprint(f\"Final hidden state shape: {final_hidden.shape}\")\nprint(f\"\\nThe final hidden state is our 'summary' of '{text}'\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "unrolling-viz",
   "source": "## Visualizing Hidden States Over Time\n\n**What to look for:** In the heatmap below, each column is a timestep (a character from our text). Each row is a hidden unit. Watch how the pattern of activations changes as we read through \"The cat sat\".\n\n**Hypothesis:** If the RNN is working, we should see:\n1. Similar patterns when we see the same character (e.g., 'a' appears twice)\n2. Distinct patterns for different characters\n3. Smooth transitions\u2014nearby timesteps should look related"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "unroll-viz",
   "outputs": [],
   "source": "# | unroll-viz inline visualization\ndef visualize_hidden_states(outputs, chars_sequence, title=\"Hidden State Evolution\"):\n    \"\"\"Visualize how hidden states evolve over time.\"\"\"\n    states = outputs[:, 0, :]  # Take first batch: (seq_len, hidden_size)\n    \n    fig, ax = plt.subplots(figsize=(14, 5))\n    im = ax.imshow(states.T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n    \n    # Label x-axis with actual characters\n    ax.set_xticks(range(len(chars_sequence)))\n    ax.set_xticklabels(list(chars_sequence), fontsize=12)\n    \n    plt.colorbar(im, label='Activation (tanh output: -1 to 1)')\n    ax.set_xlabel('Character in sequence')\n    ax.set_ylabel('Hidden Unit')\n    ax.set_title(title)\n    plt.tight_layout()\n    plt.show()\n\nvisualize_hidden_states(all_hidden, text, f\"Hidden States While Reading '{text}'\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "bptt-section",
   "source": "## Training RNNs: Backpropagation Through Time (BPTT)\n\nNow we understand the forward pass. But how do we *train* this thing?\n\n**The key insight:** When we unroll an RNN across time, it looks like a very deep feedforward network where each \"layer\" is a timestep.\n\nTo compute gradients, we backpropagate through this unrolled network. The gradient for the weights at timestep $t$ depends on *all future timesteps*:\n\n$$\\frac{\\partial L}{\\partial W} = \\sum_{t=1}^{T} \\frac{\\partial L_t}{\\partial h_t} \\cdot \\underbrace{\\prod_{k=1}^{t} \\frac{\\partial h_k}{\\partial h_{k-1}}}_{\\text{This product is the problem!}}$$\n\nThat product of Jacobians is where things go wrong. Let's implement BPTT to see what happens."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "bptt-impl",
   "outputs": [],
   "source": "# | bptt-impl inline expanded code-aside\nclass RNNWithBPTT:\n    \"\"\"RNN with full BPTT implementation to track gradient flow.\"\"\"\n    \n    def __init__(self, input_size, hidden_size):\n        self.hidden_size = hidden_size\n        self.input_size = input_size\n        \n        # Initialize weights\n        scale = np.sqrt(2.0 / (input_size + hidden_size))\n        self.W_xh = np.random.randn(input_size, hidden_size) * scale\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale\n        self.b = np.zeros(hidden_size)\n        \n    def forward(self, sequence):\n        \"\"\"Forward pass, storing values needed for backprop.\"\"\"\n        seq_len, batch_size, _ = sequence.shape\n        \n        self.inputs = sequence\n        self.hiddens = [np.zeros((batch_size, self.hidden_size))]\n        self.pre_activations = []\n        \n        for t in range(seq_len):\n            pre_act = sequence[t] @ self.W_xh + self.hiddens[-1] @ self.W_hh + self.b\n            h = np.tanh(pre_act)\n            self.pre_activations.append(pre_act)\n            self.hiddens.append(h)\n            \n        return np.stack(self.hiddens[1:]), self.hiddens[-1]\n    \n    def backward(self, d_outputs):\n        \"\"\"Full BPTT backward pass. Returns gradient magnitudes at each timestep.\"\"\"\n        seq_len = len(self.pre_activations)\n        batch_size = d_outputs.shape[1]\n        \n        # Gradient flowing back through hidden states\n        dh_next = np.zeros((batch_size, self.hidden_size))\n        grad_magnitudes = []\n        \n        # Backprop through time (reverse order!)\n        for t in reversed(range(seq_len)):\n            dh = d_outputs[t] + dh_next\n            \n            # Gradient through tanh: d/dx tanh(x) = 1 - tanh(x)^2\n            d_pre_act = dh * (1 - np.tanh(self.pre_activations[t])**2)\n            \n            # Gradient to previous hidden state (this is what we multiply repeatedly)\n            dh_next = d_pre_act @ self.W_hh.T\n            \n            grad_magnitudes.append(np.linalg.norm(dh_next))\n            \n        return list(reversed(grad_magnitudes))\n\n# Test on a 10-step sequence\nrnn_bptt = RNNWithBPTT(input_size=len(chars), hidden_size=8)\noutputs, _ = rnn_bptt.forward(sequence)\n\n# Imagine our loss is the sum of all outputs\nd_outputs = np.ones_like(outputs) * 0.1\ngrad_mags = rnn_bptt.backward(d_outputs)\n\nprint(\"Gradient magnitude at each timestep (from end back to start):\")\nprint(\"-\" * 55)\nfor t, (char, mag) in enumerate(zip(text, grad_mags)):\n    bar = \"#\" * int(mag * 50)\n    print(f\"t={t:2d} '{char}': {mag:.4f} {bar}\")\n\nprint(\"\\n** KEY TAKEAWAY **\")\nprint(\"Notice: gradients are NOT decaying much here because our sequence is short (11 chars).\")\nprint(\"The vanishing gradient problem appears with LONGER sequences. Let's see that next.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "vanishing-gradients",
   "source": "## The Vanishing Gradient Problem\n\n**Why do gradients vanish?**\n\nDuring backprop, we multiply gradients through each timestep. If these multiplications are mostly < 1, the gradient shrinks exponentially.\n\nThink about it: if you multiply 0.9 by itself 100 times, you get $0.9^{100} \\approx 0.00003$.\n\n**What to look for in the plot below:**\n- Each line is a different sequence length\n- The y-axis (log scale) shows gradient magnitude relative to the final timestep\n- Watch how the gradient at the *start* of the sequence gets smaller as sequences get longer\n- For length 100: the first timestep gets almost no gradient signal!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "vanishing-demo",
   "outputs": [],
   "source": "# | vanishing-demo inline visualization\ndef demonstrate_vanishing_gradients(seq_lengths=[10, 25, 50, 100]):\n    \"\"\"Show how gradients vanish with increasing sequence length.\"\"\"\n    \n    fig, ax = plt.subplots(figsize=(12, 5))\n    \n    for seq_len in seq_lengths:\n        # Create random sequence data\n        fake_sequence = np.random.randn(seq_len, 1, 10) * 0.1\n        \n        rnn = RNNWithBPTT(input_size=10, hidden_size=20)\n        outputs, _ = rnn.forward(fake_sequence)\n        d_outputs = np.ones_like(outputs) * 0.1\n        grad_mags = rnn.backward(d_outputs)\n        \n        # Normalize so we can compare different lengths\n        grad_mags = np.array(grad_mags) / max(grad_mags)\n        ax.plot(range(seq_len), grad_mags, label=f'Length {seq_len}', alpha=0.8, linewidth=2)\n    \n    ax.set_xlabel('Timestep (position in sequence)')\n    ax.set_ylabel('Relative Gradient Magnitude (log scale)')\n    ax.set_title('Vanishing Gradients: Earlier timesteps get exponentially less gradient')\n    ax.legend()\n    ax.set_yscale('log')\n    ax.grid(True, alpha=0.3)\n    ax.axhline(y=0.01, color='red', linestyle='--', alpha=0.5, label='1% threshold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"IMPLICATION: For length-100 sequences, the RNN can barely learn from the first 50 tokens!\")\n    print(\"This is why vanilla RNNs fail at long-range dependencies.\")\n\ndemonstrate_vanishing_gradients()"
  },
  {
   "cell_type": "markdown",
   "id": "870rq1fqn04",
   "source": "## The Math Behind Vanishing Gradients\n\nLet's make this concrete with a simple model: what happens when you multiply a number by itself many times?\n\n**Hypothesis:** If our \"gradient retention factor\" is less than 1 (say, 0.9), after 100 multiplications we'll have essentially nothing left. If it's greater than 1 (say, 1.1), we'll explode to infinity.\n\n**What to look for:**\n- Left plot: linear scale shows the gradient plummeting to zero\n- Right plot: log scale reveals the exponential nature of decay\n- Notice how even 0.9 (which seems close to 1!) leads to almost complete signal loss",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "exponential-decay",
   "outputs": [],
   "source": "# | exponential-decay inline visualization\n# Exponential decay demonstration\ndecay_factor = 0.9\ntimesteps = 100\n\n# What fraction of gradient remains after t steps?\ngradient_remaining = [decay_factor ** t for t in range(timesteps)]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Linear scale\naxes[0].plot(gradient_remaining, linewidth=2)\naxes[0].set_xlabel('Timesteps back')\naxes[0].set_ylabel('Gradient remaining')\naxes[0].set_title(f'Gradient decay with factor = {decay_factor}')\naxes[0].grid(True, alpha=0.3)\naxes[0].axhline(y=0.01, color='red', linestyle='--', alpha=0.7)\naxes[0].text(60, 0.3, f'After just 50 steps:\\n{decay_factor**50:.4f} remaining', fontsize=10)\n\n# Log scale (reveals exponential nature)\naxes[1].semilogy(gradient_remaining, linewidth=2)\naxes[1].set_xlabel('Timesteps back')\naxes[1].set_ylabel('Gradient remaining (log scale)')\naxes[1].set_title('Same data, log scale - reveals exponential decay')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Starting gradient: 1.0\")\nprint(f\"After 50 timesteps:  {decay_factor**50:.6f} ({decay_factor**50 * 100:.4f}%)\")\nprint(f\"After 100 timesteps: {decay_factor**100:.2e} ({decay_factor**100 * 100:.6f}%)\")\nprint()\nprint(\"This is why RNNs 'forget' long-range dependencies!\")\nprint(\"The gradient signal from early tokens is essentially ZERO by the time it reaches the weights.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "gradient-clipping-section",
   "source": "## Gradient Clipping: A Partial Fix\n\nBefore we get to LSTMs, let's address the *opposite* problem: **exploding gradients**.\n\nIf the gradient retention factor is > 1, gradients grow exponentially. This causes:\n- NaN losses during training\n- Wild parameter updates that destabilize learning\n\n**The fix:** If gradients get too big, scale them down to a maximum norm. This is called \"gradient clipping.\"\n\nNote: Clipping helps with exploding gradients but does NOT help with vanishing gradients. We need a different architecture for that."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "gradient-clipping",
   "outputs": [],
   "source": "def clip_gradients(gradients, max_norm=1.0):\n    \"\"\"Clip gradients to prevent explosion.\n    \n    If total_norm > max_norm, scale all gradients down proportionally.\n    \"\"\"\n    total_norm = np.sqrt(sum(np.sum(g**2) for g in gradients))\n    \n    if total_norm > max_norm:\n        scale = max_norm / total_norm\n        clipped = [g * scale for g in gradients]\n        return clipped, True, total_norm\n    return gradients, False, total_norm\n\n# Simulate exploding gradients\nlarge_gradients = [np.random.randn(10, 20) * 10 for _ in range(3)]\n\noriginal_norm = np.sqrt(sum(np.sum(g**2) for g in large_gradients))\nclipped, was_clipped, _ = clip_gradients(large_gradients, max_norm=5.0)\nclipped_norm = np.sqrt(sum(np.sum(g**2) for g in clipped))\n\nprint(f\"Original gradient norm: {original_norm:.2f}\")\nprint(f\"Max allowed norm: 5.0\")\nprint(f\"Was clipped: {was_clipped}\")\nprint(f\"Clipped gradient norm: {clipped_norm:.2f}\")\nprint()\nprint(\"In PyTorch, this is: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "lstm-section",
   "source": "## The LSTM: Solving Vanishing Gradients\n\nWe've seen the problem: gradients vanish because we *multiply* through each timestep.\n\n**The LSTM insight:** What if we could *add* instead of multiply?\n\nThe LSTM introduces a **cell state** $C_t$ that flows through time with minimal modification. Think of it like a conveyor belt: information can hop on and off, but the belt itself keeps moving.\n\nThree \"gates\" control the flow:\n1. **Forget gate** ($f_t$): How much of the old cell state to keep (0 = forget everything, 1 = keep everything)\n2. **Input gate** ($i_t$): How much of the new candidate to add\n3. **Output gate** ($o_t$): How much of the cell state to expose as output\n\n$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\quad \\text{(forget gate: what to erase)}$$\n$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\quad \\text{(input gate: what to write)}$$\n$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\quad \\text{(candidate: new information)}$$\n$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\quad \\text{(update cell state)}$$\n$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\quad \\text{(output gate: what to reveal)}$$\n$$h_t = o_t \\odot \\tanh(C_t) \\quad \\text{(hidden state)}$$\n\n**Why this helps:** If the forget gate is close to 1, gradients flow through $C_t$ almost unchanged! No vanishing."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "lstm-impl",
   "outputs": [],
   "source": "# | lstm-impl inline expanded code-aside\ndef sigmoid(x):\n    \"\"\"Numerically stable sigmoid.\"\"\"\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\nclass LSTMCell:\n    \"\"\"A single LSTM cell implemented from scratch.\"\"\"\n    \n    def __init__(self, input_size, hidden_size):\n        self.hidden_size = hidden_size\n        combined_size = input_size + hidden_size\n        scale = np.sqrt(2.0 / combined_size)\n        \n        # Forget gate weights\n        self.W_f = np.random.randn(combined_size, hidden_size) * scale\n        self.b_f = np.ones(hidden_size)  # Initialize forget bias to 1! (keep everything at start)\n        \n        # Input gate weights\n        self.W_i = np.random.randn(combined_size, hidden_size) * scale\n        self.b_i = np.zeros(hidden_size)\n        \n        # Candidate values weights\n        self.W_c = np.random.randn(combined_size, hidden_size) * scale\n        self.b_c = np.zeros(hidden_size)\n        \n        # Output gate weights\n        self.W_o = np.random.randn(combined_size, hidden_size) * scale\n        self.b_o = np.zeros(hidden_size)\n        \n    def forward(self, x, h_prev, c_prev):\n        \"\"\"Single LSTM step. Returns new hidden state, cell state, and gate values.\"\"\"\n        # Concatenate input and previous hidden state\n        combined = np.concatenate([x, h_prev], axis=1)\n        \n        # Compute all gates\n        f = sigmoid(combined @ self.W_f + self.b_f)  # Forget: what to erase\n        i = sigmoid(combined @ self.W_i + self.b_i)  # Input: what to write\n        c_tilde = np.tanh(combined @ self.W_c + self.b_c)  # Candidate: new info\n        o = sigmoid(combined @ self.W_o + self.b_o)  # Output: what to reveal\n        \n        # Update cell state: erase old + write new\n        c_new = f * c_prev + i * c_tilde\n        \n        # Compute hidden state from cell state\n        h_new = o * np.tanh(c_new)\n        \n        gates = {'forget': f, 'input': i, 'output': o, 'candidate': c_tilde}\n        return h_new, c_new, gates\n    \n    def init_state(self, batch_size):\n        \"\"\"Initialize hidden and cell states to zeros.\"\"\"\n        return np.zeros((batch_size, self.hidden_size)), np.zeros((batch_size, self.hidden_size))\n\n# Test LSTM on our \"The cat sat\" text\nlstm_cell = LSTMCell(input_size=len(chars), hidden_size=8)\nh, c = lstm_cell.init_state(1)\n\nprint(f\"Processing '{text}' with LSTM:\")\nprint(\"-\" * 60)\n\nfor char in text:\n    x = one_hot(char, len(chars))\n    h, c, gates = lstm_cell.forward(x, h, c)\n    # Show forget gate (most important for understanding LSTM behavior)\n    f_mean = gates['forget'].mean()\n    print(f\"'{char}' -> forget gate mean: {f_mean:.3f} ({'keeping ~' + str(int(f_mean*100)) + '%'})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "full-lstm",
   "source": "Notice how the forget gate hovers around 0.6-0.8, meaning the LSTM is keeping most of the cell state at each step. This is how information persists across long sequences!\n\n## Full LSTM for Sequences"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "lstm-sequence",
   "outputs": [],
   "source": "class LSTM:\n    \"\"\"Full LSTM that processes sequences.\"\"\"\n    \n    def __init__(self, input_size, hidden_size):\n        self.cell = LSTMCell(input_size, hidden_size)\n        self.hidden_size = hidden_size\n        \n    def forward(self, sequence, return_all_gates=False):\n        \"\"\"Process a sequence, optionally returning all gate activations.\"\"\"\n        seq_len, batch_size, _ = sequence.shape\n        h, c = self.cell.init_state(batch_size)\n        \n        outputs, cell_states, all_gates = [], [], []\n        \n        for t in range(seq_len):\n            h, c, gates = self.cell.forward(sequence[t], h, c)\n            outputs.append(h)\n            cell_states.append(c)\n            all_gates.append(gates)\n            \n        result = (np.stack(outputs), (h, c))\n        if return_all_gates:\n            result = result + (all_gates, np.stack(cell_states))\n        return result\n\n# Process our text sequence\nlstm = LSTM(input_size=len(chars), hidden_size=8)\noutputs, (h_final, c_final), all_gates, cell_states = lstm.forward(sequence, return_all_gates=True)\n\nprint(f\"Input: '{text}'\")\nprint(f\"Sequence shape: {sequence.shape}\")\nprint(f\"All outputs shape: {outputs.shape}\")\nprint(f\"Cell states shape: {cell_states.shape}\")\nprint(f\"\\nThe cell state is our 'long-term memory'\")\nprint(f\"The hidden state is our 'working memory' (what we expose to the next layer)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "gates-viz",
   "source": "## Visualizing LSTM Gate Behavior\n\n**What to look for in these plots:**\n\n1. **Forget gate (top-left):** Values close to 1 mean \"keep old information.\" Watch for when it drops\u2014that's when the LSTM decides to \"forget\" something.\n\n2. **Input gate (top-right):** High values mean \"this new input is important, write it to memory.\" \n\n3. **Output gate (bottom-left):** Controls what gets exposed. High = \"use this cell state for output.\"\n\n4. **Cell state magnitude (bottom-right):** Shows if information is accumulating. Should grow smoothly, not explode.\n\nThe gray dashed line at 0.5 is the \"neutral\" point\u2014above means \"do this,\" below means \"don't.\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "visualize-gates",
   "outputs": [],
   "source": "# | visualize-gates inline visualization\ndef visualize_lstm_gates(all_gates, cell_states, chars_sequence):\n    \"\"\"Visualize how LSTM gates behave while reading text.\"\"\"\n    seq_len = len(all_gates)\n    \n    # Extract gate values (average across hidden units)\n    forget_vals = [g['forget'].mean() for g in all_gates]\n    input_vals = [g['input'].mean() for g in all_gates]\n    output_vals = [g['output'].mean() for g in all_gates]\n    cell_magnitude = [np.abs(c).mean() for c in cell_states]\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n    \n    # Forget gate\n    axes[0, 0].plot(forget_vals, 'b-o', linewidth=2, markersize=6)\n    axes[0, 0].set_title('Forget Gate: How much to keep from previous')\n    axes[0, 0].set_ylim(0, 1)\n    axes[0, 0].set_ylabel('Gate value')\n    axes[0, 0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n    axes[0, 0].set_xticks(range(len(chars_sequence)))\n    axes[0, 0].set_xticklabels(list(chars_sequence))\n    \n    # Input gate\n    axes[0, 1].plot(input_vals, 'g-o', linewidth=2, markersize=6)\n    axes[0, 1].set_title('Input Gate: How much new info to write')\n    axes[0, 1].set_ylim(0, 1)\n    axes[0, 1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n    axes[0, 1].set_xticks(range(len(chars_sequence)))\n    axes[0, 1].set_xticklabels(list(chars_sequence))\n    \n    # Output gate\n    axes[1, 0].plot(output_vals, 'r-o', linewidth=2, markersize=6)\n    axes[1, 0].set_title('Output Gate: How much to reveal')\n    axes[1, 0].set_ylim(0, 1)\n    axes[1, 0].set_xlabel('Character')\n    axes[1, 0].set_ylabel('Gate value')\n    axes[1, 0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n    axes[1, 0].set_xticks(range(len(chars_sequence)))\n    axes[1, 0].set_xticklabels(list(chars_sequence))\n    \n    # Cell state magnitude\n    axes[1, 1].plot(cell_magnitude, 'm-o', linewidth=2, markersize=6)\n    axes[1, 1].set_title('Cell State Magnitude (accumulated information)')\n    axes[1, 1].set_xlabel('Character')\n    axes[1, 1].set_xticks(range(len(chars_sequence)))\n    axes[1, 1].set_xticklabels(list(chars_sequence))\n    \n    plt.tight_layout()\n    plt.suptitle(f\"LSTM Gates While Reading '{chars_sequence}'\", y=1.02, fontsize=14)\n    plt.show()\n\nvisualize_lstm_gates(all_gates, cell_states, text)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "activation-section",
   "source": "## Why Sigmoid for Gates? Why Tanh for Values?\n\nLSTMs make a deliberate choice:\n- **Sigmoid (0 to 1)** for gates: We need values that act as \"percentages\" \u2014 how much to keep, write, reveal\n- **Tanh (-1 to 1)** for values: We need values that can be positive OR negative, centered around zero\n\n**What to look for below:**\n- Sigmoid squashes everything to [0, 1] \u2014 perfect for \"how much?\" questions\n- Tanh is zero-centered and bounded \u2014 perfect for representing \"how positive or negative?\"\n- ReLU is unbounded and can \"die\" (output 0 forever) \u2014 not suitable for gating"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "activation-comparison",
   "outputs": [],
   "source": "# | activation-comparison inline visualization\ndef compare_activations():\n    \"\"\"Compare ReLU, Sigmoid, and Tanh to understand why LSTMs use each.\"\"\"\n    x = np.linspace(-5, 5, 200)\n    \n    relu = np.maximum(0, x)\n    sigmoid_vals = 1 / (1 + np.exp(-x))\n    tanh_vals = np.tanh(x)\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    # ReLU\n    axes[0].plot(x, relu, 'g-', linewidth=2)\n    axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n    axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n    axes[0].fill_between(x, 0, relu, alpha=0.2, color='green')\n    axes[0].set_title('ReLU: max(0, x)', fontsize=12)\n    axes[0].set_xlabel('x')\n    axes[0].set_ylabel('f(x)')\n    axes[0].set_ylim(-1, 5)\n    axes[0].text(-4, 4, 'Range: [0, inf)\\n\\nGood for hidden layers\\nBad for gates (unbounded!)', fontsize=10)\n    \n    # Sigmoid\n    axes[1].plot(x, sigmoid_vals, 'b-', linewidth=2)\n    axes[1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n    axes[1].axhline(y=0, color='lightgray', linestyle='--', alpha=0.3)\n    axes[1].axhline(y=1, color='lightgray', linestyle='--', alpha=0.3)\n    axes[1].fill_between(x, 0, sigmoid_vals, alpha=0.2, color='blue')\n    axes[1].set_title('Sigmoid: \"How much?\" (used for GATES)', fontsize=12)\n    axes[1].set_xlabel('x')\n    axes[1].set_ylim(-0.2, 1.2)\n    axes[1].text(-4.8, 0.8, 'Range: [0, 1]\\n\\nPerfect for gates!\\n0 = none, 1 = all', fontsize=10)\n    \n    # Tanh\n    axes[2].plot(x, tanh_vals, 'r-', linewidth=2)\n    axes[2].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n    axes[2].fill_between(x, 0, tanh_vals, where=(tanh_vals >= 0), alpha=0.2, color='red')\n    axes[2].fill_between(x, tanh_vals, 0, where=(tanh_vals < 0), alpha=0.2, color='red')\n    axes[2].set_title('Tanh: \"How much +/-?\" (used for VALUES)', fontsize=12)\n    axes[2].set_xlabel('x')\n    axes[2].set_ylim(-1.4, 1.4)\n    axes[2].text(-4.8, 1, 'Range: [-1, 1]\\n\\nZero-centered!\\nCan push info + or -', fontsize=10)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"SUMMARY:\")\n    print(\"- Gates (forget, input, output) use SIGMOID because they answer 'how much?'\")\n    print(\"- Candidate values use TANH because they can add positive OR negative information\")\n    print(\"- This is why LSTM gates are sometimes called 'soft switches'\")\n\ncompare_activations()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "he-init-section",
   "source": "## Initialization Matters: Xavier and He\n\nProper weight initialization is critical for training deep networks (including unrolled RNNs).\n\n**The problem:** If weights are too large, activations explode. Too small, they vanish. \n\n**The solution:** Scale weights based on layer size:\n- **Xavier/Glorot:** `std = sqrt(1/fan_in)` \u2014 good for tanh/sigmoid\n- **He:** `std = sqrt(2/fan_in)` \u2014 good for ReLU\n\n**What to look for below:** After 10 layers of tanh, random initialization produces either exploded or vanishing activations, while Xavier keeps values in a healthy range."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "he-init",
   "outputs": [],
   "source": "def compare_initializations():\n    \"\"\"Compare different weight initialization schemes.\"\"\"\n    hidden_size = 256\n    num_layers = 10\n    \n    def forward_random(x):\n        \"\"\"Random initialization (no scaling).\"\"\"\n        for _ in range(num_layers):\n            W = np.random.randn(hidden_size, hidden_size)  # Too big!\n            x = np.tanh(x @ W)\n        return x\n    \n    def forward_xavier(x):\n        \"\"\"Xavier/Glorot initialization (proper scaling).\"\"\"\n        for _ in range(num_layers):\n            W = np.random.randn(hidden_size, hidden_size) * np.sqrt(1.0 / hidden_size)\n            x = np.tanh(x @ W)\n        return x\n    \n    # Test both\n    x = np.random.randn(32, hidden_size)  # Batch of 32\n    \n    random_out = forward_random(x.copy())\n    xavier_out = forward_xavier(x.copy())\n    \n    print(\"After 10 tanh layers (hidden_size=256):\")\n    print(\"-\" * 50)\n    print(f\"{'Init Method':<15} {'Mean':>10} {'Std':>10} {'Max Abs':>10}\")\n    print(\"-\" * 50)\n    print(f\"{'Random':.<15} {random_out.mean():>10.4f} {random_out.std():>10.4f} {np.abs(random_out).max():>10.4f}\")\n    print(f\"{'Xavier':.<15} {xavier_out.mean():>10.4f} {xavier_out.std():>10.4f} {np.abs(xavier_out).max():>10.4f}\")\n    print()\n    print(\"Random init: activations likely saturated (all -1 or 1) or vanished\")\n    print(\"Xavier init: activations stay in a healthy range for learning\")\n\ncompare_initializations()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "rnn-vs-lstm",
   "source": "## The Proof: LSTM vs RNN Gradient Retention\n\nNow the moment of truth. Let's compare how well gradients flow through RNN vs LSTM.\n\n**What to look for:**\n- The y-axis shows gradient magnitude at the *first* input relative to the last\n- For RNNs: gradient drops exponentially with sequence length\n- For LSTMs: gradient stays relatively stable even for long sequences\n- The gap widens dramatically as sequences get longer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "compare-gradient-flow",
   "outputs": [],
   "source": "# | compare-gradient-flow inline visualization\ndef compare_gradient_retention(seq_lengths=[10, 25, 50, 100, 200]):\n    \"\"\"Compare how RNN and LSTM retain gradients over sequence length.\"\"\"\n    \n    rnn_retention = []\n    lstm_retention = []\n    \n    for seq_len in seq_lengths:\n        # RNN: gradient at first input when loss is at last output\n        rnn = nn.RNN(input_size=10, hidden_size=20, batch_first=False)\n        x_rnn = torch.randn(seq_len, 1, 10, requires_grad=True)\n        outputs_rnn, _ = rnn(x_rnn)\n        loss_rnn = outputs_rnn[-1].sum()  # Loss at final timestep only\n        loss_rnn.backward()\n        rnn_grad = x_rnn.grad[0].abs().mean().item()  # Gradient at FIRST input\n        rnn_retention.append(rnn_grad)\n        \n        # LSTM: same setup\n        lstm = nn.LSTM(input_size=10, hidden_size=20, batch_first=False)\n        x_lstm = torch.randn(seq_len, 1, 10, requires_grad=True)\n        outputs_lstm, _ = lstm(x_lstm)\n        loss_lstm = outputs_lstm[-1].sum()\n        loss_lstm.backward()\n        lstm_grad = x_lstm.grad[0].abs().mean().item()\n        lstm_retention.append(lstm_grad)\n    \n    # Normalize to first value for comparison\n    rnn_retention = np.array(rnn_retention) / rnn_retention[0]\n    lstm_retention = np.array(lstm_retention) / lstm_retention[0]\n    \n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(seq_lengths, rnn_retention, 'b-o', label='RNN', linewidth=2, markersize=8)\n    ax.plot(seq_lengths, lstm_retention, 'r-s', label='LSTM', linewidth=2, markersize=8)\n    ax.set_xlabel('Sequence Length')\n    ax.set_ylabel('Gradient at First Input (relative)')\n    ax.set_title('LSTM Solves Vanishing Gradients!')\n    ax.legend(fontsize=12)\n    ax.set_yscale('log')\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"Gradient retention at different sequence lengths:\")\n    print(\"-\" * 55)\n    print(f\"{'Length':>8}  {'RNN':>12}  {'LSTM':>12}  {'LSTM/RNN':>10}\")\n    print(\"-\" * 55)\n    for i, seq_len in enumerate(seq_lengths):\n        ratio = lstm_retention[i] / rnn_retention[i] if rnn_retention[i] > 0 else float('inf')\n        print(f\"{seq_len:>8}  {rnn_retention[i]:>12.6f}  {lstm_retention[i]:>12.6f}  {ratio:>10.1f}x\")\n    \n    print()\n    print(\"CONCLUSION: For long sequences, LSTM gradients are ORDERS OF MAGNITUDE larger!\")\n    print(\"This is why LSTMs can learn long-range dependencies that RNNs cannot.\")\n\ncompare_gradient_retention()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "pytorch-section",
   "source": "## PyTorch's Built-in LSTM\n\nIn practice, use PyTorch's optimized CUDA implementation. It handles:\n- Bidirectional processing (read sequence both ways)\n- Multiple stacked layers\n- Dropout between layers\n- Efficient batched computation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "pytorch-lstm",
   "outputs": [],
   "source": "# PyTorch LSTM with common configuration\npytorch_lstm = nn.LSTM(\n    input_size=10,       # Size of each input token\n    hidden_size=20,      # Size of hidden state\n    num_layers=2,        # Stack 2 LSTM layers\n    dropout=0.1,         # Dropout between layers (not on last layer)\n    bidirectional=False, # Process forward only (True = both directions)\n    batch_first=True     # Input shape: (batch, seq_len, features)\n)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in pytorch_lstm.parameters())\nprint(f\"Total parameters: {total_params:,}\")\n\n# Forward pass\nx = torch.randn(4, 15, 10)  # batch=4, seq_len=15, features=10\noutputs, (h_n, c_n) = pytorch_lstm(x)\n\nprint(f\"\\nInput shape:       {tuple(x.shape)} = (batch, seq_len, input_size)\")\nprint(f\"Output shape:      {tuple(outputs.shape)} = (batch, seq_len, hidden_size)\")\nprint(f\"Final hidden:      {tuple(h_n.shape)} = (num_layers, batch, hidden_size)\")\nprint(f\"Final cell state:  {tuple(c_n.shape)} = (num_layers, batch, hidden_size)\")\nprint()\nprint(\"Tip: Use outputs for sequence labeling (POS tagging), h_n for classification\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "char-lm-section",
   "source": "## Putting It Together: Character-Level Language Model\n\nLet's train a tiny LSTM to predict the next character. This demonstrates:\n1. How to use LSTM for sequence-to-sequence prediction\n2. That even tiny models can learn patterns from repetitive text\n\n**Note:** This is a minimal 200-step training loop on tiny data. Real language models train on gigabytes of text for days."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "char-lm",
   "outputs": [],
   "source": "# | char-lm inline expanded code-aside\nclass CharLSTM(nn.Module):\n    \"\"\"Simple character-level LSTM language model.\"\"\"\n    \n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, x, hidden=None):\n        embedded = self.embedding(x)\n        outputs, hidden = self.lstm(embedded, hidden)\n        logits = self.fc(outputs)\n        return logits, hidden\n    \n    def generate(self, start_char, char_to_idx, idx_to_char, length=50, temperature=0.8):\n        \"\"\"Generate text starting from a character.\"\"\"\n        self.eval()\n        generated = [start_char]\n        x = torch.tensor([[char_to_idx[start_char]]])\n        hidden = None\n        \n        with torch.no_grad():\n            for _ in range(length):\n                logits, hidden = self(x, hidden)\n                probs = F.softmax(logits[0, -1] / temperature, dim=0)\n                next_idx = torch.multinomial(probs, 1).item()\n                generated.append(idx_to_char[next_idx])\n                x = torch.tensor([[next_idx]])\n        \n        return ''.join(generated)\n\n# Training data: repetitive text so our tiny model can learn something\ntrain_text = \"hello world hello neural network hello lstm hello deep learning \" * 20\n\n# Build vocabulary\ntrain_chars = sorted(set(train_text))\ntrain_char_to_idx = {c: i for i, c in enumerate(train_chars)}\ntrain_idx_to_char = {i: c for c, i in train_char_to_idx.items()}\n\nprint(f\"Training text length: {len(train_text)} characters\")\nprint(f\"Vocabulary: {''.join(train_chars)}\")\nprint(f\"Vocabulary size: {len(train_chars)}\")\n\n# Create model\nmodel = CharLSTM(vocab_size=len(train_chars), embed_size=16, hidden_size=64)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# Convert text to tensor\ndata = torch.tensor([train_char_to_idx[c] for c in train_text])\n\n# Training loop\nprint(\"\\nTraining for 200 steps...\")\nmodel.train()\nseq_length = 30\nlosses = []\n\nfor step in range(200):\n    # Random starting position\n    start = np.random.randint(0, len(data) - seq_length - 1)\n    x = data[start:start + seq_length].unsqueeze(0)\n    y = data[start + 1:start + seq_length + 1].unsqueeze(0)\n    \n    optimizer.zero_grad()\n    logits, _ = model(x)\n    loss = F.cross_entropy(logits.view(-1, len(train_chars)), y.view(-1))\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    \n    if (step + 1) % 50 == 0:\n        print(f\"Step {step+1}: loss = {loss.item():.3f}\")\n\n# Generate some text\nprint(\"\\n\" + \"=\"*50)\nprint(\"Generated text (starting with 'h'):\")\nprint(\"=\"*50)\nprint(model.generate('h', train_char_to_idx, train_idx_to_char, length=100))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "huggingface-section",
   "source": "## Beyond LSTMs: What Came Next\n\nLSTMs dominated NLP from 2014-2017, enabling breakthroughs in:\n- Machine translation (seq2seq with attention)\n- Speech recognition\n- Sentiment analysis\n- Text generation\n\nBut LSTMs have limitations:\n- **Sequential processing:** Can't parallelize across timesteps (slow to train)\n- **Fixed memory:** Cell state is a fixed-size vector (hard to store many facts)\n- **No direct access:** To use information from timestep 5 at timestep 100, it must pass through all intermediate steps\n\n**The Transformer (2017)** solved these by replacing recurrence with **attention**\u2014letting each position directly attend to all other positions. This enables massive parallelization and led to GPT, BERT, and modern LLMs.\n\nBut the intuitions from this notebook carry forward:\n- Residual connections (like the cell state conveyor belt) are everywhere\n- Gating mechanisms appear in transformers too (GLU, Mamba)\n- The vanishing gradient problem drove the development of normalization techniques"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "huggingface",
   "outputs": [],
   "source": "# Historical context: Important LSTM-based models\n\nprint(\"Key LSTM Milestones:\")\nprint(\"-\" * 60)\nprint(\"1997: LSTM invented (Hochreiter & Schmidhuber)\")\nprint(\"2014: Seq2seq for translation (Sutskever et al.)\")\nprint(\"2015: Attention mechanism (Bahdanau et al.)\")\nprint(\"2016: AWD-LSTM achieves SOTA language modeling\")\nprint(\"2017: Transformer - 'Attention Is All You Need'\")\nprint(\"2018: BERT, GPT - transformers dominate NLP\")\nprint()\nprint(\"LSTMs are still used in edge cases (low latency, streaming),\")\nprint(\"but transformers have taken over for most NLP tasks.\")"
  },
  {
   "cell_type": "markdown",
   "id": "lrvh6aytem",
   "source": "## Practice Problems\n\nTest your understanding with these exercises.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pfc8qv18wff",
   "source": "# EXERCISE 1: Implement a GRU cell\n# GRU is a simplified LSTM with only 2 gates (reset and update) instead of 3\n# \n# Equations:\n#   r_t = sigmoid(W_r @ [h_{t-1}, x_t])     # reset gate\n#   z_t = sigmoid(W_z @ [h_{t-1}, x_t])     # update gate\n#   h_tilde = tanh(W_h @ [r_t * h_{t-1}, x_t])  # candidate\n#   h_t = (1 - z_t) * h_{t-1} + z_t * h_tilde  # new hidden state\n#\n# Notice: No separate cell state! The update gate decides how much old vs new.\n\nclass GRUCell:\n    def __init__(self, input_size, hidden_size):\n        self.hidden_size = hidden_size\n        combined_size = input_size + hidden_size\n        scale = np.sqrt(2.0 / combined_size)\n        \n        # Fill in: Initialize weights for reset gate, update gate, and candidate\n        # self.W_r = ...\n        # self.W_z = ...\n        # self.W_h = ...\n        pass\n    \n    def forward(self, x, h_prev):\n        # Fill in: Implement the GRU equations\n        # Return: h_new\n        pass\n\n# Test your implementation:\n# test_gru = GRUCell(input_size=10, hidden_size=20)\n# test_x = np.random.randn(1, 10)\n# test_h = np.zeros((1, 20))\n# test_h_new = test_gru.forward(test_x, test_h)\n# assert test_h_new.shape == (1, 20), f\"Expected shape (1, 20), got {test_h_new.shape}\"\n# print(\"GRU cell works!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7vwqg6vmirx",
   "source": "# EXERCISE 2: What happens if you initialize forget bias to 0 instead of 1?\n#\n# Hypothesis: _______________________________________________\n#\n# Run this experiment and explain the results:\n\ndef test_forget_bias_initialization():\n    \"\"\"Compare LSTM with forget bias = 0 vs forget bias = 1\"\"\"\n    \n    # LSTM with forget bias = 0 (fill in the initialization)\n    # lstm_forget0 = ...\n    \n    # LSTM with forget bias = 1 (our original)\n    # lstm_forget1 = ...\n    \n    # Process a sequence and compare:\n    # - How do the forget gate values differ?\n    # - What happens to cell state magnitude over time?\n    \n    pass\n\n# Your answer: Why does forget bias = 1 work better for long sequences?\n# _______________________________________________",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1e6umz0cf22",
   "source": "# EXERCISE 3: Bidirectional RNN\n#\n# Sometimes context from the FUTURE helps understand the present:\n# \"The bank by the river was muddy\" - seeing \"river\" helps interpret \"bank\"\n#\n# Implement a bidirectional wrapper that:\n# 1. Processes sequence forward: [x1, x2, x3] -> [h1_fwd, h2_fwd, h3_fwd]\n# 2. Processes sequence backward: [x3, x2, x1] -> [h1_bwd, h2_bwd, h3_bwd]\n# 3. Concatenates: [h1_fwd;h1_bwd, h2_fwd;h2_bwd, h3_fwd;h3_bwd]\n\nclass BidirectionalRNN:\n    def __init__(self, input_size, hidden_size):\n        self.forward_rnn = SimpleRNN(input_size, hidden_size)\n        self.backward_rnn = SimpleRNN(input_size, hidden_size)\n        \n    def forward(self, sequence):\n        \"\"\"\n        Args:\n            sequence: (seq_len, batch_size, input_size)\n        Returns:\n            outputs: (seq_len, batch_size, hidden_size * 2)  # Concatenated!\n        \"\"\"\n        # Fill in:\n        # 1. Run forward RNN on sequence\n        # 2. Run backward RNN on reversed sequence\n        # 3. Reverse backward outputs so they align with forward\n        # 4. Concatenate along hidden dimension\n        pass\n\n# Test:\n# test_birnn = BidirectionalRNN(input_size=10, hidden_size=8)\n# test_seq = np.random.randn(5, 1, 10)\n# test_out = test_birnn.forward(test_seq)\n# assert test_out.shape == (5, 1, 16), f\"Expected (5, 1, 16), got {test_out.shape}\"\n# print(\"Bidirectional RNN works!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "summary",
   "source": "## Summary: The Journey from RNN to LSTM\n\nWe started with a simple question: **How can neural networks remember across time?**\n\n### The RNN Answer\nUse the previous hidden state as additional input. Same weights at every timestep.\n```\nh_t = tanh(W_xh @ x_t + W_hh @ h_{t-1} + b)\n```\n\n### The Problem\nTraining via BPTT multiplies gradients through each timestep. These products either:\n- **Vanish** (factor < 1): Early tokens get no gradient signal\n- **Explode** (factor > 1): Gradients blow up to infinity\n\n### The LSTM Solution\nAdd a **cell state** that flows through time via addition (not multiplication!). Three gates control:\n- **Forget:** What to erase from memory\n- **Input:** What new information to write\n- **Output:** What to reveal to the next layer\n\nWhen the forget gate is close to 1, gradients flow through the cell state almost unchanged. This is why LSTMs can learn long-range dependencies.\n\n### Key Takeaways\n\n1. **Vanishing gradients are exponential** \u2014 even 0.9^100 is basically zero\n2. **Addition > Multiplication** for gradient flow (residual connections everywhere!)\n3. **Sigmoid for gates, tanh for values** \u2014 bounded ranges matter\n4. **Initialize forget bias to 1** \u2014 helps at the start of training\n5. **Gradient clipping prevents explosion** but doesn't help vanishing\n\n### What's Next?\nTransformers replaced recurrence with attention, enabling massive parallelization. But the LSTM intuitions persist: residual connections, gating mechanisms, and careful initialization all trace back to solving the vanishing gradient problem."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}