{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "title",
   "source": [
    "# RNNs and LSTMs from Scratch\n",
    "\n",
    "This notebook builds up Recurrent Neural Networks and LSTMs from first principles, matching the stardust article's Architectures tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "imports",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "rnn-intro",
   "source": [
    "## 1. Simple RNN from Scratch\n",
    "\n",
    "The core RNN equation:\n",
    "$$h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b)$$\n",
    "\n",
    "Let's implement this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "simple-rnn-cell",
   "outputs": [],
   "source": [
    "class SimpleRNNCell:\n",
    "    \"\"\"A single RNN cell implemented from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize weights (using Xavier/Glorot initialization)\n",
    "        scale = np.sqrt(2.0 / (input_size + hidden_size))\n",
    "        self.W_xh = np.random.randn(input_size, hidden_size) * scale\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"Single forward step.\n",
    "        \n",
    "        Args:\n",
    "            x: Input at current timestep (batch_size, input_size)\n",
    "            h_prev: Hidden state from previous timestep (batch_size, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "            h_new: New hidden state (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b)\n",
    "        h_new = np.tanh(x @ self.W_xh + h_prev @ self.W_hh + self.b)\n",
    "        return h_new\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden state to zeros.\"\"\"\n",
    "        return np.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "# Test the RNN cell\n",
    "rnn_cell = SimpleRNNCell(input_size=10, hidden_size=20)\n",
    "x = np.random.randn(1, 10)  # Single input\n",
    "h = rnn_cell.init_hidden(1)\n",
    "\n",
    "# Process one step\n",
    "h_new = rnn_cell.forward(x, h)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Hidden state shape: {h_new.shape}\")\n",
    "print(f\"Hidden state values (first 5): {h_new[0, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "sequence-processing",
   "source": [
    "## 2. Processing a Sequence\n",
    "\n",
    "Now let's process an entire sequence, keeping the hidden state across timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "process-sequence",
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"Full RNN that processes sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.cell = SimpleRNNCell(input_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, sequence):\n",
    "        \"\"\"Process an entire sequence.\n",
    "        \n",
    "        Args:\n",
    "            sequence: (seq_len, batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: All hidden states (seq_len, batch_size, hidden_size)\n",
    "            h_final: Final hidden state (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = sequence.shape\n",
    "        h = self.cell.init_hidden(batch_size)\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            h = self.cell.forward(sequence[t], h)\n",
    "            outputs.append(h)\n",
    "            \n",
    "        return np.stack(outputs), h\n",
    "\n",
    "# Test with a sequence\n",
    "rnn = SimpleRNN(input_size=10, hidden_size=20)\n",
    "sequence = np.random.randn(5, 1, 10)  # 5 timesteps, batch of 1\n",
    "\n",
    "outputs, h_final = rnn.forward(sequence)\n",
    "print(f\"Sequence length: 5\")\n",
    "print(f\"All outputs shape: {outputs.shape}\")\n",
    "print(f\"Final hidden state shape: {h_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "unrolling-viz",
   "source": [
    "## 3. Visualizing the Unrolled RNN\n",
    "\n",
    "When we process a sequence, the RNN \"unrolls\" into a deep network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "unroll-viz",
   "outputs": [],
   "source": [
    "def visualize_hidden_states(outputs, title=\"Hidden State Evolution\"):\n",
    "    \"\"\"Visualize how hidden states evolve over time.\"\"\"\n",
    "    # outputs: (seq_len, batch_size, hidden_size)\n",
    "    states = outputs[:, 0, :]  # Take first batch\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.imshow(states.T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    plt.colorbar(label='Activation')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Hidden Unit')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Process a longer sequence and visualize\n",
    "long_sequence = np.random.randn(20, 1, 10)\n",
    "outputs, _ = rnn.forward(long_sequence)\n",
    "visualize_hidden_states(outputs, \"Hidden States Over 20 Timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "bptt-section",
   "source": [
    "## 4. Backpropagation Through Time (BPTT)\n",
    "\n",
    "The gradient for RNN weights involves a product across all timesteps:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\sum_{t=1}^{T} \\frac{\\partial L_t}{\\partial W} = \\sum_{t} \\frac{\\partial L_t}{\\partial h_t} \\prod_{k=1}^{t} \\frac{\\partial h_k}{\\partial h_{k-1}}$$\n",
    "\n",
    "Let's implement BPTT for our simple RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "bptt-impl",
   "outputs": [],
   "source": [
    "class RNNWithBPTT:\n",
    "    \"\"\"RNN with full BPTT implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        scale = np.sqrt(2.0 / (input_size + hidden_size))\n",
    "        self.W_xh = np.random.randn(input_size, hidden_size) * scale\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        \n",
    "        # Gradients\n",
    "        self.dW_xh = np.zeros_like(self.W_xh)\n",
    "        self.dW_hh = np.zeros_like(self.W_hh)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def forward(self, sequence):\n",
    "        \"\"\"Forward pass, storing values for backprop.\"\"\"\n",
    "        seq_len, batch_size, _ = sequence.shape\n",
    "        \n",
    "        # Store values for backprop\n",
    "        self.inputs = sequence\n",
    "        self.hiddens = [np.zeros((batch_size, self.hidden_size))]\n",
    "        self.pre_activations = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            pre_act = sequence[t] @ self.W_xh + self.hiddens[-1] @ self.W_hh + self.b\n",
    "            h = np.tanh(pre_act)\n",
    "            self.pre_activations.append(pre_act)\n",
    "            self.hiddens.append(h)\n",
    "            \n",
    "        return np.stack(self.hiddens[1:]), self.hiddens[-1]\n",
    "    \n",
    "    def backward(self, d_outputs):\n",
    "        \"\"\"Full BPTT backward pass.\n",
    "        \n",
    "        Args:\n",
    "            d_outputs: Gradient of loss w.r.t. outputs (seq_len, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        seq_len = len(self.pre_activations)\n",
    "        batch_size = d_outputs.shape[1]\n",
    "        \n",
    "        # Reset gradients\n",
    "        self.dW_xh = np.zeros_like(self.W_xh)\n",
    "        self.dW_hh = np.zeros_like(self.W_hh)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "        # Gradient flowing back through hidden states\n",
    "        dh_next = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        # Track gradient magnitudes for visualization\n",
    "        grad_magnitudes = []\n",
    "        \n",
    "        # Backprop through time (reverse order)\n",
    "        for t in reversed(range(seq_len)):\n",
    "            # Total gradient at this timestep\n",
    "            dh = d_outputs[t] + dh_next\n",
    "            \n",
    "            # Gradient through tanh\n",
    "            d_pre_act = dh * (1 - np.tanh(self.pre_activations[t])**2)\n",
    "            \n",
    "            # Accumulate parameter gradients\n",
    "            self.dW_xh += self.inputs[t].T @ d_pre_act\n",
    "            self.dW_hh += self.hiddens[t].T @ d_pre_act\n",
    "            self.db += d_pre_act.sum(axis=0)\n",
    "            \n",
    "            # Gradient to previous hidden state\n",
    "            dh_next = d_pre_act @ self.W_hh.T\n",
    "            \n",
    "            grad_magnitudes.append(np.linalg.norm(dh_next))\n",
    "            \n",
    "        return list(reversed(grad_magnitudes))\n",
    "\n",
    "# Test BPTT\n",
    "rnn_bptt = RNNWithBPTT(input_size=10, hidden_size=20)\n",
    "sequence = np.random.randn(10, 1, 10)\n",
    "outputs, h_final = rnn_bptt.forward(sequence)\n",
    "\n",
    "# Fake gradient from loss (as if loss was sum of all outputs)\n",
    "d_outputs = np.ones_like(outputs) * 0.1\n",
    "grad_mags = rnn_bptt.backward(d_outputs)\n",
    "\n",
    "print(\"Gradient magnitudes through time:\")\n",
    "for t, mag in enumerate(grad_mags):\n",
    "    print(f\"  t={t}: {mag:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "vanishing-gradients",
   "source": [
    "## 5. The Vanishing Gradient Problem\n",
    "\n",
    "Let's visualize why gradients vanish in RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "vanishing-demo",
   "outputs": [],
   "source": [
    "def demonstrate_vanishing_gradients(seq_lengths=[10, 25, 50, 100]):\n",
    "    \"\"\"Show how gradients vanish with increasing sequence length.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        rnn = RNNWithBPTT(input_size=10, hidden_size=20)\n",
    "        sequence = np.random.randn(seq_len, 1, 10) * 0.1\n",
    "        \n",
    "        outputs, _ = rnn.forward(sequence)\n",
    "        d_outputs = np.ones_like(outputs) * 0.1\n",
    "        grad_mags = rnn.backward(d_outputs)\n",
    "        \n",
    "        # Normalize for comparison\n",
    "        grad_mags = np.array(grad_mags) / max(grad_mags)\n",
    "        plt.plot(range(seq_len), grad_mags, label=f'Length {seq_len}', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Relative Gradient Magnitude')\n",
    "    plt.title('Vanishing Gradients: Gradient magnitude decreases with distance')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_vanishing_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "exponential-decay",
   "outputs": [],
   "source": [
    "# Mathematical demonstration: product of values < 1\n",
    "decay_factor = 0.9\n",
    "timesteps = 100\n",
    "\n",
    "gradient_remaining = [decay_factor ** t for t in range(timesteps)]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gradient_remaining)\n",
    "plt.xlabel('Timesteps back')\n",
    "plt.ylabel('Gradient remaining')\n",
    "plt.title(f'Gradient decay with factor = {decay_factor}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(gradient_remaining)\n",
    "plt.xlabel('Timesteps back')\n",
    "plt.ylabel('Gradient remaining (log scale)')\n",
    "plt.title('Same data, log scale')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"After 100 timesteps with decay factor {decay_factor}:\")\n",
    "print(f\"  Gradient remaining: {decay_factor**100:.2e}\")\n",
    "print(f\"  That's {decay_factor**100 * 100:.6f}% of the original gradient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "gradient-clipping-section",
   "source": [
    "## 6. Gradient Clipping\n",
    "\n",
    "For exploding gradients, we clip the gradient norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "gradient-clipping",
   "outputs": [],
   "source": [
    "def clip_gradients(gradients, max_norm=1.0):\n",
    "    \"\"\"Clip gradients to prevent explosion.\n",
    "    \n",
    "    If ||g|| > max_norm: g = g * (max_norm / ||g||)\n",
    "    \"\"\"\n",
    "    total_norm = np.sqrt(sum(np.sum(g**2) for g in gradients))\n",
    "    \n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        clipped = [g * scale for g in gradients]\n",
    "        return clipped, True, total_norm\n",
    "    return gradients, False, total_norm\n",
    "\n",
    "# Simulate exploding gradients\n",
    "np.random.seed(42)\n",
    "large_gradients = [np.random.randn(10, 20) * 10 for _ in range(3)]\n",
    "\n",
    "clipped, was_clipped, original_norm = clip_gradients(large_gradients, max_norm=5.0)\n",
    "clipped_norm = np.sqrt(sum(np.sum(g**2) for g in clipped))\n",
    "\n",
    "print(f\"Original gradient norm: {original_norm:.2f}\")\n",
    "print(f\"Was clipped: {was_clipped}\")\n",
    "print(f\"Clipped gradient norm: {clipped_norm:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "lstm-section",
   "source": [
    "## 7. LSTM from Scratch\n",
    "\n",
    "The LSTM adds a cell state $C_t$ and three gates:\n",
    "\n",
    "**Forget Gate:** $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "\n",
    "**Input Gate:** $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "\n",
    "**Candidate Values:** $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
    "\n",
    "**Cell State Update:** $C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$\n",
    "\n",
    "**Output Gate:** $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "\n",
    "**Hidden State:** $h_t = o_t \\odot \\tanh(C_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "lstm-impl",
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "class LSTMCell:\n",
    "    \"\"\"A single LSTM cell implemented from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        combined_size = input_size + hidden_size\n",
    "        \n",
    "        # All gates share similar structure, so we can combine weights\n",
    "        # Order: forget, input, candidate, output\n",
    "        scale = np.sqrt(2.0 / combined_size)\n",
    "        \n",
    "        # Forget gate weights\n",
    "        self.W_f = np.random.randn(combined_size, hidden_size) * scale\n",
    "        self.b_f = np.ones(hidden_size)  # Initialize forget bias to 1 (keep everything initially)\n",
    "        \n",
    "        # Input gate weights\n",
    "        self.W_i = np.random.randn(combined_size, hidden_size) * scale\n",
    "        self.b_i = np.zeros(hidden_size)\n",
    "        \n",
    "        # Candidate values weights\n",
    "        self.W_c = np.random.randn(combined_size, hidden_size) * scale\n",
    "        self.b_c = np.zeros(hidden_size)\n",
    "        \n",
    "        # Output gate weights\n",
    "        self.W_o = np.random.randn(combined_size, hidden_size) * scale\n",
    "        self.b_o = np.zeros(hidden_size)\n",
    "        \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \"\"\"Single LSTM forward step.\n",
    "        \n",
    "        Args:\n",
    "            x: Current input (batch_size, input_size)\n",
    "            h_prev: Previous hidden state (batch_size, hidden_size)\n",
    "            c_prev: Previous cell state (batch_size, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "            h_new: New hidden state\n",
    "            c_new: New cell state\n",
    "            gates: Dictionary of gate activations for visualization\n",
    "        \"\"\"\n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = np.concatenate([x, h_prev], axis=1)\n",
    "        \n",
    "        # Compute gates\n",
    "        f = sigmoid(combined @ self.W_f + self.b_f)  # Forget gate\n",
    "        i = sigmoid(combined @ self.W_i + self.b_i)  # Input gate\n",
    "        c_tilde = np.tanh(combined @ self.W_c + self.b_c)  # Candidate values\n",
    "        o = sigmoid(combined @ self.W_o + self.b_o)  # Output gate\n",
    "        \n",
    "        # Update cell state\n",
    "        c_new = f * c_prev + i * c_tilde\n",
    "        \n",
    "        # Compute hidden state\n",
    "        h_new = o * np.tanh(c_new)\n",
    "        \n",
    "        gates = {'forget': f, 'input': i, 'output': o, 'candidate': c_tilde}\n",
    "        return h_new, c_new, gates\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        \"\"\"Initialize hidden and cell states.\"\"\"\n",
    "        h = np.zeros((batch_size, self.hidden_size))\n",
    "        c = np.zeros((batch_size, self.hidden_size))\n",
    "        return h, c\n",
    "\n",
    "# Test LSTM cell\n",
    "lstm_cell = LSTMCell(input_size=10, hidden_size=20)\n",
    "x = np.random.randn(1, 10)\n",
    "h, c = lstm_cell.init_state(1)\n",
    "\n",
    "h_new, c_new, gates = lstm_cell.forward(x, h, c)\n",
    "print(f\"Hidden state shape: {h_new.shape}\")\n",
    "print(f\"Cell state shape: {c_new.shape}\")\n",
    "print(f\"\\nGate activations (first 5 values):\")\n",
    "for name, values in gates.items():\n",
    "    print(f\"  {name}: {values[0, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "full-lstm",
   "source": [
    "## 8. Full LSTM for Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "lstm-sequence",
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \"\"\"Full LSTM that processes sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.cell = LSTMCell(input_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, sequence, return_all_gates=False):\n",
    "        \"\"\"Process an entire sequence.\n",
    "        \n",
    "        Args:\n",
    "            sequence: (seq_len, batch_size, input_size)\n",
    "            return_all_gates: Whether to return gate activations\n",
    "        \n",
    "        Returns:\n",
    "            outputs: All hidden states (seq_len, batch_size, hidden_size)\n",
    "            (h_final, c_final): Final states\n",
    "            all_gates: List of gate dictionaries (if return_all_gates=True)\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = sequence.shape\n",
    "        h, c = self.cell.init_state(batch_size)\n",
    "        \n",
    "        outputs = []\n",
    "        cell_states = []\n",
    "        all_gates = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            h, c, gates = self.cell.forward(sequence[t], h, c)\n",
    "            outputs.append(h)\n",
    "            cell_states.append(c)\n",
    "            all_gates.append(gates)\n",
    "            \n",
    "        result = (np.stack(outputs), (h, c))\n",
    "        if return_all_gates:\n",
    "            result = result + (all_gates, np.stack(cell_states))\n",
    "        return result\n",
    "\n",
    "# Test LSTM\n",
    "lstm = LSTM(input_size=10, hidden_size=20)\n",
    "sequence = np.random.randn(15, 1, 10)\n",
    "\n",
    "outputs, (h_final, c_final), all_gates, cell_states = lstm.forward(sequence, return_all_gates=True)\n",
    "print(f\"Outputs shape: {outputs.shape}\")\n",
    "print(f\"Cell states shape: {cell_states.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "gates-viz",
   "source": [
    "## 9. Visualizing LSTM Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "visualize-gates",
   "outputs": [],
   "source": [
    "def visualize_lstm_gates(all_gates, cell_states):\n",
    "    \"\"\"Visualize how LSTM gates behave over time.\"\"\"\n",
    "    seq_len = len(all_gates)\n",
    "    \n",
    "    # Extract gate values (average across hidden units)\n",
    "    forget_vals = [g['forget'].mean() for g in all_gates]\n",
    "    input_vals = [g['input'].mean() for g in all_gates]\n",
    "    output_vals = [g['output'].mean() for g in all_gates]\n",
    "    cell_magnitude = [np.abs(c).mean() for c in cell_states]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    axes[0, 0].plot(forget_vals, 'b-', linewidth=2)\n",
    "    axes[0, 0].set_title('Forget Gate (average)')\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    axes[0, 0].set_ylabel('Gate value')\n",
    "    axes[0, 0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    axes[0, 1].plot(input_vals, 'g-', linewidth=2)\n",
    "    axes[0, 1].set_title('Input Gate (average)')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    axes[0, 1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    axes[1, 0].plot(output_vals, 'r-', linewidth=2)\n",
    "    axes[1, 0].set_title('Output Gate (average)')\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    axes[1, 0].set_xlabel('Timestep')\n",
    "    axes[1, 0].set_ylabel('Gate value')\n",
    "    axes[1, 0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    axes[1, 1].plot(cell_magnitude, 'm-', linewidth=2)\n",
    "    axes[1, 1].set_title('Cell State Magnitude (average)')\n",
    "    axes[1, 1].set_xlabel('Timestep')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('LSTM Gate Activations Over Time', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "visualize_lstm_gates(all_gates, cell_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "activation-section",
   "source": [
    "## 10. Activation Functions Comparison\n",
    "\n",
    "LSTMs use sigmoid for gates (0 to 1) and tanh for values (-1 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "activation-comparison",
   "outputs": [],
   "source": [
    "def compare_activations():\n",
    "    x = np.linspace(-5, 5, 200)\n",
    "    \n",
    "    relu = np.maximum(0, x)\n",
    "    sigmoid_vals = 1 / (1 + np.exp(-x))\n",
    "    tanh_vals = np.tanh(x)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    \n",
    "    # ReLU\n",
    "    axes[0].plot(x, relu, 'g-', linewidth=2)\n",
    "    axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[0].set_title('ReLU: max(0, x)')\n",
    "    axes[0].set_xlabel('x')\n",
    "    axes[0].set_ylabel('f(x)')\n",
    "    axes[0].set_ylim(-1, 5)\n",
    "    axes[0].text(2, 3.5, 'Range: [0, inf)\\nFast, but can \"die\"', fontsize=10)\n",
    "    \n",
    "    # Sigmoid\n",
    "    axes[1].plot(x, sigmoid_vals, 'b-', linewidth=2)\n",
    "    axes[1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[1].set_title('Sigmoid: 1/(1+e^-x)')\n",
    "    axes[1].set_xlabel('x')\n",
    "    axes[1].set_ylim(-0.2, 1.2)\n",
    "    axes[1].text(-4.5, 0.8, 'Range: [0, 1]\\nPerfect for gates', fontsize=10)\n",
    "    \n",
    "    # Tanh\n",
    "    axes[2].plot(x, tanh_vals, 'r-', linewidth=2)\n",
    "    axes[2].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[2].axhline(y=-1, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[2].axhline(y=1, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[2].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[2].set_title('Tanh: (e^x - e^-x)/(e^x + e^-x)')\n",
    "    axes[2].set_xlabel('x')\n",
    "    axes[2].set_ylim(-1.4, 1.4)\n",
    "    axes[2].text(-4.5, 0.8, 'Range: [-1, 1]\\nZero-centered, for values', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "he-init-section",
   "source": [
    "## 11. He Initialization\n",
    "\n",
    "Proper initialization prevents vanishing/exploding activations in early training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "he-init",
   "outputs": [],
   "source": [
    "def compare_initializations():\n",
    "    \"\"\"Compare different weight initialization schemes.\"\"\"\n",
    "    hidden_size = 256\n",
    "    num_layers = 10\n",
    "    \n",
    "    def forward_random(x):\n",
    "        \"\"\"Random initialization (bad).\"\"\"\n",
    "        for _ in range(num_layers):\n",
    "            W = np.random.randn(hidden_size, hidden_size)\n",
    "            x = np.tanh(x @ W)\n",
    "        return x\n",
    "    \n",
    "    def forward_xavier(x):\n",
    "        \"\"\"Xavier/Glorot initialization.\"\"\"\n",
    "        for _ in range(num_layers):\n",
    "            W = np.random.randn(hidden_size, hidden_size) * np.sqrt(1.0 / hidden_size)\n",
    "            x = np.tanh(x @ W)\n",
    "        return x\n",
    "    \n",
    "    def forward_he(x):\n",
    "        \"\"\"He initialization (for ReLU-like, but works well generally).\"\"\"\n",
    "        for _ in range(num_layers):\n",
    "            W = np.random.randn(hidden_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "            x = np.tanh(x @ W)\n",
    "        return x\n",
    "    \n",
    "    # Test\n",
    "    np.random.seed(42)\n",
    "    x = np.random.randn(32, hidden_size)\n",
    "    \n",
    "    results = {\n",
    "        'Random': forward_random(x.copy()),\n",
    "        'Xavier': forward_xavier(x.copy()),\n",
    "        'He': forward_he(x.copy()),\n",
    "    }\n",
    "    \n",
    "    print(\"Activation statistics after 10 layers:\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, output in results.items():\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Mean: {output.mean():.6f}\")\n",
    "        print(f\"  Std:  {output.std():.6f}\")\n",
    "        print(f\"  Max:  {np.abs(output).max():.6f}\")\n",
    "        print()\n",
    "\n",
    "compare_initializations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "rnn-vs-lstm",
   "source": [
    "## 12. RNN vs LSTM: Gradient Flow Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "compare-gradient-flow",
   "outputs": [],
   "source": [
    "def compare_gradient_retention(seq_lengths=[10, 25, 50, 100, 200]):\n",
    "    \"\"\"Compare how RNN and LSTM retain gradients over sequence length.\"\"\"\n",
    "    \n",
    "    # Use PyTorch for cleaner gradient computation\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    rnn_retention = []\n",
    "    lstm_retention = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        # RNN\n",
    "        rnn = nn.RNN(input_size=10, hidden_size=20, batch_first=False)\n",
    "        x_rnn = torch.randn(seq_len, 1, 10, requires_grad=True)\n",
    "        outputs_rnn, _ = rnn(x_rnn)\n",
    "        loss_rnn = outputs_rnn[-1].sum()  # Loss at final timestep\n",
    "        loss_rnn.backward()\n",
    "        # Gradient at first input\n",
    "        rnn_grad = x_rnn.grad[0].abs().mean().item()\n",
    "        rnn_retention.append(rnn_grad)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm = nn.LSTM(input_size=10, hidden_size=20, batch_first=False)\n",
    "        x_lstm = torch.randn(seq_len, 1, 10, requires_grad=True)\n",
    "        outputs_lstm, _ = lstm(x_lstm)\n",
    "        loss_lstm = outputs_lstm[-1].sum()\n",
    "        loss_lstm.backward()\n",
    "        lstm_grad = x_lstm.grad[0].abs().mean().item()\n",
    "        lstm_retention.append(lstm_grad)\n",
    "    \n",
    "    # Normalize\n",
    "    rnn_retention = np.array(rnn_retention) / rnn_retention[0]\n",
    "    lstm_retention = np.array(lstm_retention) / lstm_retention[0]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(seq_lengths, rnn_retention, 'b-o', label='RNN', linewidth=2)\n",
    "    plt.plot(seq_lengths, lstm_retention, 'r-s', label='LSTM', linewidth=2)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Relative Gradient at First Input')\n",
    "    plt.title('Gradient Retention: RNN vs LSTM')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nGradient retention at different sequence lengths:\")\n",
    "    for i, seq_len in enumerate(seq_lengths):\n",
    "        print(f\"  Length {seq_len:3d}: RNN={rnn_retention[i]:.4f}, LSTM={lstm_retention[i]:.4f}\")\n",
    "\n",
    "compare_gradient_retention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "pytorch-section",
   "source": [
    "## 13. Using PyTorch's Built-in LSTM\n",
    "\n",
    "In practice, we use optimized implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "pytorch-lstm",
   "outputs": [],
   "source": [
    "# PyTorch LSTM\n",
    "pytorch_lstm = nn.LSTM(\n",
    "    input_size=10,\n",
    "    hidden_size=20,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    bidirectional=False,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in pytorch_lstm.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(4, 15, 10)  # batch=4, seq_len=15, input=10\n",
    "outputs, (h_n, c_n) = pytorch_lstm(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "print(f\"Final hidden shape: {h_n.shape}\")\n",
    "print(f\"Final cell shape: {c_n.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "char-lm-section",
   "source": [
    "## 14. Character-Level Language Model Demo\n",
    "\n",
    "A simple example of what RNNs/LSTMs can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "char-lm",
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    \"\"\"Simple character-level LSTM language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.lstm(embedded, hidden)\n",
    "        logits = self.fc(outputs)\n",
    "        return logits, hidden\n",
    "    \n",
    "    def generate(self, start_char, char_to_idx, idx_to_char, length=100, temperature=1.0):\n",
    "        \"\"\"Generate text starting from a character.\"\"\"\n",
    "        self.eval()\n",
    "        generated = [start_char]\n",
    "        x = torch.tensor([[char_to_idx[start_char]]])\n",
    "        hidden = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(length):\n",
    "                logits, hidden = self(x, hidden)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=0)\n",
    "                next_idx = torch.multinomial(probs, 1).item()\n",
    "                generated.append(idx_to_char[next_idx])\n",
    "                x = torch.tensor([[next_idx]])\n",
    "        \n",
    "        return ''.join(generated)\n",
    "\n",
    "# Simple demo with a tiny vocabulary\n",
    "text = \"hello world hello neural network hello lstm hello deep learning\"\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "\n",
    "print(f\"Vocabulary: {chars}\")\n",
    "print(f\"Vocabulary size: {len(chars)}\")\n",
    "\n",
    "# Create model (would need training for good output)\n",
    "model = CharLSTM(vocab_size=len(chars), embed_size=16, hidden_size=32)\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "huggingface-section",
   "source": [
    "## 15. Pretrained Models from Hugging Face\n",
    "\n",
    "For real applications, use pretrained models. AWD-LSTM was state-of-the-art before transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "huggingface",
   "outputs": [],
   "source": [
    "# Note: Uncomment to use (requires internet and transformers library)\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# For AWD-LSTM style models, you can look at:\n",
    "# - salesforce/awd-lstm-lm (Language modeling)\n",
    "# - fastai's ULMFiT implementation\n",
    "\n",
    "print(\"For pretrained LSTM models, check:\")\n",
    "print(\"  - salesforce/awd-lstm-lm\")\n",
    "print(\"  - fastai's language model pretraining\")\n",
    "print(\"  - https://huggingface.co/models?filter=lstm\")\n",
    "print(\"\\nHowever, most production NLP now uses transformers (BERT, GPT, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "summary",
   "source": [
    "## Summary\n",
    "\n",
    "What we covered:\n",
    "1. **Simple RNN**: Process sequences with shared weights across time\n",
    "2. **BPTT**: Backpropagation through the unrolled network\n",
    "3. **Vanishing Gradients**: Why RNNs struggle with long sequences\n",
    "4. **Gradient Clipping**: Prevent exploding gradients\n",
    "5. **LSTM**: Cell state + gates = long-term memory\n",
    "6. **Activation Functions**: Sigmoid for gates, tanh for values\n",
    "7. **Initialization**: He/Xavier for stable training\n",
    "\n",
    "LSTMs were dominant in NLP from 2014-2017, enabling:\n",
    "- Machine translation (seq2seq)\n",
    "- Text generation\n",
    "- Sentiment analysis\n",
    "- Speech recognition\n",
    "\n",
    "But they have limitations: sequential processing (can't parallelize), fixed context window.\n",
    "\n",
    "**What came next?** The Transformer architecture (2017), which uses attention to look at all positions simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
