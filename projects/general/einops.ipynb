{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e699b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch\n",
    "from torch import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caff4685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "einsum(equation, *operands) -> Tensor\n",
      "\n",
      "Sums the product of the elements of the input :attr:`operands` along dimensions specified using a notation\n",
      "based on the Einstein summation convention.\n",
      "\n",
      "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\n",
      "in a short-hand format based on the Einstein summation convention, given by :attr:`equation`. The details of\n",
      "this format are described below, but the general idea is to label every dimension of the input :attr:`operands`\n",
      "with some subscript and define which subscripts are part of the output. The output is then computed by summing\n",
      "the product of the elements of the :attr:`operands` along the dimensions whose subscripts are not part of the\n",
      "output. For example, matrix multiplication can be computed using einsum as `torch.einsum(\"ij,jk->ik\", A, B)`.\n",
      "Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why).\n",
      "\n",
      "Equation:\n",
      "\n",
      "    The :attr:`equation` string specifies the subscripts (letters in `[a-zA-Z]`) for each dimension of\n",
      "    the input :attr:`operands` in the same order as the dimensions, separating subscripts for each operand by a\n",
      "    comma (','), e.g. `'ij,jk'` specify subscripts for two 2D operands. The dimensions labeled with the same subscript\n",
      "    must be broadcastable, that is, their size must either match or be `1`. The exception is if a subscript is\n",
      "    repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\n",
      "    must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\n",
      "    appear exactly once in the :attr:`equation` will be part of the output, sorted in increasing alphabetical order.\n",
      "    The output is computed by multiplying the input :attr:`operands` element-wise, with their dimensions aligned based\n",
      "    on the subscripts, and then summing out the dimensions whose subscripts are not part of the output.\n",
      "\n",
      "    Optionally, the output subscripts can be explicitly defined by adding an arrow ('->') at the end of the equation\n",
      "    followed by the subscripts for the output. For instance, the following equation computes the transpose of a\n",
      "    matrix multiplication: 'ij,jk->ki'. The output subscripts must appear at least once for some input operand and\n",
      "    at most once for the output.\n",
      "\n",
      "    Ellipsis ('...') can be used in place of subscripts to broadcast the dimensions covered by the ellipsis.\n",
      "    Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts,\n",
      "    e.g. for an input operand with 5 dimensions, the ellipsis in the equation `'ab...c'` cover the third and fourth\n",
      "    dimensions. The ellipsis does not need to cover the same number of dimensions across the :attr:`operands` but the\n",
      "    'shape' of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not\n",
      "    explicitly defined with the arrow ('->') notation, the ellipsis will come first in the output (left-most dimensions),\n",
      "    before the subscript labels that appear exactly once for the input operands. e.g. the following equation implements\n",
      "    batch matrix multiplication `'...ij,...jk'`.\n",
      "\n",
      "    A few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis,\n",
      "    arrow and comma) but something like `'. . .'` is not valid. An empty string `''` is valid for scalar operands.\n",
      "\n",
      ".. note::\n",
      "\n",
      "    ``torch.einsum`` handles ellipsis ('...') differently from NumPy in that it allows dimensions\n",
      "    covered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.\n",
      "\n",
      ".. note::\n",
      "\n",
      "    Please install opt-einsum (https://optimized-einsum.readthedocs.io/en/stable/) in order to enroll into a more\n",
      "    performant einsum. You can install when installing torch like so: `pip install torch[opt-einsum]` or by itself\n",
      "    with `pip install opt-einsum`.\n",
      "\n",
      "    If opt-einsum is available, this function will automatically speed up computation and/or consume less memory\n",
      "    by optimizing contraction order through our opt_einsum backend :mod:`torch.backends.opt_einsum` (The _ vs - is\n",
      "    confusing, I know). This optimization occurs when there are at least three inputs, since the order does not matter\n",
      "    otherwise. Note that finding `the` optimal path is an NP-hard problem, thus, opt-einsum relies on different\n",
      "    heuristics to achieve near-optimal results. If opt-einsum is not available, the default order is to contract\n",
      "    from left to right.\n",
      "\n",
      "    To bypass this default behavior, add the following to disable opt_einsum and skip path calculation:\n",
      "    ``torch.backends.opt_einsum.enabled = False``\n",
      "\n",
      "    To specify which strategy you'd like for opt_einsum to compute the contraction path, add the following line:\n",
      "    ``torch.backends.opt_einsum.strategy = 'auto'``. The default strategy is 'auto', and we also support 'greedy' and\n",
      "    'optimal'. Disclaimer that the runtime of 'optimal' is factorial in the number of inputs! See more details in\n",
      "    the opt_einsum documentation (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).\n",
      "\n",
      ".. note::\n",
      "\n",
      "    As of PyTorch 1.10 :func:`torch.einsum` also supports the sublist format (see examples below). In this format,\n",
      "    subscripts for each operand are specified by sublists, list of integers in the range [0, 52). These sublists\n",
      "    follow their operands, and an extra sublist can appear at the end of the input to specify the output's\n",
      "    subscripts., e.g. `torch.einsum(op1, sublist1, op2, sublist2, ..., [subslist_out])`. Python's `Ellipsis` object\n",
      "    may be provided in a sublist to enable broadcasting as described in the Equation section above.\n",
      "\n",
      "Args:\n",
      "    equation (str): The subscripts for the Einstein summation.\n",
      "    operands (List[Tensor]): The tensors to compute the Einstein summation of.\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "    >>> # trace\n",
      "    >>> torch.einsum('ii', torch.randn(4, 4))\n",
      "    tensor(-1.2104)\n",
      "\n",
      "    >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "    >>> # diagonal\n",
      "    >>> torch.einsum('ii->i', torch.randn(4, 4))\n",
      "    tensor([-0.1034,  0.7952, -0.2433,  0.4545])\n",
      "\n",
      "    >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "    >>> # outer product\n",
      "    >>> x = torch.randn(5)\n",
      "    >>> y = torch.randn(4)\n",
      "    >>> torch.einsum('i,j->ij', x, y)\n",
      "    tensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\n",
      "            [-0.3744,  0.9381,  1.2685, -1.6070],\n",
      "            [ 0.7208, -1.8058, -2.4419,  3.0936],\n",
      "            [ 0.1713, -0.4291, -0.5802,  0.7350],\n",
      "            [ 0.5704, -1.4290, -1.9323,  2.4480]])\n",
      "\n",
      "    >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "    >>> # batch matrix multiplication\n",
      "    >>> As = torch.randn(3, 2, 5)\n",
      "    >>> Bs = torch.randn(3, 5, 4)\n",
      "    >>> torch.einsum('bij,bjk->bik', As, Bs)\n",
      "    tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n",
      "            [-1.6706, -0.8097, -0.8025, -2.1183]],\n",
      "\n",
      "            [[ 4.2239,  0.3107, -0.5756, -0.2354],\n",
      "            [-1.4558, -0.3460,  1.5087, -0.8530]],\n",
      "\n",
      "            [[ 2.8153,  1.8787, -4.3839, -1.2112],\n",
      "            [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n",
      "\n",
      "    >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "    >>> # with sublist format and ellipsis\n",
      "    >>> torch.einsum(As, [..., 0, 1], Bs, [..., 1, 2], [..., 0, 2])\n",
      "    tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n",
      "            [-1.6706, -0.8097, -0.8025, -2.1183]],\n",
      "\n",
      "            [[ 4.2239,  0.3107, -0.5756, -0.2354],\n",
      "            [-1.4558, -0.3460,  1.5087, -0.8530]],\n",
      "\n",
      "            [[ 2.8153,  1.8787, -4.3839, -1.2112],\n",
      "            [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n",
      "\n",
      "    >>> # batch permute\n",
      "    >>> A = torch.randn(2, 3, 4, 5)\n",
      "    >>> torch.einsum('...ij->...ji', A).shape\n",
      "    torch.Size([2, 3, 5, 4])\n",
      "\n",
      "    >>> # equivalent to torch.nn.functional.bilinear\n",
      "    >>> A = torch.randn(3, 5, 4)\n",
      "    >>> l = torch.randn(2, 5)\n",
      "    >>> r = torch.randn(2, 4)\n",
      "    >>> torch.einsum('bn,anm,bm->ba', l, A, r)\n",
      "    tensor([[-0.3430, -5.2405,  0.4494],\n",
      "            [ 0.3311,  5.5201, -3.0356]])\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.11/site-packages/torch/functional.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "einsum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9e524da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\n",
      "This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\n",
      "stack, concatenate and other operations.\n",
      "\n",
      "Examples:\n",
      "\n",
      "```python\n",
      "# suppose we have a set of 32 images in \"h w c\" format (height-width-channel)\n",
      ">>> images = [np.random.randn(30, 40, 3) for _ in range(32)]\n",
      "\n",
      "# stack along first (batch) axis, output is a single array\n",
      ">>> rearrange(images, 'b h w c -> b h w c').shape\n",
      "(32, 30, 40, 3)\n",
      "\n",
      "# stacked and reordered axes to \"b c h w\" format\n",
      ">>> rearrange(images, 'b h w c -> b c h w').shape\n",
      "(32, 3, 30, 40)\n",
      "\n",
      "# concatenate images along height (vertical axis), 960 = 32 * 30\n",
      ">>> rearrange(images, 'b h w c -> (b h) w c').shape\n",
      "(960, 40, 3)\n",
      "\n",
      "# concatenated images along horizontal axis, 1280 = 32 * 40\n",
      ">>> rearrange(images, 'b h w c -> h (b w) c').shape\n",
      "(30, 1280, 3)\n",
      "\n",
      "# flattened each image into a vector, 3600 = 30 * 40 * 3\n",
      ">>> rearrange(images, 'b h w c -> b (c h w)').shape\n",
      "(32, 3600)\n",
      "\n",
      "# split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2\n",
      ">>> rearrange(images, 'b (h1 h) (w1 w) c -> (b h1 w1) h w c', h1=2, w1=2).shape\n",
      "(128, 15, 20, 3)\n",
      "\n",
      "# space-to-depth operation\n",
      ">>> rearrange(images, 'b (h h1) (w w1) c -> b h w (c h1 w1)', h1=2, w1=2).shape\n",
      "(32, 15, 20, 12)\n",
      "\n",
      "```\n",
      "\n",
      "When composing axes, C-order enumeration used (consecutive elements have different last axis).\n",
      "Find more examples in einops tutorial.\n",
      "\n",
      "Parameters:\n",
      "    tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch).\n",
      "            list of tensors is also accepted, those should be of the same type and shape\n",
      "    pattern: string, rearrangement pattern\n",
      "    axes_lengths: any additional specifications for dimensions\n",
      "\n",
      "Returns:\n",
      "    tensor of the same type as input. If possible, a view to the original tensor is returned.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.11/site-packages/einops/einops.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "rearrange?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a7d98d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.randn_like() â€” creates a tensor of random numbers with the same shape as another tensor\n",
    "# Create a random matrix of shape (64, 256, 32)\n",
    "x = torch.randn(64, 256, 32)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f20a6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 256, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = rearrange(x, 'n c ( h w ) -> ( n h ) c w', h=8)\n",
    "x_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163d16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x_reversed = rearrange(x_new, '( n h ) c w -> n c ( h w )', h=8)\n",
    "x_reversed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80981b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
